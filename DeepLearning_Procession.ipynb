{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPU3g4XV5isIpW9aq9GVOO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hufexv/Basic_python_code/blob/main/DeepLearning_Procession.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIwx8o4TBUET"
      },
      "outputs": [],
      "source": [
        "Deep_learning_Moudle_training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Modules\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "class FcModule(nn.Module):\n",
        "  def __init__(self, input_nums, output_nums, func):\n",
        "    super(FcModule, self).__init__()\n",
        "    self.w = nn.Parameter(torch.empty(input_nums, output_nums))\n",
        "    self.b = nn.Parameter(torch.empty(output_nums))\n",
        "    self.func = func\n",
        "    for i in self.parameters():\n",
        "        nn.init.normal(i.data)\n",
        "  def forward(self, x):\n",
        "    node = x @ self.w + self.b\n",
        "    output = self.func(node)\n",
        "\n",
        "    return output\n",
        "\n",
        "class CrossModule(nn.Module):\n",
        "  def __init__(self, input_nums):\n",
        "    super(CrossModule, self).__init__()\n",
        "    assert input_nums // 2 != 0\n",
        "    self.split_i = input_nums // 2\n",
        "\n",
        "  def forward(self, x):\n",
        "    r1, r2 = torch.split(x, self.split_i, dim=-1)\n",
        "    result = r1*r2\n",
        "    output = torch.concat([x, result], dim=1)\n",
        "\n",
        "    return output\n",
        "\n",
        "class Classification(nn.Module):\n",
        "  def __init__(self, input_nums, output_nums):\n",
        "    super(Classification, self).__init__()\n",
        "    self.Layer = nn.Sequential(\n",
        "        FcModule(input_nums, 64, func=nn.Sigmoid()),\n",
        "        FcModule(64, 64, func=nn.Sigmoid()),\n",
        "        CrossModule(64),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(96, 64),\n",
        "            nn.Sigmoid()\n",
        "        ),\n",
        "        FcModule(64, output_nums, func=nn.Identity())\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    output = self.Layer(x)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "rSsoJW3WBoYq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_loader\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class My_dataset(Dataset):\n",
        "  def __init__(self, X, Y):\n",
        "    super(My_dataset, self).__init__()\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self, index):\n",
        "    x, y = self.X[index], self.Y[index]\n",
        "    return np.asarray(x, dtype=np.float32), np.asarray(y, dtype=np.int64)\n",
        "\n",
        "def Data_load(X, Y, batch_size, shuffle):\n",
        "  ds = My_dataset(X, Y)\n",
        "  return DataLoader(\n",
        "      dataset=ds,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      num_workers=0\n",
        "  )\n",
        ""
      ],
      "metadata": {
        "id": "85qkmmyWNcbV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#early\n",
        "class stop_early(object):\n",
        "  def __init__(self, max_step):\n",
        "    super(stop_early, self).__init__()\n",
        "    self._stop = False\n",
        "    self.best_value = -1\n",
        "    self.step = 0\n",
        "    self.max_step = max_step\n",
        "\n",
        "  def is_stop(self):\n",
        "    return self._stop\n",
        "\n",
        "  def update(self, value):\n",
        "    if(self.step > self.max_step | value > self.best_value):\n",
        "      return True\n",
        "    else:\n",
        "      self.step += 1\n",
        "      return False\n",
        ""
      ],
      "metadata": {
        "id": "PnDO2my-TbKa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Whole_procession\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "\n",
        "#1 load_data\n",
        "data = load_digits(return_X_y=False)\n",
        "X, y = data.data, data.target\n",
        "input_nums, output_nums = len(data.feature_names), len(data.target_names)\n",
        "\n",
        "#2 preprocession\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=14)\n",
        "train_loader = Data_load(train_x, train_y, batch_size=8, shuffle=True)\n",
        "test_loader = Data_load(test_x, test_y, batch_size=16, shuffle=False)\n",
        "\n",
        "#3 train_data\n",
        "net = Classification(input_nums=input_nums, output_nums=output_nums)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt_fn = optim.SGD(net.parameters(), lr=0.02)\n",
        "early = stop_early(max_step=100)\n",
        "\n",
        "epoches = 1000\n",
        "for epoch in range(epoches):\n",
        "  net.train()\n",
        "  for i, (x,y) in enumerate(train_loader):\n",
        "    #forward\n",
        "    score = net(x)\n",
        "    loss = loss_fn(score, y)\n",
        "    #backward\n",
        "    opt_fn.zero_grad()\n",
        "    loss.backward()\n",
        "    opt_fn.step()\n",
        "\n",
        "    print(f\"{epoch} {i} Loss {loss.item():.3f}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    net.eval()\n",
        "    total_pre_y = []\n",
        "    for i, (x,y) in enumerate(test_loader):\n",
        "      score = net(x)\n",
        "      pre_y = torch.argmax(score, dim=1).numpy()\n",
        "      total_pre_y.extend(list(pre_y))\n",
        "      loss = loss_fn(score, y)\n",
        "\n",
        "    print(metrics.classification_report(test_y, total_pre_y))\n",
        "    print(metrics.accuracy_score(test_y, total_pre_y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "C7IJEOsvICLQ",
        "outputId": "e032cda7-1437-4855-f6b4-0a7edbf8e6a3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-a210e171818d>:16: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
            "  nn.init.normal(i.data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 Loss 8.899\n",
            "0 1 Loss 7.707\n",
            "0 2 Loss 4.705\n",
            "0 3 Loss 5.828\n",
            "0 4 Loss 3.585\n",
            "0 5 Loss 4.688\n",
            "0 6 Loss 5.229\n",
            "0 7 Loss 2.253\n",
            "0 8 Loss 2.419\n",
            "0 9 Loss 2.579\n",
            "0 10 Loss 2.434\n",
            "0 11 Loss 3.120\n",
            "0 12 Loss 2.372\n",
            "0 13 Loss 2.500\n",
            "0 14 Loss 2.952\n",
            "0 15 Loss 2.233\n",
            "0 16 Loss 1.947\n",
            "0 17 Loss 2.255\n",
            "0 18 Loss 2.454\n",
            "0 19 Loss 2.504\n",
            "0 20 Loss 2.114\n",
            "0 21 Loss 2.229\n",
            "0 22 Loss 2.195\n",
            "0 23 Loss 2.545\n",
            "0 24 Loss 2.148\n",
            "0 25 Loss 2.661\n",
            "0 26 Loss 2.366\n",
            "0 27 Loss 2.423\n",
            "0 28 Loss 2.282\n",
            "0 29 Loss 2.089\n",
            "0 30 Loss 2.406\n",
            "0 31 Loss 1.937\n",
            "0 32 Loss 2.487\n",
            "0 33 Loss 1.915\n",
            "0 34 Loss 2.354\n",
            "0 35 Loss 2.273\n",
            "0 36 Loss 2.247\n",
            "0 37 Loss 1.928\n",
            "0 38 Loss 2.111\n",
            "0 39 Loss 1.965\n",
            "0 40 Loss 2.310\n",
            "0 41 Loss 2.184\n",
            "0 42 Loss 2.277\n",
            "0 43 Loss 2.144\n",
            "0 44 Loss 2.492\n",
            "0 45 Loss 2.330\n",
            "0 46 Loss 1.959\n",
            "0 47 Loss 2.267\n",
            "0 48 Loss 1.863\n",
            "0 49 Loss 2.051\n",
            "0 50 Loss 1.906\n",
            "0 51 Loss 2.446\n",
            "0 52 Loss 2.119\n",
            "0 53 Loss 2.045\n",
            "0 54 Loss 2.366\n",
            "0 55 Loss 2.371\n",
            "0 56 Loss 2.276\n",
            "0 57 Loss 2.112\n",
            "0 58 Loss 2.148\n",
            "0 59 Loss 2.266\n",
            "0 60 Loss 1.814\n",
            "0 61 Loss 1.993\n",
            "0 62 Loss 1.988\n",
            "0 63 Loss 1.921\n",
            "0 64 Loss 1.744\n",
            "0 65 Loss 2.187\n",
            "0 66 Loss 2.217\n",
            "0 67 Loss 1.851\n",
            "0 68 Loss 1.729\n",
            "0 69 Loss 2.274\n",
            "0 70 Loss 1.840\n",
            "0 71 Loss 2.082\n",
            "0 72 Loss 2.062\n",
            "0 73 Loss 1.885\n",
            "0 74 Loss 2.032\n",
            "0 75 Loss 2.114\n",
            "0 76 Loss 1.848\n",
            "0 77 Loss 1.890\n",
            "0 78 Loss 2.283\n",
            "0 79 Loss 1.724\n",
            "0 80 Loss 1.869\n",
            "0 81 Loss 1.868\n",
            "0 82 Loss 1.879\n",
            "0 83 Loss 1.871\n",
            "0 84 Loss 1.836\n",
            "0 85 Loss 2.028\n",
            "0 86 Loss 2.008\n",
            "0 87 Loss 2.122\n",
            "0 88 Loss 1.733\n",
            "0 89 Loss 1.875\n",
            "0 90 Loss 2.070\n",
            "0 91 Loss 2.156\n",
            "0 92 Loss 1.965\n",
            "0 93 Loss 2.142\n",
            "0 94 Loss 2.100\n",
            "0 95 Loss 2.037\n",
            "0 96 Loss 1.983\n",
            "0 97 Loss 2.080\n",
            "0 98 Loss 1.951\n",
            "0 99 Loss 2.165\n",
            "0 100 Loss 2.202\n",
            "0 101 Loss 1.906\n",
            "0 102 Loss 1.758\n",
            "0 103 Loss 2.044\n",
            "0 104 Loss 1.703\n",
            "0 105 Loss 1.818\n",
            "0 106 Loss 1.901\n",
            "0 107 Loss 1.756\n",
            "0 108 Loss 1.870\n",
            "0 109 Loss 2.150\n",
            "0 110 Loss 1.854\n",
            "0 111 Loss 1.768\n",
            "0 112 Loss 1.714\n",
            "0 113 Loss 1.837\n",
            "0 114 Loss 2.052\n",
            "0 115 Loss 1.678\n",
            "0 116 Loss 1.926\n",
            "0 117 Loss 1.948\n",
            "0 118 Loss 1.713\n",
            "0 119 Loss 1.911\n",
            "0 120 Loss 1.527\n",
            "0 121 Loss 1.931\n",
            "0 122 Loss 2.209\n",
            "0 123 Loss 1.680\n",
            "0 124 Loss 1.837\n",
            "0 125 Loss 1.931\n",
            "0 126 Loss 1.665\n",
            "0 127 Loss 1.548\n",
            "0 128 Loss 1.808\n",
            "0 129 Loss 1.685\n",
            "0 130 Loss 1.996\n",
            "0 131 Loss 1.804\n",
            "0 132 Loss 1.655\n",
            "0 133 Loss 1.620\n",
            "0 134 Loss 1.457\n",
            "0 135 Loss 1.504\n",
            "0 136 Loss 1.661\n",
            "0 137 Loss 1.781\n",
            "0 138 Loss 2.058\n",
            "0 139 Loss 2.002\n",
            "0 140 Loss 1.658\n",
            "0 141 Loss 1.948\n",
            "0 142 Loss 1.575\n",
            "0 143 Loss 1.603\n",
            "0 144 Loss 1.657\n",
            "0 145 Loss 1.656\n",
            "0 146 Loss 1.839\n",
            "0 147 Loss 1.452\n",
            "0 148 Loss 1.762\n",
            "0 149 Loss 1.926\n",
            "0 150 Loss 1.881\n",
            "0 151 Loss 1.655\n",
            "0 152 Loss 1.937\n",
            "0 153 Loss 1.739\n",
            "0 154 Loss 1.866\n",
            "0 155 Loss 1.928\n",
            "0 156 Loss 1.831\n",
            "0 157 Loss 1.568\n",
            "0 158 Loss 1.653\n",
            "0 159 Loss 1.433\n",
            "0 160 Loss 1.793\n",
            "0 161 Loss 1.427\n",
            "0 162 Loss 1.433\n",
            "0 163 Loss 1.577\n",
            "0 164 Loss 1.963\n",
            "0 165 Loss 1.306\n",
            "0 166 Loss 1.622\n",
            "0 167 Loss 1.699\n",
            "0 168 Loss 1.378\n",
            "0 169 Loss 1.945\n",
            "0 170 Loss 1.760\n",
            "0 171 Loss 1.887\n",
            "0 172 Loss 1.893\n",
            "0 173 Loss 1.917\n",
            "0 174 Loss 1.383\n",
            "0 175 Loss 1.690\n",
            "0 176 Loss 1.262\n",
            "0 177 Loss 1.275\n",
            "0 178 Loss 1.697\n",
            "0 179 Loss 2.192\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.88      0.80        32\n",
            "           1       1.00      0.11      0.19        28\n",
            "           2       0.50      0.31      0.38        29\n",
            "           3       0.37      0.81      0.51        37\n",
            "           4       0.62      0.84      0.71        31\n",
            "           5       0.79      0.31      0.45        35\n",
            "           6       0.61      0.51      0.56        37\n",
            "           7       0.83      0.12      0.20        43\n",
            "           8       0.35      0.74      0.47        38\n",
            "           9       0.65      0.60      0.62        50\n",
            "\n",
            "    accuracy                           0.53       360\n",
            "   macro avg       0.65      0.52      0.49       360\n",
            "weighted avg       0.64      0.53      0.49       360\n",
            "\n",
            "0.525\n",
            "1 0 Loss 1.429\n",
            "1 1 Loss 1.735\n",
            "1 2 Loss 1.517\n",
            "1 3 Loss 1.596\n",
            "1 4 Loss 1.761\n",
            "1 5 Loss 1.776\n",
            "1 6 Loss 1.371\n",
            "1 7 Loss 1.604\n",
            "1 8 Loss 1.788\n",
            "1 9 Loss 1.830\n",
            "1 10 Loss 1.731\n",
            "1 11 Loss 1.168\n",
            "1 12 Loss 1.300\n",
            "1 13 Loss 1.484\n",
            "1 14 Loss 1.600\n",
            "1 15 Loss 1.815\n",
            "1 16 Loss 1.346\n",
            "1 17 Loss 1.561\n",
            "1 18 Loss 1.367\n",
            "1 19 Loss 1.371\n",
            "1 20 Loss 1.700\n",
            "1 21 Loss 1.472\n",
            "1 22 Loss 1.565\n",
            "1 23 Loss 1.458\n",
            "1 24 Loss 1.232\n",
            "1 25 Loss 1.906\n",
            "1 26 Loss 1.616\n",
            "1 27 Loss 1.527\n",
            "1 28 Loss 1.328\n",
            "1 29 Loss 1.117\n",
            "1 30 Loss 1.387\n",
            "1 31 Loss 1.188\n",
            "1 32 Loss 2.236\n",
            "1 33 Loss 1.575\n",
            "1 34 Loss 1.826\n",
            "1 35 Loss 1.378\n",
            "1 36 Loss 1.613\n",
            "1 37 Loss 1.685\n",
            "1 38 Loss 1.254\n",
            "1 39 Loss 1.480\n",
            "1 40 Loss 1.299\n",
            "1 41 Loss 1.303\n",
            "1 42 Loss 1.513\n",
            "1 43 Loss 1.657\n",
            "1 44 Loss 1.446\n",
            "1 45 Loss 1.437\n",
            "1 46 Loss 1.279\n",
            "1 47 Loss 1.610\n",
            "1 48 Loss 1.605\n",
            "1 49 Loss 1.577\n",
            "1 50 Loss 1.578\n",
            "1 51 Loss 1.646\n",
            "1 52 Loss 1.422\n",
            "1 53 Loss 1.296\n",
            "1 54 Loss 1.368\n",
            "1 55 Loss 1.346\n",
            "1 56 Loss 1.444\n",
            "1 57 Loss 1.253\n",
            "1 58 Loss 1.698\n",
            "1 59 Loss 1.680\n",
            "1 60 Loss 1.341\n",
            "1 61 Loss 1.116\n",
            "1 62 Loss 1.444\n",
            "1 63 Loss 1.263\n",
            "1 64 Loss 1.644\n",
            "1 65 Loss 1.271\n",
            "1 66 Loss 1.490\n",
            "1 67 Loss 1.498\n",
            "1 68 Loss 1.520\n",
            "1 69 Loss 1.616\n",
            "1 70 Loss 1.023\n",
            "1 71 Loss 1.252\n",
            "1 72 Loss 1.558\n",
            "1 73 Loss 1.325\n",
            "1 74 Loss 1.324\n",
            "1 75 Loss 1.347\n",
            "1 76 Loss 1.323\n",
            "1 77 Loss 1.382\n",
            "1 78 Loss 1.764\n",
            "1 79 Loss 1.275\n",
            "1 80 Loss 1.707\n",
            "1 81 Loss 1.446\n",
            "1 82 Loss 1.590\n",
            "1 83 Loss 1.016\n",
            "1 84 Loss 1.132\n",
            "1 85 Loss 1.249\n",
            "1 86 Loss 0.975\n",
            "1 87 Loss 1.319\n",
            "1 88 Loss 1.440\n",
            "1 89 Loss 1.550\n",
            "1 90 Loss 1.197\n",
            "1 91 Loss 1.371\n",
            "1 92 Loss 1.631\n",
            "1 93 Loss 0.832\n",
            "1 94 Loss 1.050\n",
            "1 95 Loss 1.261\n",
            "1 96 Loss 1.179\n",
            "1 97 Loss 1.386\n",
            "1 98 Loss 1.674\n",
            "1 99 Loss 1.472\n",
            "1 100 Loss 1.657\n",
            "1 101 Loss 1.379\n",
            "1 102 Loss 1.613\n",
            "1 103 Loss 1.591\n",
            "1 104 Loss 1.765\n",
            "1 105 Loss 1.114\n",
            "1 106 Loss 0.998\n",
            "1 107 Loss 1.624\n",
            "1 108 Loss 1.158\n",
            "1 109 Loss 1.476\n",
            "1 110 Loss 1.445\n",
            "1 111 Loss 1.192\n",
            "1 112 Loss 0.846\n",
            "1 113 Loss 1.409\n",
            "1 114 Loss 1.215\n",
            "1 115 Loss 0.883\n",
            "1 116 Loss 1.262\n",
            "1 117 Loss 1.293\n",
            "1 118 Loss 1.192\n",
            "1 119 Loss 0.918\n",
            "1 120 Loss 1.507\n",
            "1 121 Loss 0.716\n",
            "1 122 Loss 1.134\n",
            "1 123 Loss 0.802\n",
            "1 124 Loss 1.196\n",
            "1 125 Loss 1.123\n",
            "1 126 Loss 1.490\n",
            "1 127 Loss 0.788\n",
            "1 128 Loss 1.373\n",
            "1 129 Loss 1.115\n",
            "1 130 Loss 1.520\n",
            "1 131 Loss 1.446\n",
            "1 132 Loss 0.961\n",
            "1 133 Loss 1.843\n",
            "1 134 Loss 1.311\n",
            "1 135 Loss 1.251\n",
            "1 136 Loss 1.295\n",
            "1 137 Loss 1.426\n",
            "1 138 Loss 1.156\n",
            "1 139 Loss 1.098\n",
            "1 140 Loss 1.124\n",
            "1 141 Loss 1.540\n",
            "1 142 Loss 1.071\n",
            "1 143 Loss 1.176\n",
            "1 144 Loss 0.933\n",
            "1 145 Loss 1.039\n",
            "1 146 Loss 1.665\n",
            "1 147 Loss 1.323\n",
            "1 148 Loss 1.257\n",
            "1 149 Loss 1.197\n",
            "1 150 Loss 1.388\n",
            "1 151 Loss 1.280\n",
            "1 152 Loss 0.820\n",
            "1 153 Loss 1.229\n",
            "1 154 Loss 1.317\n",
            "1 155 Loss 1.318\n",
            "1 156 Loss 1.344\n",
            "1 157 Loss 1.112\n",
            "1 158 Loss 0.923\n",
            "1 159 Loss 1.030\n",
            "1 160 Loss 1.680\n",
            "1 161 Loss 1.051\n",
            "1 162 Loss 1.134\n",
            "1 163 Loss 1.780\n",
            "1 164 Loss 1.356\n",
            "1 165 Loss 1.644\n",
            "1 166 Loss 0.997\n",
            "1 167 Loss 0.761\n",
            "1 168 Loss 1.566\n",
            "1 169 Loss 1.094\n",
            "1 170 Loss 1.200\n",
            "1 171 Loss 1.396\n",
            "1 172 Loss 1.169\n",
            "1 173 Loss 1.285\n",
            "1 174 Loss 1.286\n",
            "1 175 Loss 0.929\n",
            "1 176 Loss 0.955\n",
            "1 177 Loss 0.905\n",
            "1 178 Loss 1.110\n",
            "1 179 Loss 1.477\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.75      0.84        32\n",
            "           1       0.88      0.25      0.39        28\n",
            "           2       0.81      0.59      0.68        29\n",
            "           3       0.87      0.54      0.67        37\n",
            "           4       0.62      0.84      0.71        31\n",
            "           5       0.59      0.86      0.70        35\n",
            "           6       0.45      0.92      0.61        37\n",
            "           7       0.54      0.44      0.49        43\n",
            "           8       0.00      0.00      0.00        38\n",
            "           9       0.55      0.88      0.68        50\n",
            "\n",
            "    accuracy                           0.61       360\n",
            "   macro avg       0.63      0.61      0.58       360\n",
            "weighted avg       0.61      0.61      0.57       360\n",
            "\n",
            "0.6138888888888889\n",
            "2 0 Loss 1.172\n",
            "2 1 Loss 0.799\n",
            "2 2 Loss 1.121\n",
            "2 3 Loss 1.272\n",
            "2 4 Loss 1.334\n",
            "2 5 Loss 1.489\n",
            "2 6 Loss 1.107\n",
            "2 7 Loss 1.198\n",
            "2 8 Loss 1.054\n",
            "2 9 Loss 1.575\n",
            "2 10 Loss 1.117\n",
            "2 11 Loss 1.161\n",
            "2 12 Loss 1.039\n",
            "2 13 Loss 0.632\n",
            "2 14 Loss 0.780\n",
            "2 15 Loss 1.199\n",
            "2 16 Loss 1.364\n",
            "2 17 Loss 1.262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.91      0.84      0.87        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.79      0.94      0.86        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.83      0.79      0.81        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.90      0.90       360\n",
            "weighted avg       0.90      0.90      0.90       360\n",
            "\n",
            "0.8972222222222223\n",
            "93 0 Loss 0.025\n",
            "93 1 Loss 0.014\n",
            "93 2 Loss 0.021\n",
            "93 3 Loss 0.027\n",
            "93 4 Loss 0.060\n",
            "93 5 Loss 0.022\n",
            "93 6 Loss 0.028\n",
            "93 7 Loss 0.016\n",
            "93 8 Loss 0.002\n",
            "93 9 Loss 0.183\n",
            "93 10 Loss 0.008\n",
            "93 11 Loss 0.005\n",
            "93 12 Loss 0.003\n",
            "93 13 Loss 0.011\n",
            "93 14 Loss 0.033\n",
            "93 15 Loss 0.013\n",
            "93 16 Loss 0.026\n",
            "93 17 Loss 0.139\n",
            "93 18 Loss 0.027\n",
            "93 19 Loss 0.010\n",
            "93 20 Loss 0.006\n",
            "93 21 Loss 0.015\n",
            "93 22 Loss 0.013\n",
            "93 23 Loss 0.012\n",
            "93 24 Loss 0.005\n",
            "93 25 Loss 0.023\n",
            "93 26 Loss 0.041\n",
            "93 27 Loss 0.014\n",
            "93 28 Loss 0.005\n",
            "93 29 Loss 0.036\n",
            "93 30 Loss 0.019\n",
            "93 31 Loss 0.004\n",
            "93 32 Loss 0.022\n",
            "93 33 Loss 0.029\n",
            "93 34 Loss 0.003\n",
            "93 35 Loss 0.013\n",
            "93 36 Loss 0.045\n",
            "93 37 Loss 0.017\n",
            "93 38 Loss 0.058\n",
            "93 39 Loss 0.046\n",
            "93 40 Loss 0.011\n",
            "93 41 Loss 0.006\n",
            "93 42 Loss 0.027\n",
            "93 43 Loss 0.007\n",
            "93 44 Loss 0.031\n",
            "93 45 Loss 0.020\n",
            "93 46 Loss 0.025\n",
            "93 47 Loss 0.007\n",
            "93 48 Loss 0.013\n",
            "93 49 Loss 0.016\n",
            "93 50 Loss 0.017\n",
            "93 51 Loss 0.020\n",
            "93 52 Loss 0.053\n",
            "93 53 Loss 0.004\n",
            "93 54 Loss 0.016\n",
            "93 55 Loss 0.011\n",
            "93 56 Loss 0.004\n",
            "93 57 Loss 0.025\n",
            "93 58 Loss 0.031\n",
            "93 59 Loss 0.036\n",
            "93 60 Loss 0.034\n",
            "93 61 Loss 0.003\n",
            "93 62 Loss 0.005\n",
            "93 63 Loss 0.065\n",
            "93 64 Loss 0.025\n",
            "93 65 Loss 0.020\n",
            "93 66 Loss 0.008\n",
            "93 67 Loss 0.039\n",
            "93 68 Loss 0.022\n",
            "93 69 Loss 0.008\n",
            "93 70 Loss 0.007\n",
            "93 71 Loss 0.072\n",
            "93 72 Loss 0.008\n",
            "93 73 Loss 0.021\n",
            "93 74 Loss 0.013\n",
            "93 75 Loss 0.049\n",
            "93 76 Loss 0.009\n",
            "93 77 Loss 0.016\n",
            "93 78 Loss 0.029\n",
            "93 79 Loss 0.012\n",
            "93 80 Loss 0.008\n",
            "93 81 Loss 0.052\n",
            "93 82 Loss 0.039\n",
            "93 83 Loss 0.023\n",
            "93 84 Loss 0.017\n",
            "93 85 Loss 0.042\n",
            "93 86 Loss 0.042\n",
            "93 87 Loss 0.211\n",
            "93 88 Loss 0.016\n",
            "93 89 Loss 0.032\n",
            "93 90 Loss 0.022\n",
            "93 91 Loss 0.012\n",
            "93 92 Loss 0.030\n",
            "93 93 Loss 0.019\n",
            "93 94 Loss 0.019\n",
            "93 95 Loss 0.013\n",
            "93 96 Loss 0.008\n",
            "93 97 Loss 0.022\n",
            "93 98 Loss 0.083\n",
            "93 99 Loss 0.009\n",
            "93 100 Loss 0.006\n",
            "93 101 Loss 0.099\n",
            "93 102 Loss 0.036\n",
            "93 103 Loss 0.010\n",
            "93 104 Loss 0.010\n",
            "93 105 Loss 0.013\n",
            "93 106 Loss 0.033\n",
            "93 107 Loss 0.015\n",
            "93 108 Loss 0.054\n",
            "93 109 Loss 0.024\n",
            "93 110 Loss 0.027\n",
            "93 111 Loss 0.051\n",
            "93 112 Loss 0.017\n",
            "93 113 Loss 0.014\n",
            "93 114 Loss 0.012\n",
            "93 115 Loss 0.018\n",
            "93 116 Loss 0.006\n",
            "93 117 Loss 0.023\n",
            "93 118 Loss 0.091\n",
            "93 119 Loss 0.017\n",
            "93 120 Loss 0.013\n",
            "93 121 Loss 0.008\n",
            "93 122 Loss 0.006\n",
            "93 123 Loss 0.018\n",
            "93 124 Loss 0.020\n",
            "93 125 Loss 0.016\n",
            "93 126 Loss 0.048\n",
            "93 127 Loss 0.063\n",
            "93 128 Loss 0.051\n",
            "93 129 Loss 0.016\n",
            "93 130 Loss 0.208\n",
            "93 131 Loss 0.027\n",
            "93 132 Loss 0.011\n",
            "93 133 Loss 0.005\n",
            "93 134 Loss 0.040\n",
            "93 135 Loss 0.025\n",
            "93 136 Loss 0.020\n",
            "93 137 Loss 0.056\n",
            "93 138 Loss 0.026\n",
            "93 139 Loss 0.073\n",
            "93 140 Loss 0.018\n",
            "93 141 Loss 0.008\n",
            "93 142 Loss 0.025\n",
            "93 143 Loss 0.008\n",
            "93 144 Loss 0.024\n",
            "93 145 Loss 0.006\n",
            "93 146 Loss 0.011\n",
            "93 147 Loss 0.013\n",
            "93 148 Loss 0.045\n",
            "93 149 Loss 0.008\n",
            "93 150 Loss 0.058\n",
            "93 151 Loss 0.022\n",
            "93 152 Loss 0.003\n",
            "93 153 Loss 0.009\n",
            "93 154 Loss 0.009\n",
            "93 155 Loss 0.034\n",
            "93 156 Loss 0.040\n",
            "93 157 Loss 0.009\n",
            "93 158 Loss 0.090\n",
            "93 159 Loss 0.126\n",
            "93 160 Loss 0.072\n",
            "93 161 Loss 0.036\n",
            "93 162 Loss 0.016\n",
            "93 163 Loss 0.009\n",
            "93 164 Loss 0.007\n",
            "93 165 Loss 0.008\n",
            "93 166 Loss 0.024\n",
            "93 167 Loss 0.009\n",
            "93 168 Loss 0.004\n",
            "93 169 Loss 0.042\n",
            "93 170 Loss 0.009\n",
            "93 171 Loss 0.006\n",
            "93 172 Loss 0.030\n",
            "93 173 Loss 0.011\n",
            "93 174 Loss 0.018\n",
            "93 175 Loss 0.050\n",
            "93 176 Loss 0.008\n",
            "93 177 Loss 0.020\n",
            "93 178 Loss 0.006\n",
            "93 179 Loss 0.023\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.84      0.96      0.90        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.79      0.94      0.86        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.90      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "94 0 Loss 0.048\n",
            "94 1 Loss 0.029\n",
            "94 2 Loss 0.019\n",
            "94 3 Loss 0.050\n",
            "94 4 Loss 0.051\n",
            "94 5 Loss 0.009\n",
            "94 6 Loss 0.015\n",
            "94 7 Loss 0.005\n",
            "94 8 Loss 0.049\n",
            "94 9 Loss 0.020\n",
            "94 10 Loss 0.036\n",
            "94 11 Loss 0.003\n",
            "94 12 Loss 0.033\n",
            "94 13 Loss 0.011\n",
            "94 14 Loss 0.027\n",
            "94 15 Loss 0.048\n",
            "94 16 Loss 0.058\n",
            "94 17 Loss 0.115\n",
            "94 18 Loss 0.028\n",
            "94 19 Loss 0.021\n",
            "94 20 Loss 0.014\n",
            "94 21 Loss 0.025\n",
            "94 22 Loss 0.065\n",
            "94 23 Loss 0.006\n",
            "94 24 Loss 0.004\n",
            "94 25 Loss 0.020\n",
            "94 26 Loss 0.019\n",
            "94 27 Loss 0.018\n",
            "94 28 Loss 0.021\n",
            "94 29 Loss 0.019\n",
            "94 30 Loss 0.005\n",
            "94 31 Loss 0.008\n",
            "94 32 Loss 0.009\n",
            "94 33 Loss 0.009\n",
            "94 34 Loss 0.005\n",
            "94 35 Loss 0.005\n",
            "94 36 Loss 0.021\n",
            "94 37 Loss 0.011\n",
            "94 38 Loss 0.033\n",
            "94 39 Loss 0.005\n",
            "94 40 Loss 0.065\n",
            "94 41 Loss 0.005\n",
            "94 42 Loss 0.015\n",
            "94 43 Loss 0.024\n",
            "94 44 Loss 0.015\n",
            "94 45 Loss 0.003\n",
            "94 46 Loss 0.004\n",
            "94 47 Loss 0.013\n",
            "94 48 Loss 0.021\n",
            "94 49 Loss 0.017\n",
            "94 50 Loss 0.020\n",
            "94 51 Loss 0.023\n",
            "94 52 Loss 0.023\n",
            "94 53 Loss 0.025\n",
            "94 54 Loss 0.016\n",
            "94 55 Loss 0.031\n",
            "94 56 Loss 0.013\n",
            "94 57 Loss 0.081\n",
            "94 58 Loss 0.024\n",
            "94 59 Loss 0.015\n",
            "94 60 Loss 0.002\n",
            "94 61 Loss 0.095\n",
            "94 62 Loss 0.015\n",
            "94 63 Loss 0.118\n",
            "94 64 Loss 0.089\n",
            "94 65 Loss 0.018\n",
            "94 66 Loss 0.027\n",
            "94 67 Loss 0.003\n",
            "94 68 Loss 0.028\n",
            "94 69 Loss 0.052\n",
            "94 70 Loss 0.012\n",
            "94 71 Loss 0.014\n",
            "94 72 Loss 0.026\n",
            "94 73 Loss 0.010\n",
            "94 74 Loss 0.037\n",
            "94 75 Loss 0.088\n",
            "94 76 Loss 0.003\n",
            "94 77 Loss 0.121\n",
            "94 78 Loss 0.069\n",
            "94 79 Loss 0.007\n",
            "94 80 Loss 0.024\n",
            "94 81 Loss 0.014\n",
            "94 82 Loss 0.044\n",
            "94 83 Loss 0.011\n",
            "94 84 Loss 0.031\n",
            "94 85 Loss 0.022\n",
            "94 86 Loss 0.039\n",
            "94 87 Loss 0.012\n",
            "94 88 Loss 0.010\n",
            "94 89 Loss 0.113\n",
            "94 90 Loss 0.036\n",
            "94 91 Loss 0.016\n",
            "94 92 Loss 0.006\n",
            "94 93 Loss 0.009\n",
            "94 94 Loss 0.007\n",
            "94 95 Loss 0.009\n",
            "94 96 Loss 0.010\n",
            "94 97 Loss 0.017\n",
            "94 98 Loss 0.035\n",
            "94 99 Loss 0.017\n",
            "94 100 Loss 0.149\n",
            "94 101 Loss 0.023\n",
            "94 102 Loss 0.006\n",
            "94 103 Loss 0.006\n",
            "94 104 Loss 0.025\n",
            "94 105 Loss 0.208\n",
            "94 106 Loss 0.025\n",
            "94 107 Loss 0.005\n",
            "94 108 Loss 0.006\n",
            "94 109 Loss 0.017\n",
            "94 110 Loss 0.031\n",
            "94 111 Loss 0.017\n",
            "94 112 Loss 0.021\n",
            "94 113 Loss 0.030\n",
            "94 114 Loss 0.018\n",
            "94 115 Loss 0.011\n",
            "94 116 Loss 0.006\n",
            "94 117 Loss 0.011\n",
            "94 118 Loss 0.034\n",
            "94 119 Loss 0.016\n",
            "94 120 Loss 0.004\n",
            "94 121 Loss 0.017\n",
            "94 122 Loss 0.012\n",
            "94 123 Loss 0.098\n",
            "94 124 Loss 0.017\n",
            "94 125 Loss 0.008\n",
            "94 126 Loss 0.011\n",
            "94 127 Loss 0.066\n",
            "94 128 Loss 0.008\n",
            "94 129 Loss 0.014\n",
            "94 130 Loss 0.001\n",
            "94 131 Loss 0.016\n",
            "94 132 Loss 0.021\n",
            "94 133 Loss 0.014\n",
            "94 134 Loss 0.050\n",
            "94 135 Loss 0.016\n",
            "94 136 Loss 0.012\n",
            "94 137 Loss 0.017\n",
            "94 138 Loss 0.005\n",
            "94 139 Loss 0.035\n",
            "94 140 Loss 0.017\n",
            "94 141 Loss 0.004\n",
            "94 142 Loss 0.020\n",
            "94 143 Loss 0.015\n",
            "94 144 Loss 0.008\n",
            "94 145 Loss 0.033\n",
            "94 146 Loss 0.048\n",
            "94 147 Loss 0.006\n",
            "94 148 Loss 0.052\n",
            "94 149 Loss 0.076\n",
            "94 150 Loss 0.061\n",
            "94 151 Loss 0.007\n",
            "94 152 Loss 0.016\n",
            "94 153 Loss 0.020\n",
            "94 154 Loss 0.010\n",
            "94 155 Loss 0.047\n",
            "94 156 Loss 0.055\n",
            "94 157 Loss 0.013\n",
            "94 158 Loss 0.046\n",
            "94 159 Loss 0.007\n",
            "94 160 Loss 0.016\n",
            "94 161 Loss 0.007\n",
            "94 162 Loss 0.054\n",
            "94 163 Loss 0.030\n",
            "94 164 Loss 0.012\n",
            "94 165 Loss 0.028\n",
            "94 166 Loss 0.016\n",
            "94 167 Loss 0.004\n",
            "94 168 Loss 0.008\n",
            "94 169 Loss 0.015\n",
            "94 170 Loss 0.085\n",
            "94 171 Loss 0.013\n",
            "94 172 Loss 0.007\n",
            "94 173 Loss 0.009\n",
            "94 174 Loss 0.024\n",
            "94 175 Loss 0.015\n",
            "94 176 Loss 0.004\n",
            "94 177 Loss 0.006\n",
            "94 178 Loss 0.012\n",
            "94 179 Loss 0.178\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.94      0.97      0.95        31\n",
            "           5       0.79      0.94      0.86        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.95      0.81      0.88        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.86      0.91        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9\n",
            "95 0 Loss 0.003\n",
            "95 1 Loss 0.021\n",
            "95 2 Loss 0.007\n",
            "95 3 Loss 0.079\n",
            "95 4 Loss 0.248\n",
            "95 5 Loss 0.026\n",
            "95 6 Loss 0.004\n",
            "95 7 Loss 0.019\n",
            "95 8 Loss 0.078\n",
            "95 9 Loss 0.051\n",
            "95 10 Loss 0.019\n",
            "95 11 Loss 0.014\n",
            "95 12 Loss 0.009\n",
            "95 13 Loss 0.018\n",
            "95 14 Loss 0.008\n",
            "95 15 Loss 0.013\n",
            "95 16 Loss 0.011\n",
            "95 17 Loss 0.018\n",
            "95 18 Loss 0.023\n",
            "95 19 Loss 0.035\n",
            "95 20 Loss 0.139\n",
            "95 21 Loss 0.024\n",
            "95 22 Loss 0.014\n",
            "95 23 Loss 0.009\n",
            "95 24 Loss 0.011\n",
            "95 25 Loss 0.078\n",
            "95 26 Loss 0.024\n",
            "95 27 Loss 0.007\n",
            "95 28 Loss 0.004\n",
            "95 29 Loss 0.025\n",
            "95 30 Loss 0.003\n",
            "95 31 Loss 0.075\n",
            "95 32 Loss 0.033\n",
            "95 33 Loss 0.015\n",
            "95 34 Loss 0.010\n",
            "95 35 Loss 0.003\n",
            "95 36 Loss 0.038\n",
            "95 37 Loss 0.016\n",
            "95 38 Loss 0.010\n",
            "95 39 Loss 0.029\n",
            "95 40 Loss 0.016\n",
            "95 41 Loss 0.085\n",
            "95 42 Loss 0.044\n",
            "95 43 Loss 0.031\n",
            "95 44 Loss 0.008\n",
            "95 45 Loss 0.046\n",
            "95 46 Loss 0.028\n",
            "95 47 Loss 0.002\n",
            "95 48 Loss 0.057\n",
            "95 49 Loss 0.017\n",
            "95 50 Loss 0.007\n",
            "95 51 Loss 0.011\n",
            "95 52 Loss 0.006\n",
            "95 53 Loss 0.025\n",
            "95 54 Loss 0.013\n",
            "95 55 Loss 0.012\n",
            "95 56 Loss 0.047\n",
            "95 57 Loss 0.007\n",
            "95 58 Loss 0.004\n",
            "95 59 Loss 0.006\n",
            "95 60 Loss 0.040\n",
            "95 61 Loss 0.008\n",
            "95 62 Loss 0.015\n",
            "95 63 Loss 0.014\n",
            "95 64 Loss 0.030\n",
            "95 65 Loss 0.005\n",
            "95 66 Loss 0.031\n",
            "95 67 Loss 0.043\n",
            "95 68 Loss 0.081\n",
            "95 69 Loss 0.010\n",
            "95 70 Loss 0.031\n",
            "95 71 Loss 0.003\n",
            "95 72 Loss 0.006\n",
            "95 73 Loss 0.023\n",
            "95 74 Loss 0.005\n",
            "95 75 Loss 0.027\n",
            "95 76 Loss 0.013\n",
            "95 77 Loss 0.008\n",
            "95 78 Loss 0.052\n",
            "95 79 Loss 0.041\n",
            "95 80 Loss 0.010\n",
            "95 81 Loss 0.057\n",
            "95 82 Loss 0.034\n",
            "95 83 Loss 0.056\n",
            "95 84 Loss 0.004\n",
            "95 85 Loss 0.048\n",
            "95 86 Loss 0.004\n",
            "95 87 Loss 0.023\n",
            "95 88 Loss 0.012\n",
            "95 89 Loss 0.044\n",
            "95 90 Loss 0.051\n",
            "95 91 Loss 0.005\n",
            "95 92 Loss 0.077\n",
            "95 93 Loss 0.010\n",
            "95 94 Loss 0.014\n",
            "95 95 Loss 0.025\n",
            "95 96 Loss 0.028\n",
            "95 97 Loss 0.003\n",
            "95 98 Loss 0.013\n",
            "95 99 Loss 0.011\n",
            "95 100 Loss 0.010\n",
            "95 101 Loss 0.013\n",
            "95 102 Loss 0.037\n",
            "95 103 Loss 0.022\n",
            "95 104 Loss 0.019\n",
            "95 105 Loss 0.035\n",
            "95 106 Loss 0.018\n",
            "95 107 Loss 0.007\n",
            "95 108 Loss 0.047\n",
            "95 109 Loss 0.045\n",
            "95 110 Loss 0.034\n",
            "95 111 Loss 0.023\n",
            "95 112 Loss 0.018\n",
            "95 113 Loss 0.102\n",
            "95 114 Loss 0.014\n",
            "95 115 Loss 0.049\n",
            "95 116 Loss 0.006\n",
            "95 117 Loss 0.046\n",
            "95 118 Loss 0.015\n",
            "95 119 Loss 0.027\n",
            "95 120 Loss 0.009\n",
            "95 121 Loss 0.013\n",
            "95 122 Loss 0.015\n",
            "95 123 Loss 0.031\n",
            "95 124 Loss 0.010\n",
            "95 125 Loss 0.068\n",
            "95 126 Loss 0.006\n",
            "95 127 Loss 0.011\n",
            "95 128 Loss 0.010\n",
            "95 129 Loss 0.016\n",
            "95 130 Loss 0.023\n",
            "95 131 Loss 0.012\n",
            "95 132 Loss 0.033\n",
            "95 133 Loss 0.205\n",
            "95 134 Loss 0.054\n",
            "95 135 Loss 0.012\n",
            "95 136 Loss 0.072\n",
            "95 137 Loss 0.023\n",
            "95 138 Loss 0.057\n",
            "95 139 Loss 0.006\n",
            "95 140 Loss 0.027\n",
            "95 141 Loss 0.040\n",
            "95 142 Loss 0.002\n",
            "95 143 Loss 0.036\n",
            "95 144 Loss 0.023\n",
            "95 145 Loss 0.006\n",
            "95 146 Loss 0.010\n",
            "95 147 Loss 0.019\n",
            "95 148 Loss 0.004\n",
            "95 149 Loss 0.015\n",
            "95 150 Loss 0.013\n",
            "95 151 Loss 0.010\n",
            "95 152 Loss 0.008\n",
            "95 153 Loss 0.005\n",
            "95 154 Loss 0.011\n",
            "95 155 Loss 0.036\n",
            "95 156 Loss 0.013\n",
            "95 157 Loss 0.017\n",
            "95 158 Loss 0.006\n",
            "95 159 Loss 0.016\n",
            "95 160 Loss 0.007\n",
            "95 161 Loss 0.057\n",
            "95 162 Loss 0.003\n",
            "95 163 Loss 0.016\n",
            "95 164 Loss 0.027\n",
            "95 165 Loss 0.024\n",
            "95 166 Loss 0.019\n",
            "95 167 Loss 0.017\n",
            "95 168 Loss 0.009\n",
            "95 169 Loss 0.082\n",
            "95 170 Loss 0.015\n",
            "95 171 Loss 0.004\n",
            "95 172 Loss 0.012\n",
            "95 173 Loss 0.014\n",
            "95 174 Loss 0.006\n",
            "95 175 Loss 0.033\n",
            "95 176 Loss 0.026\n",
            "95 177 Loss 0.006\n",
            "95 178 Loss 0.002\n",
            "95 179 Loss 0.014\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.79      0.96      0.87        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.81      0.88        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.79      0.94      0.86        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.90      0.84      0.87        43\n",
            "           8       0.85      0.76      0.81        38\n",
            "           9       0.94      0.90      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.90      0.90       360\n",
            "weighted avg       0.90      0.90      0.90       360\n",
            "\n",
            "0.8972222222222223\n",
            "96 0 Loss 0.020\n",
            "96 1 Loss 0.012\n",
            "96 2 Loss 0.066\n",
            "96 3 Loss 0.061\n",
            "96 4 Loss 0.037\n",
            "96 5 Loss 0.008\n",
            "96 6 Loss 0.040\n",
            "96 7 Loss 0.127\n",
            "96 8 Loss 0.040\n",
            "96 9 Loss 0.031\n",
            "96 10 Loss 0.008\n",
            "96 11 Loss 0.009\n",
            "96 12 Loss 0.022\n",
            "96 13 Loss 0.007\n",
            "96 14 Loss 0.010\n",
            "96 15 Loss 0.029\n",
            "96 16 Loss 0.039\n",
            "96 17 Loss 0.005\n",
            "96 18 Loss 0.027\n",
            "96 19 Loss 0.011\n",
            "96 20 Loss 0.029\n",
            "96 21 Loss 0.008\n",
            "96 22 Loss 0.035\n",
            "96 23 Loss 0.009\n",
            "96 24 Loss 0.023\n",
            "96 25 Loss 0.006\n",
            "96 26 Loss 0.139\n",
            "96 27 Loss 0.005\n",
            "96 28 Loss 0.019\n",
            "96 29 Loss 0.003\n",
            "96 30 Loss 0.008\n",
            "96 31 Loss 0.007\n",
            "96 32 Loss 0.075\n",
            "96 33 Loss 0.015\n",
            "96 34 Loss 0.156\n",
            "96 35 Loss 0.040\n",
            "96 36 Loss 0.010\n",
            "96 37 Loss 0.011\n",
            "96 38 Loss 0.013\n",
            "96 39 Loss 0.006\n",
            "96 40 Loss 0.011\n",
            "96 41 Loss 0.012\n",
            "96 42 Loss 0.023\n",
            "96 43 Loss 0.010\n",
            "96 44 Loss 0.020\n",
            "96 45 Loss 0.005\n",
            "96 46 Loss 0.018\n",
            "96 47 Loss 0.015\n",
            "96 48 Loss 0.018\n",
            "96 49 Loss 0.006\n",
            "96 50 Loss 0.010\n",
            "96 51 Loss 0.011\n",
            "96 52 Loss 0.018\n",
            "96 53 Loss 0.020\n",
            "96 54 Loss 0.015\n",
            "96 55 Loss 0.015\n",
            "96 56 Loss 0.022\n",
            "96 57 Loss 0.014\n",
            "96 58 Loss 0.048\n",
            "96 59 Loss 0.028\n",
            "96 60 Loss 0.011\n",
            "96 61 Loss 0.020\n",
            "96 62 Loss 0.020\n",
            "96 63 Loss 0.002\n",
            "96 64 Loss 0.016\n",
            "96 65 Loss 0.006\n",
            "96 66 Loss 0.009\n",
            "96 67 Loss 0.057\n",
            "96 68 Loss 0.018\n",
            "96 69 Loss 0.026\n",
            "96 70 Loss 0.020\n",
            "96 71 Loss 0.077\n",
            "96 72 Loss 0.060\n",
            "96 73 Loss 0.031\n",
            "96 74 Loss 0.005\n",
            "96 75 Loss 0.007\n",
            "96 76 Loss 0.002\n",
            "96 77 Loss 0.015\n",
            "96 78 Loss 0.007\n",
            "96 79 Loss 0.018\n",
            "96 80 Loss 0.006\n",
            "96 81 Loss 0.009\n",
            "96 82 Loss 0.057\n",
            "96 83 Loss 0.116\n",
            "96 84 Loss 0.012\n",
            "96 85 Loss 0.053\n",
            "96 86 Loss 0.003\n",
            "96 87 Loss 0.014\n",
            "96 88 Loss 0.017\n",
            "96 89 Loss 0.016\n",
            "96 90 Loss 0.024\n",
            "96 91 Loss 0.002\n",
            "96 92 Loss 0.051\n",
            "96 93 Loss 0.006\n",
            "96 94 Loss 0.075\n",
            "96 95 Loss 0.050\n",
            "96 96 Loss 0.031\n",
            "96 97 Loss 0.026\n",
            "96 98 Loss 0.005\n",
            "96 99 Loss 0.022\n",
            "96 100 Loss 0.055\n",
            "96 101 Loss 0.111\n",
            "96 102 Loss 0.011\n",
            "96 103 Loss 0.041\n",
            "96 104 Loss 0.009\n",
            "96 105 Loss 0.016\n",
            "96 106 Loss 0.023\n",
            "96 107 Loss 0.012\n",
            "96 108 Loss 0.022\n",
            "96 109 Loss 0.028\n",
            "96 110 Loss 0.003\n",
            "96 111 Loss 0.064\n",
            "96 112 Loss 0.048\n",
            "96 113 Loss 0.015\n",
            "96 114 Loss 0.112\n",
            "96 115 Loss 0.019\n",
            "96 116 Loss 0.003\n",
            "96 117 Loss 0.017\n",
            "96 118 Loss 0.006\n",
            "96 119 Loss 0.046\n",
            "96 120 Loss 0.026\n",
            "96 121 Loss 0.016\n",
            "96 122 Loss 0.016\n",
            "96 123 Loss 0.010\n",
            "96 124 Loss 0.026\n",
            "96 125 Loss 0.004\n",
            "96 126 Loss 0.006\n",
            "96 127 Loss 0.008\n",
            "96 128 Loss 0.017\n",
            "96 129 Loss 0.006\n",
            "96 130 Loss 0.008\n",
            "96 131 Loss 0.057\n",
            "96 132 Loss 0.003\n",
            "96 133 Loss 0.039\n",
            "96 134 Loss 0.018\n",
            "96 135 Loss 0.013\n",
            "96 136 Loss 0.008\n",
            "96 137 Loss 0.077\n",
            "96 138 Loss 0.011\n",
            "96 139 Loss 0.055\n",
            "96 140 Loss 0.021\n",
            "96 141 Loss 0.051\n",
            "96 142 Loss 0.034\n",
            "96 143 Loss 0.017\n",
            "96 144 Loss 0.052\n",
            "96 145 Loss 0.027\n",
            "96 146 Loss 0.019\n",
            "96 147 Loss 0.021\n",
            "96 148 Loss 0.017\n",
            "96 149 Loss 0.011\n",
            "96 150 Loss 0.004\n",
            "96 151 Loss 0.010\n",
            "96 152 Loss 0.013\n",
            "96 153 Loss 0.023\n",
            "96 154 Loss 0.030\n",
            "96 155 Loss 0.076\n",
            "96 156 Loss 0.155\n",
            "96 157 Loss 0.062\n",
            "96 158 Loss 0.005\n",
            "96 159 Loss 0.033\n",
            "96 160 Loss 0.008\n",
            "96 161 Loss 0.017\n",
            "96 162 Loss 0.015\n",
            "96 163 Loss 0.009\n",
            "96 164 Loss 0.014\n",
            "96 165 Loss 0.015\n",
            "96 166 Loss 0.002\n",
            "96 167 Loss 0.025\n",
            "96 168 Loss 0.005\n",
            "96 169 Loss 0.016\n",
            "96 170 Loss 0.043\n",
            "96 171 Loss 0.019\n",
            "96 172 Loss 0.004\n",
            "96 173 Loss 0.083\n",
            "96 174 Loss 0.048\n",
            "96 175 Loss 0.025\n",
            "96 176 Loss 0.010\n",
            "96 177 Loss 0.042\n",
            "96 178 Loss 0.005\n",
            "96 179 Loss 0.013\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.89      0.93        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.82      0.94      0.88        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.90      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9083333333333333\n",
            "97 0 Loss 0.004\n",
            "97 1 Loss 0.038\n",
            "97 2 Loss 0.002\n",
            "97 3 Loss 0.007\n",
            "97 4 Loss 0.011\n",
            "97 5 Loss 0.003\n",
            "97 6 Loss 0.062\n",
            "97 7 Loss 0.026\n",
            "97 8 Loss 0.004\n",
            "97 9 Loss 0.039\n",
            "97 10 Loss 0.002\n",
            "97 11 Loss 0.009\n",
            "97 12 Loss 0.008\n",
            "97 13 Loss 0.021\n",
            "97 14 Loss 0.008\n",
            "97 15 Loss 0.008\n",
            "97 16 Loss 0.037\n",
            "97 17 Loss 0.021\n",
            "97 18 Loss 0.009\n",
            "97 19 Loss 0.002\n",
            "97 20 Loss 0.014\n",
            "97 21 Loss 0.051\n",
            "97 22 Loss 0.008\n",
            "97 23 Loss 0.031\n",
            "97 24 Loss 0.010\n",
            "97 25 Loss 0.031\n",
            "97 26 Loss 0.009\n",
            "97 27 Loss 0.041\n",
            "97 28 Loss 0.023\n",
            "97 29 Loss 0.010\n",
            "97 30 Loss 0.037\n",
            "97 31 Loss 0.023\n",
            "97 32 Loss 0.005\n",
            "97 33 Loss 0.037\n",
            "97 34 Loss 0.007\n",
            "97 35 Loss 0.025\n",
            "97 36 Loss 0.032\n",
            "97 37 Loss 0.039\n",
            "97 38 Loss 0.015\n",
            "97 39 Loss 0.006\n",
            "97 40 Loss 0.040\n",
            "97 41 Loss 0.011\n",
            "97 42 Loss 0.015\n",
            "97 43 Loss 0.007\n",
            "97 44 Loss 0.036\n",
            "97 45 Loss 0.012\n",
            "97 46 Loss 0.017\n",
            "97 47 Loss 0.014\n",
            "97 48 Loss 0.003\n",
            "97 49 Loss 0.021\n",
            "97 50 Loss 0.007\n",
            "97 51 Loss 0.008\n",
            "97 52 Loss 0.032\n",
            "97 53 Loss 0.013\n",
            "97 54 Loss 0.007\n",
            "97 55 Loss 0.012\n",
            "97 56 Loss 0.008\n",
            "97 57 Loss 0.092\n",
            "97 58 Loss 0.038\n",
            "97 59 Loss 0.017\n",
            "97 60 Loss 0.007\n",
            "97 61 Loss 0.039\n",
            "97 62 Loss 0.010\n",
            "97 63 Loss 0.090\n",
            "97 64 Loss 0.027\n",
            "97 65 Loss 0.010\n",
            "97 66 Loss 0.009\n",
            "97 67 Loss 0.016\n",
            "97 68 Loss 0.011\n",
            "97 69 Loss 0.044\n",
            "97 70 Loss 0.028\n",
            "97 71 Loss 0.059\n",
            "97 72 Loss 0.036\n",
            "97 73 Loss 0.019\n",
            "97 74 Loss 0.039\n",
            "97 75 Loss 0.004\n",
            "97 76 Loss 0.040\n",
            "97 77 Loss 0.004\n",
            "97 78 Loss 0.056\n",
            "97 79 Loss 0.005\n",
            "97 80 Loss 0.007\n",
            "97 81 Loss 0.003\n",
            "97 82 Loss 0.034\n",
            "97 83 Loss 0.013\n",
            "97 84 Loss 0.029\n",
            "97 85 Loss 0.012\n",
            "97 86 Loss 0.006\n",
            "97 87 Loss 0.023\n",
            "97 88 Loss 0.011\n",
            "97 89 Loss 0.013\n",
            "97 90 Loss 0.034\n",
            "97 91 Loss 0.041\n",
            "97 92 Loss 0.016\n",
            "97 93 Loss 0.203\n",
            "97 94 Loss 0.018\n",
            "97 95 Loss 0.013\n",
            "97 96 Loss 0.018\n",
            "97 97 Loss 0.007\n",
            "97 98 Loss 0.039\n",
            "97 99 Loss 0.012\n",
            "97 100 Loss 0.060\n",
            "97 101 Loss 0.169\n",
            "97 102 Loss 0.016\n",
            "97 103 Loss 0.012\n",
            "97 104 Loss 0.012\n",
            "97 105 Loss 0.006\n",
            "97 106 Loss 0.017\n",
            "97 107 Loss 0.060\n",
            "97 108 Loss 0.005\n",
            "97 109 Loss 0.123\n",
            "97 110 Loss 0.006\n",
            "97 111 Loss 0.087\n",
            "97 112 Loss 0.026\n",
            "97 113 Loss 0.040\n",
            "97 114 Loss 0.019\n",
            "97 115 Loss 0.068\n",
            "97 116 Loss 0.023\n",
            "97 117 Loss 0.036\n",
            "97 118 Loss 0.057\n",
            "97 119 Loss 0.016\n",
            "97 120 Loss 0.021\n",
            "97 121 Loss 0.025\n",
            "97 122 Loss 0.036\n",
            "97 123 Loss 0.005\n",
            "97 124 Loss 0.017\n",
            "97 125 Loss 0.012\n",
            "97 126 Loss 0.038\n",
            "97 127 Loss 0.016\n",
            "97 128 Loss 0.049\n",
            "97 129 Loss 0.018\n",
            "97 130 Loss 0.009\n",
            "97 131 Loss 0.006\n",
            "97 132 Loss 0.032\n",
            "97 133 Loss 0.025\n",
            "97 134 Loss 0.012\n",
            "97 135 Loss 0.026\n",
            "97 136 Loss 0.004\n",
            "97 137 Loss 0.063\n",
            "97 138 Loss 0.013\n",
            "97 139 Loss 0.026\n",
            "97 140 Loss 0.023\n",
            "97 141 Loss 0.023\n",
            "97 142 Loss 0.012\n",
            "97 143 Loss 0.026\n",
            "97 144 Loss 0.028\n",
            "97 145 Loss 0.011\n",
            "97 146 Loss 0.016\n",
            "97 147 Loss 0.043\n",
            "97 148 Loss 0.003\n",
            "97 149 Loss 0.008\n",
            "97 150 Loss 0.017\n",
            "97 151 Loss 0.010\n",
            "97 152 Loss 0.034\n",
            "97 153 Loss 0.003\n",
            "97 154 Loss 0.017\n",
            "97 155 Loss 0.044\n",
            "97 156 Loss 0.048\n",
            "97 157 Loss 0.024\n",
            "97 158 Loss 0.048\n",
            "97 159 Loss 0.033\n",
            "97 160 Loss 0.017\n",
            "97 161 Loss 0.020\n",
            "97 162 Loss 0.017\n",
            "97 163 Loss 0.016\n",
            "97 164 Loss 0.010\n",
            "97 165 Loss 0.010\n",
            "97 166 Loss 0.098\n",
            "97 167 Loss 0.009\n",
            "97 168 Loss 0.037\n",
            "97 169 Loss 0.017\n",
            "97 170 Loss 0.015\n",
            "97 171 Loss 0.009\n",
            "97 172 Loss 0.047\n",
            "97 173 Loss 0.034\n",
            "97 174 Loss 0.015\n",
            "97 175 Loss 0.012\n",
            "97 176 Loss 0.044\n",
            "97 177 Loss 0.091\n",
            "97 178 Loss 0.110\n",
            "97 179 Loss 0.002\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.86      0.79      0.82        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "98 0 Loss 0.026\n",
            "98 1 Loss 0.008\n",
            "98 2 Loss 0.015\n",
            "98 3 Loss 0.029\n",
            "98 4 Loss 0.006\n",
            "98 5 Loss 0.086\n",
            "98 6 Loss 0.003\n",
            "98 7 Loss 0.031\n",
            "98 8 Loss 0.020\n",
            "98 9 Loss 0.025\n",
            "98 10 Loss 0.006\n",
            "98 11 Loss 0.043\n",
            "98 12 Loss 0.041\n",
            "98 13 Loss 0.006\n",
            "98 14 Loss 0.014\n",
            "98 15 Loss 0.013\n",
            "98 16 Loss 0.027\n",
            "98 17 Loss 0.007\n",
            "98 18 Loss 0.005\n",
            "98 19 Loss 0.016\n",
            "98 20 Loss 0.044\n",
            "98 21 Loss 0.017\n",
            "98 22 Loss 0.011\n",
            "98 23 Loss 0.048\n",
            "98 24 Loss 0.052\n",
            "98 25 Loss 0.002\n",
            "98 26 Loss 0.017\n",
            "98 27 Loss 0.008\n",
            "98 28 Loss 0.020\n",
            "98 29 Loss 0.043\n",
            "98 30 Loss 0.021\n",
            "98 31 Loss 0.061\n",
            "98 32 Loss 0.007\n",
            "98 33 Loss 0.119\n",
            "98 34 Loss 0.014\n",
            "98 35 Loss 0.052\n",
            "98 36 Loss 0.019\n",
            "98 37 Loss 0.012\n",
            "98 38 Loss 0.033\n",
            "98 39 Loss 0.010\n",
            "98 40 Loss 0.007\n",
            "98 41 Loss 0.081\n",
            "98 42 Loss 0.007\n",
            "98 43 Loss 0.008\n",
            "98 44 Loss 0.028\n",
            "98 45 Loss 0.009\n",
            "98 46 Loss 0.003\n",
            "98 47 Loss 0.014\n",
            "98 48 Loss 0.071\n",
            "98 49 Loss 0.028\n",
            "98 50 Loss 0.059\n",
            "98 51 Loss 0.006\n",
            "98 52 Loss 0.009\n",
            "98 53 Loss 0.016\n",
            "98 54 Loss 0.011\n",
            "98 55 Loss 0.014\n",
            "98 56 Loss 0.011\n",
            "98 57 Loss 0.038\n",
            "98 58 Loss 0.056\n",
            "98 59 Loss 0.003\n",
            "98 60 Loss 0.026\n",
            "98 61 Loss 0.007\n",
            "98 62 Loss 0.002\n",
            "98 63 Loss 0.031\n",
            "98 64 Loss 0.016\n",
            "98 65 Loss 0.007\n",
            "98 66 Loss 0.013\n",
            "98 67 Loss 0.018\n",
            "98 68 Loss 0.028\n",
            "98 69 Loss 0.031\n",
            "98 70 Loss 0.010\n",
            "98 71 Loss 0.022\n",
            "98 72 Loss 0.019\n",
            "98 73 Loss 0.022\n",
            "98 74 Loss 0.010\n",
            "98 75 Loss 0.022\n",
            "98 76 Loss 0.041\n",
            "98 77 Loss 0.045\n",
            "98 78 Loss 0.033\n",
            "98 79 Loss 0.016\n",
            "98 80 Loss 0.021\n",
            "98 81 Loss 0.015\n",
            "98 82 Loss 0.072\n",
            "98 83 Loss 0.009\n",
            "98 84 Loss 0.008\n",
            "98 85 Loss 0.026\n",
            "98 86 Loss 0.017\n",
            "98 87 Loss 0.037\n",
            "98 88 Loss 0.017\n",
            "98 89 Loss 0.017\n",
            "98 90 Loss 0.022\n",
            "98 91 Loss 0.012\n",
            "98 92 Loss 0.011\n",
            "98 93 Loss 0.007\n",
            "98 94 Loss 0.015\n",
            "98 95 Loss 0.026\n",
            "98 96 Loss 0.037\n",
            "98 97 Loss 0.010\n",
            "98 98 Loss 0.031\n",
            "98 99 Loss 0.031\n",
            "98 100 Loss 0.119\n",
            "98 101 Loss 0.022\n",
            "98 102 Loss 0.005\n",
            "98 103 Loss 0.007\n",
            "98 104 Loss 0.009\n",
            "98 105 Loss 0.056\n",
            "98 106 Loss 0.006\n",
            "98 107 Loss 0.012\n",
            "98 108 Loss 0.008\n",
            "98 109 Loss 0.013\n",
            "98 110 Loss 0.070\n",
            "98 111 Loss 0.001\n",
            "98 112 Loss 0.017\n",
            "98 113 Loss 0.021\n",
            "98 114 Loss 0.046\n",
            "98 115 Loss 0.009\n",
            "98 116 Loss 0.010\n",
            "98 117 Loss 0.010\n",
            "98 118 Loss 0.007\n",
            "98 119 Loss 0.021\n",
            "98 120 Loss 0.004\n",
            "98 121 Loss 0.012\n",
            "98 122 Loss 0.021\n",
            "98 123 Loss 0.049\n",
            "98 124 Loss 0.008\n",
            "98 125 Loss 0.009\n",
            "98 126 Loss 0.011\n",
            "98 127 Loss 0.023\n",
            "98 128 Loss 0.015\n",
            "98 129 Loss 0.015\n",
            "98 130 Loss 0.011\n",
            "98 131 Loss 0.024\n",
            "98 132 Loss 0.005\n",
            "98 133 Loss 0.006\n",
            "98 134 Loss 0.042\n",
            "98 135 Loss 0.026\n",
            "98 136 Loss 0.010\n",
            "98 137 Loss 0.024\n",
            "98 138 Loss 0.024\n",
            "98 139 Loss 0.052\n",
            "98 140 Loss 0.009\n",
            "98 141 Loss 0.010\n",
            "98 142 Loss 0.021\n",
            "98 143 Loss 0.002\n",
            "98 144 Loss 0.190\n",
            "98 145 Loss 0.009\n",
            "98 146 Loss 0.020\n",
            "98 147 Loss 0.031\n",
            "98 148 Loss 0.061\n",
            "98 149 Loss 0.020\n",
            "98 150 Loss 0.113\n",
            "98 151 Loss 0.009\n",
            "98 152 Loss 0.009\n",
            "98 153 Loss 0.005\n",
            "98 154 Loss 0.020\n",
            "98 155 Loss 0.030\n",
            "98 156 Loss 0.007\n",
            "98 157 Loss 0.014\n",
            "98 158 Loss 0.032\n",
            "98 159 Loss 0.033\n",
            "98 160 Loss 0.004\n",
            "98 161 Loss 0.036\n",
            "98 162 Loss 0.025\n",
            "98 163 Loss 0.019\n",
            "98 164 Loss 0.006\n",
            "98 165 Loss 0.104\n",
            "98 166 Loss 0.023\n",
            "98 167 Loss 0.013\n",
            "98 168 Loss 0.079\n",
            "98 169 Loss 0.011\n",
            "98 170 Loss 0.084\n",
            "98 171 Loss 0.009\n",
            "98 172 Loss 0.015\n",
            "98 173 Loss 0.022\n",
            "98 174 Loss 0.020\n",
            "98 175 Loss 0.024\n",
            "98 176 Loss 0.152\n",
            "98 177 Loss 0.011\n",
            "98 178 Loss 0.017\n",
            "98 179 Loss 0.034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.83      0.89      0.86        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.78      0.87        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.90      0.84      0.87        43\n",
            "           8       0.78      0.84      0.81        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.89       360\n",
            "   macro avg       0.90      0.90      0.90       360\n",
            "weighted avg       0.90      0.89      0.89       360\n",
            "\n",
            "0.8944444444444445\n",
            "99 0 Loss 0.023\n",
            "99 1 Loss 0.054\n",
            "99 2 Loss 0.008\n",
            "99 3 Loss 0.008\n",
            "99 4 Loss 0.017\n",
            "99 5 Loss 0.009\n",
            "99 6 Loss 0.010\n",
            "99 7 Loss 0.023\n",
            "99 8 Loss 0.050\n",
            "99 9 Loss 0.019\n",
            "99 10 Loss 0.076\n",
            "99 11 Loss 0.005\n",
            "99 12 Loss 0.056\n",
            "99 13 Loss 0.014\n",
            "99 14 Loss 0.007\n",
            "99 15 Loss 0.037\n",
            "99 16 Loss 0.054\n",
            "99 17 Loss 0.017\n",
            "99 18 Loss 0.042\n",
            "99 19 Loss 0.046\n",
            "99 20 Loss 0.008\n",
            "99 21 Loss 0.021\n",
            "99 22 Loss 0.033\n",
            "99 23 Loss 0.019\n",
            "99 24 Loss 0.025\n",
            "99 25 Loss 0.066\n",
            "99 26 Loss 0.005\n",
            "99 27 Loss 0.019\n",
            "99 28 Loss 0.011\n",
            "99 29 Loss 0.025\n",
            "99 30 Loss 0.013\n",
            "99 31 Loss 0.020\n",
            "99 32 Loss 0.052\n",
            "99 33 Loss 0.016\n",
            "99 34 Loss 0.009\n",
            "99 35 Loss 0.016\n",
            "99 36 Loss 0.023\n",
            "99 37 Loss 0.020\n",
            "99 38 Loss 0.011\n",
            "99 39 Loss 0.012\n",
            "99 40 Loss 0.011\n",
            "99 41 Loss 0.018\n",
            "99 42 Loss 0.007\n",
            "99 43 Loss 0.018\n",
            "99 44 Loss 0.014\n",
            "99 45 Loss 0.004\n",
            "99 46 Loss 0.142\n",
            "99 47 Loss 0.010\n",
            "99 48 Loss 0.016\n",
            "99 49 Loss 0.004\n",
            "99 50 Loss 0.013\n",
            "99 51 Loss 0.047\n",
            "99 52 Loss 0.016\n",
            "99 53 Loss 0.012\n",
            "99 54 Loss 0.004\n",
            "99 55 Loss 0.013\n",
            "99 56 Loss 0.020\n",
            "99 57 Loss 0.004\n",
            "99 58 Loss 0.020\n",
            "99 59 Loss 0.002\n",
            "99 60 Loss 0.017\n",
            "99 61 Loss 0.029\n",
            "99 62 Loss 0.001\n",
            "99 63 Loss 0.024\n",
            "99 64 Loss 0.008\n",
            "99 65 Loss 0.013\n",
            "99 66 Loss 0.010\n",
            "99 67 Loss 0.005\n",
            "99 68 Loss 0.075\n",
            "99 69 Loss 0.015\n",
            "99 70 Loss 0.008\n",
            "99 71 Loss 0.094\n",
            "99 72 Loss 0.069\n",
            "99 73 Loss 0.025\n",
            "99 74 Loss 0.053\n",
            "99 75 Loss 0.038\n",
            "99 76 Loss 0.017\n",
            "99 77 Loss 0.033\n",
            "99 78 Loss 0.007\n",
            "99 79 Loss 0.027\n",
            "99 80 Loss 0.024\n",
            "99 81 Loss 0.055\n",
            "99 82 Loss 0.035\n",
            "99 83 Loss 0.123\n",
            "99 84 Loss 0.043\n",
            "99 85 Loss 0.059\n",
            "99 86 Loss 0.019\n",
            "99 87 Loss 0.033\n",
            "99 88 Loss 0.016\n",
            "99 89 Loss 0.031\n",
            "99 90 Loss 0.007\n",
            "99 91 Loss 0.011\n",
            "99 92 Loss 0.022\n",
            "99 93 Loss 0.041\n",
            "99 94 Loss 0.016\n",
            "99 95 Loss 0.038\n",
            "99 96 Loss 0.022\n",
            "99 97 Loss 0.010\n",
            "99 98 Loss 0.035\n",
            "99 99 Loss 0.017\n",
            "99 100 Loss 0.034\n",
            "99 101 Loss 0.042\n",
            "99 102 Loss 0.009\n",
            "99 103 Loss 0.022\n",
            "99 104 Loss 0.017\n",
            "99 105 Loss 0.048\n",
            "99 106 Loss 0.013\n",
            "99 107 Loss 0.010\n",
            "99 108 Loss 0.021\n",
            "99 109 Loss 0.010\n",
            "99 110 Loss 0.004\n",
            "99 111 Loss 0.010\n",
            "99 112 Loss 0.043\n",
            "99 113 Loss 0.007\n",
            "99 114 Loss 0.050\n",
            "99 115 Loss 0.012\n",
            "99 116 Loss 0.019\n",
            "99 117 Loss 0.021\n",
            "99 118 Loss 0.017\n",
            "99 119 Loss 0.011\n",
            "99 120 Loss 0.007\n",
            "99 121 Loss 0.009\n",
            "99 122 Loss 0.009\n",
            "99 123 Loss 0.015\n",
            "99 124 Loss 0.011\n",
            "99 125 Loss 0.012\n",
            "99 126 Loss 0.007\n",
            "99 127 Loss 0.005\n",
            "99 128 Loss 0.043\n",
            "99 129 Loss 0.013\n",
            "99 130 Loss 0.009\n",
            "99 131 Loss 0.008\n",
            "99 132 Loss 0.055\n",
            "99 133 Loss 0.018\n",
            "99 134 Loss 0.033\n",
            "99 135 Loss 0.005\n",
            "99 136 Loss 0.009\n",
            "99 137 Loss 0.003\n",
            "99 138 Loss 0.213\n",
            "99 139 Loss 0.027\n",
            "99 140 Loss 0.002\n",
            "99 141 Loss 0.068\n",
            "99 142 Loss 0.007\n",
            "99 143 Loss 0.003\n",
            "99 144 Loss 0.017\n",
            "99 145 Loss 0.018\n",
            "99 146 Loss 0.004\n",
            "99 147 Loss 0.016\n",
            "99 148 Loss 0.099\n",
            "99 149 Loss 0.013\n",
            "99 150 Loss 0.040\n",
            "99 151 Loss 0.004\n",
            "99 152 Loss 0.015\n",
            "99 153 Loss 0.033\n",
            "99 154 Loss 0.007\n",
            "99 155 Loss 0.029\n",
            "99 156 Loss 0.111\n",
            "99 157 Loss 0.011\n",
            "99 158 Loss 0.031\n",
            "99 159 Loss 0.030\n",
            "99 160 Loss 0.041\n",
            "99 161 Loss 0.054\n",
            "99 162 Loss 0.008\n",
            "99 163 Loss 0.017\n",
            "99 164 Loss 0.011\n",
            "99 165 Loss 0.002\n",
            "99 166 Loss 0.015\n",
            "99 167 Loss 0.016\n",
            "99 168 Loss 0.005\n",
            "99 169 Loss 0.021\n",
            "99 170 Loss 0.024\n",
            "99 171 Loss 0.035\n",
            "99 172 Loss 0.029\n",
            "99 173 Loss 0.064\n",
            "99 174 Loss 0.007\n",
            "99 175 Loss 0.037\n",
            "99 176 Loss 0.013\n",
            "99 177 Loss 0.012\n",
            "99 178 Loss 0.005\n",
            "99 179 Loss 0.031\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.79      0.96      0.87        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.79      0.94      0.86        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.83      0.79      0.81        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.90      0.90       360\n",
            "weighted avg       0.90      0.90      0.90       360\n",
            "\n",
            "0.8972222222222223\n",
            "100 0 Loss 0.015\n",
            "100 1 Loss 0.031\n",
            "100 2 Loss 0.103\n",
            "100 3 Loss 0.006\n",
            "100 4 Loss 0.010\n",
            "100 5 Loss 0.026\n",
            "100 6 Loss 0.043\n",
            "100 7 Loss 0.022\n",
            "100 8 Loss 0.004\n",
            "100 9 Loss 0.005\n",
            "100 10 Loss 0.009\n",
            "100 11 Loss 0.048\n",
            "100 12 Loss 0.013\n",
            "100 13 Loss 0.012\n",
            "100 14 Loss 0.072\n",
            "100 15 Loss 0.006\n",
            "100 16 Loss 0.017\n",
            "100 17 Loss 0.006\n",
            "100 18 Loss 0.015\n",
            "100 19 Loss 0.046\n",
            "100 20 Loss 0.023\n",
            "100 21 Loss 0.012\n",
            "100 22 Loss 0.008\n",
            "100 23 Loss 0.015\n",
            "100 24 Loss 0.024\n",
            "100 25 Loss 0.004\n",
            "100 26 Loss 0.017\n",
            "100 27 Loss 0.027\n",
            "100 28 Loss 0.070\n",
            "100 29 Loss 0.044\n",
            "100 30 Loss 0.010\n",
            "100 31 Loss 0.012\n",
            "100 32 Loss 0.008\n",
            "100 33 Loss 0.006\n",
            "100 34 Loss 0.038\n",
            "100 35 Loss 0.006\n",
            "100 36 Loss 0.023\n",
            "100 37 Loss 0.011\n",
            "100 38 Loss 0.016\n",
            "100 39 Loss 0.036\n",
            "100 40 Loss 0.003\n",
            "100 41 Loss 0.017\n",
            "100 42 Loss 0.008\n",
            "100 43 Loss 0.015\n",
            "100 44 Loss 0.018\n",
            "100 45 Loss 0.022\n",
            "100 46 Loss 0.001\n",
            "100 47 Loss 0.009\n",
            "100 48 Loss 0.009\n",
            "100 49 Loss 0.013\n",
            "100 50 Loss 0.016\n",
            "100 51 Loss 0.043\n",
            "100 52 Loss 0.022\n",
            "100 53 Loss 0.036\n",
            "100 54 Loss 0.008\n",
            "100 55 Loss 0.015\n",
            "100 56 Loss 0.020\n",
            "100 57 Loss 0.032\n",
            "100 58 Loss 0.050\n",
            "100 59 Loss 0.002\n",
            "100 60 Loss 0.006\n",
            "100 61 Loss 0.008\n",
            "100 62 Loss 0.023\n",
            "100 63 Loss 0.019\n",
            "100 64 Loss 0.067\n",
            "100 65 Loss 0.005\n",
            "100 66 Loss 0.014\n",
            "100 67 Loss 0.044\n",
            "100 68 Loss 0.093\n",
            "100 69 Loss 0.025\n",
            "100 70 Loss 0.030\n",
            "100 71 Loss 0.008\n",
            "100 72 Loss 0.004\n",
            "100 73 Loss 0.026\n",
            "100 74 Loss 0.004\n",
            "100 75 Loss 0.040\n",
            "100 76 Loss 0.026\n",
            "100 77 Loss 0.006\n",
            "100 78 Loss 0.023\n",
            "100 79 Loss 0.008\n",
            "100 80 Loss 0.151\n",
            "100 81 Loss 0.007\n",
            "100 82 Loss 0.001\n",
            "100 83 Loss 0.085\n",
            "100 84 Loss 0.020\n",
            "100 85 Loss 0.006\n",
            "100 86 Loss 0.007\n",
            "100 87 Loss 0.010\n",
            "100 88 Loss 0.005\n",
            "100 89 Loss 0.004\n",
            "100 90 Loss 0.058\n",
            "100 91 Loss 0.011\n",
            "100 92 Loss 0.011\n",
            "100 93 Loss 0.005\n",
            "100 94 Loss 0.024\n",
            "100 95 Loss 0.038\n",
            "100 96 Loss 0.007\n",
            "100 97 Loss 0.084\n",
            "100 98 Loss 0.011\n",
            "100 99 Loss 0.029\n",
            "100 100 Loss 0.010\n",
            "100 101 Loss 0.006\n",
            "100 102 Loss 0.008\n",
            "100 103 Loss 0.012\n",
            "100 104 Loss 0.008\n",
            "100 105 Loss 0.029\n",
            "100 106 Loss 0.061\n",
            "100 107 Loss 0.020\n",
            "100 108 Loss 0.015\n",
            "100 109 Loss 0.010\n",
            "100 110 Loss 0.027\n",
            "100 111 Loss 0.006\n",
            "100 112 Loss 0.003\n",
            "100 113 Loss 0.004\n",
            "100 114 Loss 0.007\n",
            "100 115 Loss 0.043\n",
            "100 116 Loss 0.006\n",
            "100 117 Loss 0.008\n",
            "100 118 Loss 0.007\n",
            "100 119 Loss 0.009\n",
            "100 120 Loss 0.006\n",
            "100 121 Loss 0.023\n",
            "100 122 Loss 0.019\n",
            "100 123 Loss 0.053\n",
            "100 124 Loss 0.159\n",
            "100 125 Loss 0.019\n",
            "100 126 Loss 0.010\n",
            "100 127 Loss 0.059\n",
            "100 128 Loss 0.005\n",
            "100 129 Loss 0.003\n",
            "100 130 Loss 0.025\n",
            "100 131 Loss 0.020\n",
            "100 132 Loss 0.021\n",
            "100 133 Loss 0.076\n",
            "100 134 Loss 0.030\n",
            "100 135 Loss 0.156\n",
            "100 136 Loss 0.007\n",
            "100 137 Loss 0.011\n",
            "100 138 Loss 0.016\n",
            "100 139 Loss 0.031\n",
            "100 140 Loss 0.134\n",
            "100 141 Loss 0.043\n",
            "100 142 Loss 0.018\n",
            "100 143 Loss 0.008\n",
            "100 144 Loss 0.006\n",
            "100 145 Loss 0.033\n",
            "100 146 Loss 0.059\n",
            "100 147 Loss 0.023\n",
            "100 148 Loss 0.015\n",
            "100 149 Loss 0.003\n",
            "100 150 Loss 0.042\n",
            "100 151 Loss 0.035\n",
            "100 152 Loss 0.010\n",
            "100 153 Loss 0.003\n",
            "100 154 Loss 0.037\n",
            "100 155 Loss 0.008\n",
            "100 156 Loss 0.043\n",
            "100 157 Loss 0.042\n",
            "100 158 Loss 0.017\n",
            "100 159 Loss 0.024\n",
            "100 160 Loss 0.017\n",
            "100 161 Loss 0.008\n",
            "100 162 Loss 0.010\n",
            "100 163 Loss 0.043\n",
            "100 164 Loss 0.048\n",
            "100 165 Loss 0.026\n",
            "100 166 Loss 0.027\n",
            "100 167 Loss 0.031\n",
            "100 168 Loss 0.020\n",
            "100 169 Loss 0.023\n",
            "100 170 Loss 0.019\n",
            "100 171 Loss 0.061\n",
            "100 172 Loss 0.004\n",
            "100 173 Loss 0.017\n",
            "100 174 Loss 0.023\n",
            "100 175 Loss 0.017\n",
            "100 176 Loss 0.014\n",
            "100 177 Loss 0.018\n",
            "100 178 Loss 0.012\n",
            "100 179 Loss 0.015\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.84      0.93      0.88        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.86      0.91        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.82      0.82      0.82        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "101 0 Loss 0.073\n",
            "101 1 Loss 0.034\n",
            "101 2 Loss 0.008\n",
            "101 3 Loss 0.004\n",
            "101 4 Loss 0.002\n",
            "101 5 Loss 0.006\n",
            "101 6 Loss 0.018\n",
            "101 7 Loss 0.067\n",
            "101 8 Loss 0.024\n",
            "101 9 Loss 0.047\n",
            "101 10 Loss 0.009\n",
            "101 11 Loss 0.012\n",
            "101 12 Loss 0.100\n",
            "101 13 Loss 0.008\n",
            "101 14 Loss 0.002\n",
            "101 15 Loss 0.006\n",
            "101 16 Loss 0.022\n",
            "101 17 Loss 0.005\n",
            "101 18 Loss 0.014\n",
            "101 19 Loss 0.017\n",
            "101 20 Loss 0.021\n",
            "101 21 Loss 0.011\n",
            "101 22 Loss 0.043\n",
            "101 23 Loss 0.080\n",
            "101 24 Loss 0.043\n",
            "101 25 Loss 0.003\n",
            "101 26 Loss 0.023\n",
            "101 27 Loss 0.019\n",
            "101 28 Loss 0.026\n",
            "101 29 Loss 0.014\n",
            "101 30 Loss 0.005\n",
            "101 31 Loss 0.027\n",
            "101 32 Loss 0.011\n",
            "101 33 Loss 0.066\n",
            "101 34 Loss 0.064\n",
            "101 35 Loss 0.004\n",
            "101 36 Loss 0.009\n",
            "101 37 Loss 0.011\n",
            "101 38 Loss 0.018\n",
            "101 39 Loss 0.046\n",
            "101 40 Loss 0.011\n",
            "101 41 Loss 0.100\n",
            "101 42 Loss 0.033\n",
            "101 43 Loss 0.019\n",
            "101 44 Loss 0.021\n",
            "101 45 Loss 0.025\n",
            "101 46 Loss 0.010\n",
            "101 47 Loss 0.010\n",
            "101 48 Loss 0.054\n",
            "101 49 Loss 0.041\n",
            "101 50 Loss 0.031\n",
            "101 51 Loss 0.010\n",
            "101 52 Loss 0.008\n",
            "101 53 Loss 0.021\n",
            "101 54 Loss 0.011\n",
            "101 55 Loss 0.019\n",
            "101 56 Loss 0.014\n",
            "101 57 Loss 0.016\n",
            "101 58 Loss 0.004\n",
            "101 59 Loss 0.046\n",
            "101 60 Loss 0.049\n",
            "101 61 Loss 0.015\n",
            "101 62 Loss 0.024\n",
            "101 63 Loss 0.023\n",
            "101 64 Loss 0.003\n",
            "101 65 Loss 0.008\n",
            "101 66 Loss 0.045\n",
            "101 67 Loss 0.025\n",
            "101 68 Loss 0.009\n",
            "101 69 Loss 0.021\n",
            "101 70 Loss 0.011\n",
            "101 71 Loss 0.009\n",
            "101 72 Loss 0.004\n",
            "101 73 Loss 0.002\n",
            "101 74 Loss 0.002\n",
            "101 75 Loss 0.090\n",
            "101 76 Loss 0.090\n",
            "101 77 Loss 0.012\n",
            "101 78 Loss 0.005\n",
            "101 79 Loss 0.006\n",
            "101 80 Loss 0.035\n",
            "101 81 Loss 0.026\n",
            "101 82 Loss 0.004\n",
            "101 83 Loss 0.011\n",
            "101 84 Loss 0.019\n",
            "101 85 Loss 0.030\n",
            "101 86 Loss 0.027\n",
            "101 87 Loss 0.028\n",
            "101 88 Loss 0.012\n",
            "101 89 Loss 0.007\n",
            "101 90 Loss 0.009\n",
            "101 91 Loss 0.020\n",
            "101 92 Loss 0.130\n",
            "101 93 Loss 0.009\n",
            "101 94 Loss 0.004\n",
            "101 95 Loss 0.004\n",
            "101 96 Loss 0.025\n",
            "101 97 Loss 0.007\n",
            "101 98 Loss 0.006\n",
            "101 99 Loss 0.012\n",
            "101 100 Loss 0.030\n",
            "101 101 Loss 0.002\n",
            "101 102 Loss 0.021\n",
            "101 103 Loss 0.012\n",
            "101 104 Loss 0.108\n",
            "101 105 Loss 0.013\n",
            "101 106 Loss 0.006\n",
            "101 107 Loss 0.006\n",
            "101 108 Loss 0.036\n",
            "101 109 Loss 0.010\n",
            "101 110 Loss 0.007\n",
            "101 111 Loss 0.010\n",
            "101 112 Loss 0.007\n",
            "101 113 Loss 0.011\n",
            "101 114 Loss 0.005\n",
            "101 115 Loss 0.024\n",
            "101 116 Loss 0.012\n",
            "101 117 Loss 0.006\n",
            "101 118 Loss 0.011\n",
            "101 119 Loss 0.006\n",
            "101 120 Loss 0.021\n",
            "101 121 Loss 0.004\n",
            "101 122 Loss 0.007\n",
            "101 123 Loss 0.024\n",
            "101 124 Loss 0.013\n",
            "101 125 Loss 0.017\n",
            "101 126 Loss 0.046\n",
            "101 127 Loss 0.011\n",
            "101 128 Loss 0.048\n",
            "101 129 Loss 0.009\n",
            "101 130 Loss 0.020\n",
            "101 131 Loss 0.008\n",
            "101 132 Loss 0.003\n",
            "101 133 Loss 0.039\n",
            "101 134 Loss 0.020\n",
            "101 135 Loss 0.028\n",
            "101 136 Loss 0.011\n",
            "101 137 Loss 0.005\n",
            "101 138 Loss 0.016\n",
            "101 139 Loss 0.027\n",
            "101 140 Loss 0.018\n",
            "101 141 Loss 0.009\n",
            "101 142 Loss 0.010\n",
            "101 143 Loss 0.051\n",
            "101 144 Loss 0.018\n",
            "101 145 Loss 0.023\n",
            "101 146 Loss 0.015\n",
            "101 147 Loss 0.163\n",
            "101 148 Loss 0.021\n",
            "101 149 Loss 0.047\n",
            "101 150 Loss 0.027\n",
            "101 151 Loss 0.061\n",
            "101 152 Loss 0.010\n",
            "101 153 Loss 0.019\n",
            "101 154 Loss 0.039\n",
            "101 155 Loss 0.011\n",
            "101 156 Loss 0.004\n",
            "101 157 Loss 0.020\n",
            "101 158 Loss 0.015\n",
            "101 159 Loss 0.072\n",
            "101 160 Loss 0.018\n",
            "101 161 Loss 0.011\n",
            "101 162 Loss 0.017\n",
            "101 163 Loss 0.008\n",
            "101 164 Loss 0.009\n",
            "101 165 Loss 0.034\n",
            "101 166 Loss 0.021\n",
            "101 167 Loss 0.007\n",
            "101 168 Loss 0.046\n",
            "101 169 Loss 0.024\n",
            "101 170 Loss 0.054\n",
            "101 171 Loss 0.009\n",
            "101 172 Loss 0.020\n",
            "101 173 Loss 0.080\n",
            "101 174 Loss 0.034\n",
            "101 175 Loss 0.015\n",
            "101 176 Loss 0.169\n",
            "101 177 Loss 0.007\n",
            "101 178 Loss 0.016\n",
            "101 179 Loss 0.008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.86      0.91        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "102 0 Loss 0.017\n",
            "102 1 Loss 0.083\n",
            "102 2 Loss 0.032\n",
            "102 3 Loss 0.052\n",
            "102 4 Loss 0.045\n",
            "102 5 Loss 0.020\n",
            "102 6 Loss 0.004\n",
            "102 7 Loss 0.021\n",
            "102 8 Loss 0.010\n",
            "102 9 Loss 0.006\n",
            "102 10 Loss 0.006\n",
            "102 11 Loss 0.013\n",
            "102 12 Loss 0.025\n",
            "102 13 Loss 0.006\n",
            "102 14 Loss 0.024\n",
            "102 15 Loss 0.012\n",
            "102 16 Loss 0.011\n",
            "102 17 Loss 0.015\n",
            "102 18 Loss 0.015\n",
            "102 19 Loss 0.032\n",
            "102 20 Loss 0.038\n",
            "102 21 Loss 0.023\n",
            "102 22 Loss 0.026\n",
            "102 23 Loss 0.010\n",
            "102 24 Loss 0.009\n",
            "102 25 Loss 0.009\n",
            "102 26 Loss 0.014\n",
            "102 27 Loss 0.067\n",
            "102 28 Loss 0.016\n",
            "102 29 Loss 0.019\n",
            "102 30 Loss 0.009\n",
            "102 31 Loss 0.034\n",
            "102 32 Loss 0.016\n",
            "102 33 Loss 0.039\n",
            "102 34 Loss 0.014\n",
            "102 35 Loss 0.016\n",
            "102 36 Loss 0.012\n",
            "102 37 Loss 0.013\n",
            "102 38 Loss 0.010\n",
            "102 39 Loss 0.003\n",
            "102 40 Loss 0.007\n",
            "102 41 Loss 0.025\n",
            "102 42 Loss 0.007\n",
            "102 43 Loss 0.012\n",
            "102 44 Loss 0.019\n",
            "102 45 Loss 0.012\n",
            "102 46 Loss 0.005\n",
            "102 47 Loss 0.029\n",
            "102 48 Loss 0.012\n",
            "102 49 Loss 0.017\n",
            "102 50 Loss 0.025\n",
            "102 51 Loss 0.006\n",
            "102 52 Loss 0.017\n",
            "102 53 Loss 0.010\n",
            "102 54 Loss 0.049\n",
            "102 55 Loss 0.030\n",
            "102 56 Loss 0.005\n",
            "102 57 Loss 0.006\n",
            "102 58 Loss 0.015\n",
            "102 59 Loss 0.053\n",
            "102 60 Loss 0.036\n",
            "102 61 Loss 0.024\n",
            "102 62 Loss 0.004\n",
            "102 63 Loss 0.012\n",
            "102 64 Loss 0.030\n",
            "102 65 Loss 0.006\n",
            "102 66 Loss 0.037\n",
            "102 67 Loss 0.102\n",
            "102 68 Loss 0.003\n",
            "102 69 Loss 0.041\n",
            "102 70 Loss 0.009\n",
            "102 71 Loss 0.016\n",
            "102 72 Loss 0.014\n",
            "102 73 Loss 0.013\n",
            "102 74 Loss 0.013\n",
            "102 75 Loss 0.038\n",
            "102 76 Loss 0.020\n",
            "102 77 Loss 0.011\n",
            "102 78 Loss 0.003\n",
            "102 79 Loss 0.026\n",
            "102 80 Loss 0.029\n",
            "102 81 Loss 0.010\n",
            "102 82 Loss 0.007\n",
            "102 83 Loss 0.006\n",
            "102 84 Loss 0.008\n",
            "102 85 Loss 0.039\n",
            "102 86 Loss 0.015\n",
            "102 87 Loss 0.017\n",
            "102 88 Loss 0.015\n",
            "102 89 Loss 0.014\n",
            "102 90 Loss 0.006\n",
            "102 91 Loss 0.024\n",
            "102 92 Loss 0.011\n",
            "102 93 Loss 0.063\n",
            "102 94 Loss 0.006\n",
            "102 95 Loss 0.066\n",
            "102 96 Loss 0.001\n",
            "102 97 Loss 0.059\n",
            "102 98 Loss 0.005\n",
            "102 99 Loss 0.016\n",
            "102 100 Loss 0.044\n",
            "102 101 Loss 0.046\n",
            "102 102 Loss 0.142\n",
            "102 103 Loss 0.015\n",
            "102 104 Loss 0.010\n",
            "102 105 Loss 0.019\n",
            "102 106 Loss 0.020\n",
            "102 107 Loss 0.008\n",
            "102 108 Loss 0.009\n",
            "102 109 Loss 0.012\n",
            "102 110 Loss 0.015\n",
            "102 111 Loss 0.017\n",
            "102 112 Loss 0.020\n",
            "102 113 Loss 0.009\n",
            "102 114 Loss 0.014\n",
            "102 115 Loss 0.008\n",
            "102 116 Loss 0.047\n",
            "102 117 Loss 0.013\n",
            "102 118 Loss 0.011\n",
            "102 119 Loss 0.043\n",
            "102 120 Loss 0.036\n",
            "102 121 Loss 0.046\n",
            "102 122 Loss 0.008\n",
            "102 123 Loss 0.042\n",
            "102 124 Loss 0.015\n",
            "102 125 Loss 0.020\n",
            "102 126 Loss 0.024\n",
            "102 127 Loss 0.012\n",
            "102 128 Loss 0.005\n",
            "102 129 Loss 0.001\n",
            "102 130 Loss 0.013\n",
            "102 131 Loss 0.006\n",
            "102 132 Loss 0.046\n",
            "102 133 Loss 0.037\n",
            "102 134 Loss 0.014\n",
            "102 135 Loss 0.024\n",
            "102 136 Loss 0.030\n",
            "102 137 Loss 0.003\n",
            "102 138 Loss 0.046\n",
            "102 139 Loss 0.012\n",
            "102 140 Loss 0.125\n",
            "102 141 Loss 0.003\n",
            "102 142 Loss 0.004\n",
            "102 143 Loss 0.013\n",
            "102 144 Loss 0.028\n",
            "102 145 Loss 0.040\n",
            "102 146 Loss 0.034\n",
            "102 147 Loss 0.003\n",
            "102 148 Loss 0.004\n",
            "102 149 Loss 0.020\n",
            "102 150 Loss 0.033\n",
            "102 151 Loss 0.012\n",
            "102 152 Loss 0.033\n",
            "102 153 Loss 0.028\n",
            "102 154 Loss 0.003\n",
            "102 155 Loss 0.005\n",
            "102 156 Loss 0.037\n",
            "102 157 Loss 0.136\n",
            "102 158 Loss 0.015\n",
            "102 159 Loss 0.177\n",
            "102 160 Loss 0.012\n",
            "102 161 Loss 0.026\n",
            "102 162 Loss 0.008\n",
            "102 163 Loss 0.014\n",
            "102 164 Loss 0.023\n",
            "102 165 Loss 0.020\n",
            "102 166 Loss 0.014\n",
            "102 167 Loss 0.020\n",
            "102 168 Loss 0.053\n",
            "102 169 Loss 0.042\n",
            "102 170 Loss 0.123\n",
            "102 171 Loss 0.025\n",
            "102 172 Loss 0.011\n",
            "102 173 Loss 0.015\n",
            "102 174 Loss 0.014\n",
            "102 175 Loss 0.008\n",
            "102 176 Loss 0.020\n",
            "102 177 Loss 0.082\n",
            "102 178 Loss 0.010\n",
            "102 179 Loss 0.008\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.81      0.93      0.87        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.82      0.82      0.82        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.90      0.90      0.90       360\n",
            "\n",
            "0.9\n",
            "103 0 Loss 0.012\n",
            "103 1 Loss 0.021\n",
            "103 2 Loss 0.026\n",
            "103 3 Loss 0.033\n",
            "103 4 Loss 0.021\n",
            "103 5 Loss 0.045\n",
            "103 6 Loss 0.047\n",
            "103 7 Loss 0.025\n",
            "103 8 Loss 0.017\n",
            "103 9 Loss 0.009\n",
            "103 10 Loss 0.065\n",
            "103 11 Loss 0.008\n",
            "103 12 Loss 0.008\n",
            "103 13 Loss 0.041\n",
            "103 14 Loss 0.073\n",
            "103 15 Loss 0.005\n",
            "103 16 Loss 0.008\n",
            "103 17 Loss 0.007\n",
            "103 18 Loss 0.001\n",
            "103 19 Loss 0.005\n",
            "103 20 Loss 0.047\n",
            "103 21 Loss 0.005\n",
            "103 22 Loss 0.018\n",
            "103 23 Loss 0.002\n",
            "103 24 Loss 0.004\n",
            "103 25 Loss 0.014\n",
            "103 26 Loss 0.183\n",
            "103 27 Loss 0.006\n",
            "103 28 Loss 0.057\n",
            "103 29 Loss 0.019\n",
            "103 30 Loss 0.006\n",
            "103 31 Loss 0.043\n",
            "103 32 Loss 0.040\n",
            "103 33 Loss 0.020\n",
            "103 34 Loss 0.020\n",
            "103 35 Loss 0.018\n",
            "103 36 Loss 0.008\n",
            "103 37 Loss 0.014\n",
            "103 38 Loss 0.004\n",
            "103 39 Loss 0.010\n",
            "103 40 Loss 0.034\n",
            "103 41 Loss 0.008\n",
            "103 42 Loss 0.008\n",
            "103 43 Loss 0.035\n",
            "103 44 Loss 0.016\n",
            "103 45 Loss 0.008\n",
            "103 46 Loss 0.003\n",
            "103 47 Loss 0.020\n",
            "103 48 Loss 0.011\n",
            "103 49 Loss 0.018\n",
            "103 50 Loss 0.013\n",
            "103 51 Loss 0.003\n",
            "103 52 Loss 0.005\n",
            "103 53 Loss 0.010\n",
            "103 54 Loss 0.015\n",
            "103 55 Loss 0.005\n",
            "103 56 Loss 0.078\n",
            "103 57 Loss 0.031\n",
            "103 58 Loss 0.011\n",
            "103 59 Loss 0.030\n",
            "103 60 Loss 0.003\n",
            "103 61 Loss 0.112\n",
            "103 62 Loss 0.055\n",
            "103 63 Loss 0.022\n",
            "103 64 Loss 0.058\n",
            "103 65 Loss 0.009\n",
            "103 66 Loss 0.008\n",
            "103 67 Loss 0.042\n",
            "103 68 Loss 0.009\n",
            "103 69 Loss 0.010\n",
            "103 70 Loss 0.007\n",
            "103 71 Loss 0.014\n",
            "103 72 Loss 0.007\n",
            "103 73 Loss 0.005\n",
            "103 74 Loss 0.025\n",
            "103 75 Loss 0.020\n",
            "103 76 Loss 0.014\n",
            "103 77 Loss 0.017\n",
            "103 78 Loss 0.028\n",
            "103 79 Loss 0.018\n",
            "103 80 Loss 0.033\n",
            "103 81 Loss 0.039\n",
            "103 82 Loss 0.009\n",
            "103 83 Loss 0.015\n",
            "103 84 Loss 0.046\n",
            "103 85 Loss 0.016\n",
            "103 86 Loss 0.119\n",
            "103 87 Loss 0.048\n",
            "103 88 Loss 0.009\n",
            "103 89 Loss 0.018\n",
            "103 90 Loss 0.011\n",
            "103 91 Loss 0.024\n",
            "103 92 Loss 0.034\n",
            "103 93 Loss 0.006\n",
            "103 94 Loss 0.014\n",
            "103 95 Loss 0.013\n",
            "103 96 Loss 0.021\n",
            "103 97 Loss 0.010\n",
            "103 98 Loss 0.008\n",
            "103 99 Loss 0.014\n",
            "103 100 Loss 0.027\n",
            "103 101 Loss 0.143\n",
            "103 102 Loss 0.048\n",
            "103 103 Loss 0.049\n",
            "103 104 Loss 0.015\n",
            "103 105 Loss 0.004\n",
            "103 106 Loss 0.008\n",
            "103 107 Loss 0.046\n",
            "103 108 Loss 0.023\n",
            "103 109 Loss 0.011\n",
            "103 110 Loss 0.039\n",
            "103 111 Loss 0.041\n",
            "103 112 Loss 0.008\n",
            "103 113 Loss 0.012\n",
            "103 114 Loss 0.008\n",
            "103 115 Loss 0.004\n",
            "103 116 Loss 0.004\n",
            "103 117 Loss 0.006\n",
            "103 118 Loss 0.017\n",
            "103 119 Loss 0.011\n",
            "103 120 Loss 0.045\n",
            "103 121 Loss 0.019\n",
            "103 122 Loss 0.027\n",
            "103 123 Loss 0.088\n",
            "103 124 Loss 0.037\n",
            "103 125 Loss 0.003\n",
            "103 126 Loss 0.012\n",
            "103 127 Loss 0.018\n",
            "103 128 Loss 0.084\n",
            "103 129 Loss 0.009\n",
            "103 130 Loss 0.015\n",
            "103 131 Loss 0.013\n",
            "103 132 Loss 0.009\n",
            "103 133 Loss 0.004\n",
            "103 134 Loss 0.015\n",
            "103 135 Loss 0.022\n",
            "103 136 Loss 0.016\n",
            "103 137 Loss 0.015\n",
            "103 138 Loss 0.015\n",
            "103 139 Loss 0.045\n",
            "103 140 Loss 0.005\n",
            "103 141 Loss 0.016\n",
            "103 142 Loss 0.021\n",
            "103 143 Loss 0.041\n",
            "103 144 Loss 0.009\n",
            "103 145 Loss 0.062\n",
            "103 146 Loss 0.009\n",
            "103 147 Loss 0.029\n",
            "103 148 Loss 0.020\n",
            "103 149 Loss 0.005\n",
            "103 150 Loss 0.017\n",
            "103 151 Loss 0.030\n",
            "103 152 Loss 0.011\n",
            "103 153 Loss 0.026\n",
            "103 154 Loss 0.010\n",
            "103 155 Loss 0.050\n",
            "103 156 Loss 0.113\n",
            "103 157 Loss 0.052\n",
            "103 158 Loss 0.013\n",
            "103 159 Loss 0.008\n",
            "103 160 Loss 0.009\n",
            "103 161 Loss 0.014\n",
            "103 162 Loss 0.006\n",
            "103 163 Loss 0.010\n",
            "103 164 Loss 0.004\n",
            "103 165 Loss 0.004\n",
            "103 166 Loss 0.022\n",
            "103 167 Loss 0.022\n",
            "103 168 Loss 0.016\n",
            "103 169 Loss 0.037\n",
            "103 170 Loss 0.020\n",
            "103 171 Loss 0.026\n",
            "103 172 Loss 0.022\n",
            "103 173 Loss 0.016\n",
            "103 174 Loss 0.043\n",
            "103 175 Loss 0.012\n",
            "103 176 Loss 0.006\n",
            "103 177 Loss 0.010\n",
            "103 178 Loss 0.003\n",
            "103 179 Loss 0.036\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.90      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "104 0 Loss 0.069\n",
            "104 1 Loss 0.006\n",
            "104 2 Loss 0.020\n",
            "104 3 Loss 0.009\n",
            "104 4 Loss 0.027\n",
            "104 5 Loss 0.013\n",
            "104 6 Loss 0.040\n",
            "104 7 Loss 0.005\n",
            "104 8 Loss 0.003\n",
            "104 9 Loss 0.006\n",
            "104 10 Loss 0.016\n",
            "104 11 Loss 0.017\n",
            "104 12 Loss 0.010\n",
            "104 13 Loss 0.045\n",
            "104 14 Loss 0.008\n",
            "104 15 Loss 0.016\n",
            "104 16 Loss 0.012\n",
            "104 17 Loss 0.002\n",
            "104 18 Loss 0.010\n",
            "104 19 Loss 0.045\n",
            "104 20 Loss 0.019\n",
            "104 21 Loss 0.004\n",
            "104 22 Loss 0.025\n",
            "104 23 Loss 0.012\n",
            "104 24 Loss 0.009\n",
            "104 25 Loss 0.012\n",
            "104 26 Loss 0.045\n",
            "104 27 Loss 0.007\n",
            "104 28 Loss 0.044\n",
            "104 29 Loss 0.008\n",
            "104 30 Loss 0.176\n",
            "104 31 Loss 0.008\n",
            "104 32 Loss 0.017\n",
            "104 33 Loss 0.006\n",
            "104 34 Loss 0.070\n",
            "104 35 Loss 0.025\n",
            "104 36 Loss 0.010\n",
            "104 37 Loss 0.006\n",
            "104 38 Loss 0.008\n",
            "104 39 Loss 0.006\n",
            "104 40 Loss 0.015\n",
            "104 41 Loss 0.003\n",
            "104 42 Loss 0.016\n",
            "104 43 Loss 0.009\n",
            "104 44 Loss 0.045\n",
            "104 45 Loss 0.093\n",
            "104 46 Loss 0.033\n",
            "104 47 Loss 0.054\n",
            "104 48 Loss 0.013\n",
            "104 49 Loss 0.001\n",
            "104 50 Loss 0.022\n",
            "104 51 Loss 0.027\n",
            "104 52 Loss 0.018\n",
            "104 53 Loss 0.007\n",
            "104 54 Loss 0.014\n",
            "104 55 Loss 0.055\n",
            "104 56 Loss 0.013\n",
            "104 57 Loss 0.002\n",
            "104 58 Loss 0.030\n",
            "104 59 Loss 0.061\n",
            "104 60 Loss 0.007\n",
            "104 61 Loss 0.021\n",
            "104 62 Loss 0.041\n",
            "104 63 Loss 0.016\n",
            "104 64 Loss 0.049\n",
            "104 65 Loss 0.027\n",
            "104 66 Loss 0.005\n",
            "104 67 Loss 0.007\n",
            "104 68 Loss 0.013\n",
            "104 69 Loss 0.023\n",
            "104 70 Loss 0.015\n",
            "104 71 Loss 0.014\n",
            "104 72 Loss 0.043\n",
            "104 73 Loss 0.053\n",
            "104 74 Loss 0.048\n",
            "104 75 Loss 0.050\n",
            "104 76 Loss 0.005\n",
            "104 77 Loss 0.007\n",
            "104 78 Loss 0.015\n",
            "104 79 Loss 0.053\n",
            "104 80 Loss 0.011\n",
            "104 81 Loss 0.017\n",
            "104 82 Loss 0.007\n",
            "104 83 Loss 0.007\n",
            "104 84 Loss 0.008\n",
            "104 85 Loss 0.058\n",
            "104 86 Loss 0.004\n",
            "104 87 Loss 0.013\n",
            "104 88 Loss 0.002\n",
            "104 89 Loss 0.040\n",
            "104 90 Loss 0.018\n",
            "104 91 Loss 0.005\n",
            "104 92 Loss 0.039\n",
            "104 93 Loss 0.047\n",
            "104 94 Loss 0.004\n",
            "104 95 Loss 0.039\n",
            "104 96 Loss 0.004\n",
            "104 97 Loss 0.151\n",
            "104 98 Loss 0.004\n",
            "104 99 Loss 0.005\n",
            "104 100 Loss 0.002\n",
            "104 101 Loss 0.042\n",
            "104 102 Loss 0.026\n",
            "104 103 Loss 0.015\n",
            "104 104 Loss 0.027\n",
            "104 105 Loss 0.009\n",
            "104 106 Loss 0.003\n",
            "104 107 Loss 0.051\n",
            "104 108 Loss 0.020\n",
            "104 109 Loss 0.007\n",
            "104 110 Loss 0.019\n",
            "104 111 Loss 0.004\n",
            "104 112 Loss 0.099\n",
            "104 113 Loss 0.021\n",
            "104 114 Loss 0.004\n",
            "104 115 Loss 0.025\n",
            "104 116 Loss 0.009\n",
            "104 117 Loss 0.004\n",
            "104 118 Loss 0.038\n",
            "104 119 Loss 0.012\n",
            "104 120 Loss 0.005\n",
            "104 121 Loss 0.090\n",
            "104 122 Loss 0.007\n",
            "104 123 Loss 0.052\n",
            "104 124 Loss 0.008\n",
            "104 125 Loss 0.040\n",
            "104 126 Loss 0.002\n",
            "104 127 Loss 0.021\n",
            "104 128 Loss 0.022\n",
            "104 129 Loss 0.007\n",
            "104 130 Loss 0.007\n",
            "104 131 Loss 0.015\n",
            "104 132 Loss 0.033\n",
            "104 133 Loss 0.024\n",
            "104 134 Loss 0.004\n",
            "104 135 Loss 0.010\n",
            "104 136 Loss 0.007\n",
            "104 137 Loss 0.009\n",
            "104 138 Loss 0.031\n",
            "104 139 Loss 0.008\n",
            "104 140 Loss 0.022\n",
            "104 141 Loss 0.025\n",
            "104 142 Loss 0.012\n",
            "104 143 Loss 0.012\n",
            "104 144 Loss 0.015\n",
            "104 145 Loss 0.013\n",
            "104 146 Loss 0.023\n",
            "104 147 Loss 0.015\n",
            "104 148 Loss 0.012\n",
            "104 149 Loss 0.008\n",
            "104 150 Loss 0.007\n",
            "104 151 Loss 0.016\n",
            "104 152 Loss 0.025\n",
            "104 153 Loss 0.042\n",
            "104 154 Loss 0.105\n",
            "104 155 Loss 0.049\n",
            "104 156 Loss 0.044\n",
            "104 157 Loss 0.019\n",
            "104 158 Loss 0.021\n",
            "104 159 Loss 0.022\n",
            "104 160 Loss 0.010\n",
            "104 161 Loss 0.019\n",
            "104 162 Loss 0.023\n",
            "104 163 Loss 0.007\n",
            "104 164 Loss 0.011\n",
            "104 165 Loss 0.015\n",
            "104 166 Loss 0.004\n",
            "104 167 Loss 0.040\n",
            "104 168 Loss 0.005\n",
            "104 169 Loss 0.035\n",
            "104 170 Loss 0.024\n",
            "104 171 Loss 0.014\n",
            "104 172 Loss 0.018\n",
            "104 173 Loss 0.132\n",
            "104 174 Loss 0.008\n",
            "104 175 Loss 0.010\n",
            "104 176 Loss 0.007\n",
            "104 177 Loss 0.026\n",
            "104 178 Loss 0.021\n",
            "104 179 Loss 0.030\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.84      0.96      0.90        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.81      0.97      0.88        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.90      0.84      0.87        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.98      0.88      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "105 0 Loss 0.033\n",
            "105 1 Loss 0.014\n",
            "105 2 Loss 0.007\n",
            "105 3 Loss 0.046\n",
            "105 4 Loss 0.048\n",
            "105 5 Loss 0.027\n",
            "105 6 Loss 0.002\n",
            "105 7 Loss 0.030\n",
            "105 8 Loss 0.006\n",
            "105 9 Loss 0.013\n",
            "105 10 Loss 0.027\n",
            "105 11 Loss 0.017\n",
            "105 12 Loss 0.024\n",
            "105 13 Loss 0.022\n",
            "105 14 Loss 0.015\n",
            "105 15 Loss 0.023\n",
            "105 16 Loss 0.064\n",
            "105 17 Loss 0.012\n",
            "105 18 Loss 0.032\n",
            "105 19 Loss 0.010\n",
            "105 20 Loss 0.011\n",
            "105 21 Loss 0.038\n",
            "105 22 Loss 0.015\n",
            "105 23 Loss 0.011\n",
            "105 24 Loss 0.005\n",
            "105 25 Loss 0.014\n",
            "105 26 Loss 0.007\n",
            "105 27 Loss 0.020\n",
            "105 28 Loss 0.016\n",
            "105 29 Loss 0.009\n",
            "105 30 Loss 0.009\n",
            "105 31 Loss 0.009\n",
            "105 32 Loss 0.002\n",
            "105 33 Loss 0.005\n",
            "105 34 Loss 0.011\n",
            "105 35 Loss 0.048\n",
            "105 36 Loss 0.019\n",
            "105 37 Loss 0.030\n",
            "105 38 Loss 0.012\n",
            "105 39 Loss 0.069\n",
            "105 40 Loss 0.023\n",
            "105 41 Loss 0.009\n",
            "105 42 Loss 0.009\n",
            "105 43 Loss 0.025\n",
            "105 44 Loss 0.007\n",
            "105 45 Loss 0.005\n",
            "105 46 Loss 0.025\n",
            "105 47 Loss 0.013\n",
            "105 48 Loss 0.007\n",
            "105 49 Loss 0.098\n",
            "105 50 Loss 0.014\n",
            "105 51 Loss 0.026\n",
            "105 52 Loss 0.003\n",
            "105 53 Loss 0.103\n",
            "105 54 Loss 0.013\n",
            "105 55 Loss 0.006\n",
            "105 56 Loss 0.007\n",
            "105 57 Loss 0.011\n",
            "105 58 Loss 0.028\n",
            "105 59 Loss 0.023\n",
            "105 60 Loss 0.007\n",
            "105 61 Loss 0.035\n",
            "105 62 Loss 0.018\n",
            "105 63 Loss 0.017\n",
            "105 64 Loss 0.009\n",
            "105 65 Loss 0.007\n",
            "105 66 Loss 0.054\n",
            "105 67 Loss 0.010\n",
            "105 68 Loss 0.021\n",
            "105 69 Loss 0.018\n",
            "105 70 Loss 0.017\n",
            "105 71 Loss 0.061\n",
            "105 72 Loss 0.006\n",
            "105 73 Loss 0.010\n",
            "105 74 Loss 0.021\n",
            "105 75 Loss 0.037\n",
            "105 76 Loss 0.017\n",
            "105 77 Loss 0.012\n",
            "105 78 Loss 0.011\n",
            "105 79 Loss 0.029\n",
            "105 80 Loss 0.004\n",
            "105 81 Loss 0.016\n",
            "105 82 Loss 0.044\n",
            "105 83 Loss 0.001\n",
            "105 84 Loss 0.005\n",
            "105 85 Loss 0.020\n",
            "105 86 Loss 0.004\n",
            "105 87 Loss 0.009\n",
            "105 88 Loss 0.031\n",
            "105 89 Loss 0.014\n",
            "105 90 Loss 0.019\n",
            "105 91 Loss 0.118\n",
            "105 92 Loss 0.016\n",
            "105 93 Loss 0.009\n",
            "105 94 Loss 0.059\n",
            "105 95 Loss 0.051\n",
            "105 96 Loss 0.006\n",
            "105 97 Loss 0.037\n",
            "105 98 Loss 0.013\n",
            "105 99 Loss 0.003\n",
            "105 100 Loss 0.013\n",
            "105 101 Loss 0.027\n",
            "105 102 Loss 0.011\n",
            "105 103 Loss 0.053\n",
            "105 104 Loss 0.014\n",
            "105 105 Loss 0.018\n",
            "105 106 Loss 0.021\n",
            "105 107 Loss 0.006\n",
            "105 108 Loss 0.004\n",
            "105 109 Loss 0.027\n",
            "105 110 Loss 0.005\n",
            "105 111 Loss 0.016\n",
            "105 112 Loss 0.003\n",
            "105 113 Loss 0.004\n",
            "105 114 Loss 0.015\n",
            "105 115 Loss 0.016\n",
            "105 116 Loss 0.103\n",
            "105 117 Loss 0.024\n",
            "105 118 Loss 0.012\n",
            "105 119 Loss 0.029\n",
            "105 120 Loss 0.007\n",
            "105 121 Loss 0.010\n",
            "105 122 Loss 0.056\n",
            "105 123 Loss 0.009\n",
            "105 124 Loss 0.040\n",
            "105 125 Loss 0.010\n",
            "105 126 Loss 0.001\n",
            "105 127 Loss 0.018\n",
            "105 128 Loss 0.005\n",
            "105 129 Loss 0.011\n",
            "105 130 Loss 0.002\n",
            "105 131 Loss 0.018\n",
            "105 132 Loss 0.011\n",
            "105 133 Loss 0.006\n",
            "105 134 Loss 0.014\n",
            "105 135 Loss 0.003\n",
            "105 136 Loss 0.029\n",
            "105 137 Loss 0.017\n",
            "105 138 Loss 0.016\n",
            "105 139 Loss 0.031\n",
            "105 140 Loss 0.015\n",
            "105 141 Loss 0.006\n",
            "105 142 Loss 0.080\n",
            "105 143 Loss 0.060\n",
            "105 144 Loss 0.063\n",
            "105 145 Loss 0.109\n",
            "105 146 Loss 0.021\n",
            "105 147 Loss 0.024\n",
            "105 148 Loss 0.007\n",
            "105 149 Loss 0.009\n",
            "105 150 Loss 0.014\n",
            "105 151 Loss 0.026\n",
            "105 152 Loss 0.005\n",
            "105 153 Loss 0.013\n",
            "105 154 Loss 0.013\n",
            "105 155 Loss 0.008\n",
            "105 156 Loss 0.021\n",
            "105 157 Loss 0.009\n",
            "105 158 Loss 0.006\n",
            "105 159 Loss 0.026\n",
            "105 160 Loss 0.013\n",
            "105 161 Loss 0.007\n",
            "105 162 Loss 0.003\n",
            "105 163 Loss 0.205\n",
            "105 164 Loss 0.097\n",
            "105 165 Loss 0.012\n",
            "105 166 Loss 0.010\n",
            "105 167 Loss 0.053\n",
            "105 168 Loss 0.059\n",
            "105 169 Loss 0.052\n",
            "105 170 Loss 0.014\n",
            "105 171 Loss 0.016\n",
            "105 172 Loss 0.025\n",
            "105 173 Loss 0.011\n",
            "105 174 Loss 0.024\n",
            "105 175 Loss 0.026\n",
            "105 176 Loss 0.038\n",
            "105 177 Loss 0.014\n",
            "105 178 Loss 0.018\n",
            "105 179 Loss 0.011\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.90      0.81      0.85        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.90      0.90      0.90       360\n",
            "\n",
            "0.9\n",
            "106 0 Loss 0.003\n",
            "106 1 Loss 0.006\n",
            "106 2 Loss 0.051\n",
            "106 3 Loss 0.033\n",
            "106 4 Loss 0.005\n",
            "106 5 Loss 0.044\n",
            "106 6 Loss 0.010\n",
            "106 7 Loss 0.023\n",
            "106 8 Loss 0.020\n",
            "106 9 Loss 0.010\n",
            "106 10 Loss 0.012\n",
            "106 11 Loss 0.003\n",
            "106 12 Loss 0.017\n",
            "106 13 Loss 0.039\n",
            "106 14 Loss 0.053\n",
            "106 15 Loss 0.009\n",
            "106 16 Loss 0.010\n",
            "106 17 Loss 0.142\n",
            "106 18 Loss 0.017\n",
            "106 19 Loss 0.014\n",
            "106 20 Loss 0.032\n",
            "106 21 Loss 0.011\n",
            "106 22 Loss 0.014\n",
            "106 23 Loss 0.001\n",
            "106 24 Loss 0.023\n",
            "106 25 Loss 0.041\n",
            "106 26 Loss 0.033\n",
            "106 27 Loss 0.014\n",
            "106 28 Loss 0.020\n",
            "106 29 Loss 0.009\n",
            "106 30 Loss 0.008\n",
            "106 31 Loss 0.012\n",
            "106 32 Loss 0.016\n",
            "106 33 Loss 0.081\n",
            "106 34 Loss 0.020\n",
            "106 35 Loss 0.020\n",
            "106 36 Loss 0.009\n",
            "106 37 Loss 0.022\n",
            "106 38 Loss 0.047\n",
            "106 39 Loss 0.009\n",
            "106 40 Loss 0.008\n",
            "106 41 Loss 0.014\n",
            "106 42 Loss 0.018\n",
            "106 43 Loss 0.019\n",
            "106 44 Loss 0.025\n",
            "106 45 Loss 0.012\n",
            "106 46 Loss 0.053\n",
            "106 47 Loss 0.029\n",
            "106 48 Loss 0.011\n",
            "106 49 Loss 0.047\n",
            "106 50 Loss 0.075\n",
            "106 51 Loss 0.013\n",
            "106 52 Loss 0.007\n",
            "106 53 Loss 0.013\n",
            "106 54 Loss 0.007\n",
            "106 55 Loss 0.026\n",
            "106 56 Loss 0.013\n",
            "106 57 Loss 0.036\n",
            "106 58 Loss 0.031\n",
            "106 59 Loss 0.016\n",
            "106 60 Loss 0.014\n",
            "106 61 Loss 0.051\n",
            "106 62 Loss 0.009\n",
            "106 63 Loss 0.040\n",
            "106 64 Loss 0.015\n",
            "106 65 Loss 0.006\n",
            "106 66 Loss 0.003\n",
            "106 67 Loss 0.006\n",
            "106 68 Loss 0.049\n",
            "106 69 Loss 0.045\n",
            "106 70 Loss 0.134\n",
            "106 71 Loss 0.010\n",
            "106 72 Loss 0.032\n",
            "106 73 Loss 0.017\n",
            "106 74 Loss 0.010\n",
            "106 75 Loss 0.009\n",
            "106 76 Loss 0.005\n",
            "106 77 Loss 0.033\n",
            "106 78 Loss 0.003\n",
            "106 79 Loss 0.007\n",
            "106 80 Loss 0.004\n",
            "106 81 Loss 0.007\n",
            "106 82 Loss 0.017\n",
            "106 83 Loss 0.013\n",
            "106 84 Loss 0.017\n",
            "106 85 Loss 0.002\n",
            "106 86 Loss 0.014\n",
            "106 87 Loss 0.032\n",
            "106 88 Loss 0.014\n",
            "106 89 Loss 0.014\n",
            "106 90 Loss 0.007\n",
            "106 91 Loss 0.013\n",
            "106 92 Loss 0.017\n",
            "106 93 Loss 0.017\n",
            "106 94 Loss 0.006\n",
            "106 95 Loss 0.007\n",
            "106 96 Loss 0.015\n",
            "106 97 Loss 0.007\n",
            "106 98 Loss 0.006\n",
            "106 99 Loss 0.029\n",
            "106 100 Loss 0.022\n",
            "106 101 Loss 0.014\n",
            "106 102 Loss 0.009\n",
            "106 103 Loss 0.009\n",
            "106 104 Loss 0.008\n",
            "106 105 Loss 0.046\n",
            "106 106 Loss 0.004\n",
            "106 107 Loss 0.026\n",
            "106 108 Loss 0.031\n",
            "106 109 Loss 0.014\n",
            "106 110 Loss 0.018\n",
            "106 111 Loss 0.028\n",
            "106 112 Loss 0.004\n",
            "106 113 Loss 0.053\n",
            "106 114 Loss 0.008\n",
            "106 115 Loss 0.025\n",
            "106 116 Loss 0.008\n",
            "106 117 Loss 0.025\n",
            "106 118 Loss 0.043\n",
            "106 119 Loss 0.030\n",
            "106 120 Loss 0.022\n",
            "106 121 Loss 0.048\n",
            "106 122 Loss 0.005\n",
            "106 123 Loss 0.004\n",
            "106 124 Loss 0.024\n",
            "106 125 Loss 0.012\n",
            "106 126 Loss 0.016\n",
            "106 127 Loss 0.036\n",
            "106 128 Loss 0.026\n",
            "106 129 Loss 0.007\n",
            "106 130 Loss 0.079\n",
            "106 131 Loss 0.010\n",
            "106 132 Loss 0.006\n",
            "106 133 Loss 0.016\n",
            "106 134 Loss 0.008\n",
            "106 135 Loss 0.016\n",
            "106 136 Loss 0.031\n",
            "106 137 Loss 0.008\n",
            "106 138 Loss 0.018\n",
            "106 139 Loss 0.010\n",
            "106 140 Loss 0.010\n",
            "106 141 Loss 0.011\n",
            "106 142 Loss 0.007\n",
            "106 143 Loss 0.049\n",
            "106 144 Loss 0.003\n",
            "106 145 Loss 0.022\n",
            "106 146 Loss 0.019\n",
            "106 147 Loss 0.019\n",
            "106 148 Loss 0.019\n",
            "106 149 Loss 0.003\n",
            "106 150 Loss 0.105\n",
            "106 151 Loss 0.033\n",
            "106 152 Loss 0.046\n",
            "106 153 Loss 0.013\n",
            "106 154 Loss 0.036\n",
            "106 155 Loss 0.014\n",
            "106 156 Loss 0.067\n",
            "106 157 Loss 0.012\n",
            "106 158 Loss 0.051\n",
            "106 159 Loss 0.014\n",
            "106 160 Loss 0.003\n",
            "106 161 Loss 0.058\n",
            "106 162 Loss 0.007\n",
            "106 163 Loss 0.019\n",
            "106 164 Loss 0.025\n",
            "106 165 Loss 0.024\n",
            "106 166 Loss 0.023\n",
            "106 167 Loss 0.010\n",
            "106 168 Loss 0.008\n",
            "106 169 Loss 0.010\n",
            "106 170 Loss 0.141\n",
            "106 171 Loss 0.028\n",
            "106 172 Loss 0.036\n",
            "106 173 Loss 0.047\n",
            "106 174 Loss 0.038\n",
            "106 175 Loss 0.005\n",
            "106 176 Loss 0.009\n",
            "106 177 Loss 0.012\n",
            "106 178 Loss 0.002\n",
            "106 179 Loss 0.003\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.84      0.96      0.90        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.91      0.86      0.89        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.83      0.79      0.81        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "107 0 Loss 0.020\n",
            "107 1 Loss 0.032\n",
            "107 2 Loss 0.081\n",
            "107 3 Loss 0.012\n",
            "107 4 Loss 0.020\n",
            "107 5 Loss 0.003\n",
            "107 6 Loss 0.012\n",
            "107 7 Loss 0.010\n",
            "107 8 Loss 0.012\n",
            "107 9 Loss 0.002\n",
            "107 10 Loss 0.020\n",
            "107 11 Loss 0.089\n",
            "107 12 Loss 0.201\n",
            "107 13 Loss 0.013\n",
            "107 14 Loss 0.014\n",
            "107 15 Loss 0.015\n",
            "107 16 Loss 0.008\n",
            "107 17 Loss 0.012\n",
            "107 18 Loss 0.007\n",
            "107 19 Loss 0.010\n",
            "107 20 Loss 0.008\n",
            "107 21 Loss 0.023\n",
            "107 22 Loss 0.019\n",
            "107 23 Loss 0.043\n",
            "107 24 Loss 0.023\n",
            "107 25 Loss 0.019\n",
            "107 26 Loss 0.032\n",
            "107 27 Loss 0.050\n",
            "107 28 Loss 0.026\n",
            "107 29 Loss 0.005\n",
            "107 30 Loss 0.018\n",
            "107 31 Loss 0.011\n",
            "107 32 Loss 0.018\n",
            "107 33 Loss 0.122\n",
            "107 34 Loss 0.004\n",
            "107 35 Loss 0.014\n",
            "107 36 Loss 0.037\n",
            "107 37 Loss 0.032\n",
            "107 38 Loss 0.009\n",
            "107 39 Loss 0.005\n",
            "107 40 Loss 0.013\n",
            "107 41 Loss 0.011\n",
            "107 42 Loss 0.076\n",
            "107 43 Loss 0.012\n",
            "107 44 Loss 0.011\n",
            "107 45 Loss 0.004\n",
            "107 46 Loss 0.015\n",
            "107 47 Loss 0.017\n",
            "107 48 Loss 0.018\n",
            "107 49 Loss 0.015\n",
            "107 50 Loss 0.018\n",
            "107 51 Loss 0.008\n",
            "107 52 Loss 0.020\n",
            "107 53 Loss 0.047\n",
            "107 54 Loss 0.008\n",
            "107 55 Loss 0.008\n",
            "107 56 Loss 0.022\n",
            "107 57 Loss 0.031\n",
            "107 58 Loss 0.021\n",
            "107 59 Loss 0.016\n",
            "107 60 Loss 0.007\n",
            "107 61 Loss 0.013\n",
            "107 62 Loss 0.028\n",
            "107 63 Loss 0.009\n",
            "107 64 Loss 0.061\n",
            "107 65 Loss 0.018\n",
            "107 66 Loss 0.003\n",
            "107 67 Loss 0.045\n",
            "107 68 Loss 0.010\n",
            "107 69 Loss 0.093\n",
            "107 70 Loss 0.013\n",
            "107 71 Loss 0.008\n",
            "107 72 Loss 0.008\n",
            "107 73 Loss 0.023\n",
            "107 74 Loss 0.049\n",
            "107 75 Loss 0.013\n",
            "107 76 Loss 0.005\n",
            "107 77 Loss 0.004\n",
            "107 78 Loss 0.014\n",
            "107 79 Loss 0.003\n",
            "107 80 Loss 0.009\n",
            "107 81 Loss 0.012\n",
            "107 82 Loss 0.049\n",
            "107 83 Loss 0.021\n",
            "107 84 Loss 0.037\n",
            "107 85 Loss 0.003\n",
            "107 86 Loss 0.019\n",
            "107 87 Loss 0.021\n",
            "107 88 Loss 0.004\n",
            "107 89 Loss 0.008\n",
            "107 90 Loss 0.078\n",
            "107 91 Loss 0.007\n",
            "107 92 Loss 0.164\n",
            "107 93 Loss 0.014\n",
            "107 94 Loss 0.035\n",
            "107 95 Loss 0.039\n",
            "107 96 Loss 0.020\n",
            "107 97 Loss 0.013\n",
            "107 98 Loss 0.024\n",
            "107 99 Loss 0.023\n",
            "107 100 Loss 0.008\n",
            "107 101 Loss 0.016\n",
            "107 102 Loss 0.049\n",
            "107 103 Loss 0.030\n",
            "107 104 Loss 0.019\n",
            "107 105 Loss 0.006\n",
            "107 106 Loss 0.004\n",
            "107 107 Loss 0.035\n",
            "107 108 Loss 0.007\n",
            "107 109 Loss 0.014\n",
            "107 110 Loss 0.031\n",
            "107 111 Loss 0.025\n",
            "107 112 Loss 0.012\n",
            "107 113 Loss 0.005\n",
            "107 114 Loss 0.007\n",
            "107 115 Loss 0.023\n",
            "107 116 Loss 0.005\n",
            "107 117 Loss 0.008\n",
            "107 118 Loss 0.016\n",
            "107 119 Loss 0.019\n",
            "107 120 Loss 0.020\n",
            "107 121 Loss 0.106\n",
            "107 122 Loss 0.001\n",
            "107 123 Loss 0.046\n",
            "107 124 Loss 0.047\n",
            "107 125 Loss 0.016\n",
            "107 126 Loss 0.013\n",
            "107 127 Loss 0.041\n",
            "107 128 Loss 0.009\n",
            "107 129 Loss 0.014\n",
            "107 130 Loss 0.030\n",
            "107 131 Loss 0.030\n",
            "107 132 Loss 0.010\n",
            "107 133 Loss 0.002\n",
            "107 134 Loss 0.005\n",
            "107 135 Loss 0.035\n",
            "107 136 Loss 0.003\n",
            "107 137 Loss 0.005\n",
            "107 138 Loss 0.020\n",
            "107 139 Loss 0.022\n",
            "107 140 Loss 0.009\n",
            "107 141 Loss 0.068\n",
            "107 142 Loss 0.031\n",
            "107 143 Loss 0.013\n",
            "107 144 Loss 0.004\n",
            "107 145 Loss 0.015\n",
            "107 146 Loss 0.006\n",
            "107 147 Loss 0.014\n",
            "107 148 Loss 0.007\n",
            "107 149 Loss 0.009\n",
            "107 150 Loss 0.027\n",
            "107 151 Loss 0.014\n",
            "107 152 Loss 0.017\n",
            "107 153 Loss 0.007\n",
            "107 154 Loss 0.027\n",
            "107 155 Loss 0.004\n",
            "107 156 Loss 0.009\n",
            "107 157 Loss 0.004\n",
            "107 158 Loss 0.014\n",
            "107 159 Loss 0.016\n",
            "107 160 Loss 0.005\n",
            "107 161 Loss 0.019\n",
            "107 162 Loss 0.005\n",
            "107 163 Loss 0.012\n",
            "107 164 Loss 0.027\n",
            "107 165 Loss 0.014\n",
            "107 166 Loss 0.007\n",
            "107 167 Loss 0.041\n",
            "107 168 Loss 0.028\n",
            "107 169 Loss 0.005\n",
            "107 170 Loss 0.053\n",
            "107 171 Loss 0.022\n",
            "107 172 Loss 0.041\n",
            "107 173 Loss 0.011\n",
            "107 174 Loss 0.010\n",
            "107 175 Loss 0.003\n",
            "107 176 Loss 0.019\n",
            "107 177 Loss 0.008\n",
            "107 178 Loss 0.006\n",
            "107 179 Loss 0.044\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.81      0.97      0.88        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.86      0.82      0.84        38\n",
            "           9       0.98      0.88      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9083333333333333\n",
            "108 0 Loss 0.014\n",
            "108 1 Loss 0.012\n",
            "108 2 Loss 0.012\n",
            "108 3 Loss 0.056\n",
            "108 4 Loss 0.006\n",
            "108 5 Loss 0.025\n",
            "108 6 Loss 0.013\n",
            "108 7 Loss 0.013\n",
            "108 8 Loss 0.024\n",
            "108 9 Loss 0.049\n",
            "108 10 Loss 0.001\n",
            "108 11 Loss 0.049\n",
            "108 12 Loss 0.009\n",
            "108 13 Loss 0.008\n",
            "108 14 Loss 0.103\n",
            "108 15 Loss 0.012\n",
            "108 16 Loss 0.013\n",
            "108 17 Loss 0.008\n",
            "108 18 Loss 0.010\n",
            "108 19 Loss 0.008\n",
            "108 20 Loss 0.046\n",
            "108 21 Loss 0.011\n",
            "108 22 Loss 0.013\n",
            "108 23 Loss 0.023\n",
            "108 24 Loss 0.006\n",
            "108 25 Loss 0.019\n",
            "108 26 Loss 0.005\n",
            "108 27 Loss 0.016\n",
            "108 28 Loss 0.037\n",
            "108 29 Loss 0.010\n",
            "108 30 Loss 0.004\n",
            "108 31 Loss 0.013\n",
            "108 32 Loss 0.010\n",
            "108 33 Loss 0.005\n",
            "108 34 Loss 0.002\n",
            "108 35 Loss 0.010\n",
            "108 36 Loss 0.003\n",
            "108 37 Loss 0.066\n",
            "108 38 Loss 0.019\n",
            "108 39 Loss 0.011\n",
            "108 40 Loss 0.001\n",
            "108 41 Loss 0.016\n",
            "108 42 Loss 0.114\n",
            "108 43 Loss 0.047\n",
            "108 44 Loss 0.011\n",
            "108 45 Loss 0.008\n",
            "108 46 Loss 0.041\n",
            "108 47 Loss 0.003\n",
            "108 48 Loss 0.004\n",
            "108 49 Loss 0.006\n",
            "108 50 Loss 0.022\n",
            "108 51 Loss 0.008\n",
            "108 52 Loss 0.008\n",
            "108 53 Loss 0.007\n",
            "108 54 Loss 0.019\n",
            "108 55 Loss 0.012\n",
            "108 56 Loss 0.022\n",
            "108 57 Loss 0.013\n",
            "108 58 Loss 0.005\n",
            "108 59 Loss 0.011\n",
            "108 60 Loss 0.075\n",
            "108 61 Loss 0.007\n",
            "108 62 Loss 0.028\n",
            "108 63 Loss 0.009\n",
            "108 64 Loss 0.005\n",
            "108 65 Loss 0.043\n",
            "108 66 Loss 0.015\n",
            "108 67 Loss 0.019\n",
            "108 68 Loss 0.015\n",
            "108 69 Loss 0.004\n",
            "108 70 Loss 0.100\n",
            "108 71 Loss 0.037\n",
            "108 72 Loss 0.008\n",
            "108 73 Loss 0.005\n",
            "108 74 Loss 0.014\n",
            "108 75 Loss 0.002\n",
            "108 76 Loss 0.010\n",
            "108 77 Loss 0.025\n",
            "108 78 Loss 0.013\n",
            "108 79 Loss 0.009\n",
            "108 80 Loss 0.004\n",
            "108 81 Loss 0.006\n",
            "108 82 Loss 0.010\n",
            "108 83 Loss 0.022\n",
            "108 84 Loss 0.035\n",
            "108 85 Loss 0.003\n",
            "108 86 Loss 0.013\n",
            "108 87 Loss 0.025\n",
            "108 88 Loss 0.004\n",
            "108 89 Loss 0.008\n",
            "108 90 Loss 0.030\n",
            "108 91 Loss 0.059\n",
            "108 92 Loss 0.024\n",
            "108 93 Loss 0.027\n",
            "108 94 Loss 0.015\n",
            "108 95 Loss 0.006\n",
            "108 96 Loss 0.010\n",
            "108 97 Loss 0.033\n",
            "108 98 Loss 0.015\n",
            "108 99 Loss 0.030\n",
            "108 100 Loss 0.024\n",
            "108 101 Loss 0.014\n",
            "108 102 Loss 0.050\n",
            "108 103 Loss 0.094\n",
            "108 104 Loss 0.030\n",
            "108 105 Loss 0.022\n",
            "108 106 Loss 0.049\n",
            "108 107 Loss 0.028\n",
            "108 108 Loss 0.020\n",
            "108 109 Loss 0.029\n",
            "108 110 Loss 0.063\n",
            "108 111 Loss 0.006\n",
            "108 112 Loss 0.053\n",
            "108 113 Loss 0.010\n",
            "108 114 Loss 0.013\n",
            "108 115 Loss 0.009\n",
            "108 116 Loss 0.006\n",
            "108 117 Loss 0.044\n",
            "108 118 Loss 0.020\n",
            "108 119 Loss 0.066\n",
            "108 120 Loss 0.010\n",
            "108 121 Loss 0.019\n",
            "108 122 Loss 0.027\n",
            "108 123 Loss 0.033\n",
            "108 124 Loss 0.022\n",
            "108 125 Loss 0.004\n",
            "108 126 Loss 0.007\n",
            "108 127 Loss 0.009\n",
            "108 128 Loss 0.041\n",
            "108 129 Loss 0.004\n",
            "108 130 Loss 0.015\n",
            "108 131 Loss 0.013\n",
            "108 132 Loss 0.081\n",
            "108 133 Loss 0.012\n",
            "108 134 Loss 0.014\n",
            "108 135 Loss 0.030\n",
            "108 136 Loss 0.025\n",
            "108 137 Loss 0.035\n",
            "108 138 Loss 0.005\n",
            "108 139 Loss 0.005\n",
            "108 140 Loss 0.022\n",
            "108 141 Loss 0.012\n",
            "108 142 Loss 0.007\n",
            "108 143 Loss 0.017\n",
            "108 144 Loss 0.022\n",
            "108 145 Loss 0.017\n",
            "108 146 Loss 0.034\n",
            "108 147 Loss 0.029\n",
            "108 148 Loss 0.020\n",
            "108 149 Loss 0.028\n",
            "108 150 Loss 0.050\n",
            "108 151 Loss 0.157\n",
            "108 152 Loss 0.026\n",
            "108 153 Loss 0.031\n",
            "108 154 Loss 0.035\n",
            "108 155 Loss 0.005\n",
            "108 156 Loss 0.040\n",
            "108 157 Loss 0.010\n",
            "108 158 Loss 0.031\n",
            "108 159 Loss 0.032\n",
            "108 160 Loss 0.010\n",
            "108 161 Loss 0.011\n",
            "108 162 Loss 0.005\n",
            "108 163 Loss 0.014\n",
            "108 164 Loss 0.015\n",
            "108 165 Loss 0.014\n",
            "108 166 Loss 0.018\n",
            "108 167 Loss 0.013\n",
            "108 168 Loss 0.005\n",
            "108 169 Loss 0.018\n",
            "108 170 Loss 0.035\n",
            "108 171 Loss 0.018\n",
            "108 172 Loss 0.003\n",
            "108 173 Loss 0.009\n",
            "108 174 Loss 0.007\n",
            "108 175 Loss 0.014\n",
            "108 176 Loss 0.067\n",
            "108 177 Loss 0.026\n",
            "108 178 Loss 0.009\n",
            "108 179 Loss 0.005\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.79      0.96      0.87        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.81      0.97      0.88        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.86      0.79      0.82        38\n",
            "           9       0.98      0.88      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "109 0 Loss 0.021\n",
            "109 1 Loss 0.058\n",
            "109 2 Loss 0.028\n",
            "109 3 Loss 0.039\n",
            "109 4 Loss 0.059\n",
            "109 5 Loss 0.017\n",
            "109 6 Loss 0.022\n",
            "109 7 Loss 0.029\n",
            "109 8 Loss 0.019\n",
            "109 9 Loss 0.005\n",
            "109 10 Loss 0.040\n",
            "109 11 Loss 0.010\n",
            "109 12 Loss 0.033\n",
            "109 13 Loss 0.027\n",
            "109 14 Loss 0.005\n",
            "109 15 Loss 0.027\n",
            "109 16 Loss 0.032\n",
            "109 17 Loss 0.028\n",
            "109 18 Loss 0.010\n",
            "109 19 Loss 0.009\n",
            "109 20 Loss 0.015\n",
            "109 21 Loss 0.016\n",
            "109 22 Loss 0.005\n",
            "109 23 Loss 0.028\n",
            "109 24 Loss 0.041\n",
            "109 25 Loss 0.002\n",
            "109 26 Loss 0.049\n",
            "109 27 Loss 0.018\n",
            "109 28 Loss 0.057\n",
            "109 29 Loss 0.012\n",
            "109 30 Loss 0.035\n",
            "109 31 Loss 0.024\n",
            "109 32 Loss 0.018\n",
            "109 33 Loss 0.004\n",
            "109 34 Loss 0.041\n",
            "109 35 Loss 0.009\n",
            "109 36 Loss 0.003\n",
            "109 37 Loss 0.017\n",
            "109 38 Loss 0.017\n",
            "109 39 Loss 0.015\n",
            "109 40 Loss 0.001\n",
            "109 41 Loss 0.016\n",
            "109 42 Loss 0.039\n",
            "109 43 Loss 0.021\n",
            "109 44 Loss 0.022\n",
            "109 45 Loss 0.190\n",
            "109 46 Loss 0.016\n",
            "109 47 Loss 0.005\n",
            "109 48 Loss 0.008\n",
            "109 49 Loss 0.011\n",
            "109 50 Loss 0.005\n",
            "109 51 Loss 0.010\n",
            "109 52 Loss 0.080\n",
            "109 53 Loss 0.025\n",
            "109 54 Loss 0.016\n",
            "109 55 Loss 0.040\n",
            "109 56 Loss 0.010\n",
            "109 57 Loss 0.036\n",
            "109 58 Loss 0.018\n",
            "109 59 Loss 0.020\n",
            "109 60 Loss 0.014\n",
            "109 61 Loss 0.013\n",
            "109 62 Loss 0.029\n",
            "109 63 Loss 0.008\n",
            "109 64 Loss 0.006\n",
            "109 65 Loss 0.008\n",
            "109 66 Loss 0.010\n",
            "109 67 Loss 0.004\n",
            "109 68 Loss 0.026\n",
            "109 69 Loss 0.012\n",
            "109 70 Loss 0.003\n",
            "109 71 Loss 0.140\n",
            "109 72 Loss 0.009\n",
            "109 73 Loss 0.005\n",
            "109 74 Loss 0.005\n",
            "109 75 Loss 0.006\n",
            "109 76 Loss 0.013\n",
            "109 77 Loss 0.009\n",
            "109 78 Loss 0.040\n",
            "109 79 Loss 0.007\n",
            "109 80 Loss 0.124\n",
            "109 81 Loss 0.001\n",
            "109 82 Loss 0.009\n",
            "109 83 Loss 0.055\n",
            "109 84 Loss 0.027\n",
            "109 85 Loss 0.010\n",
            "109 86 Loss 0.027\n",
            "109 87 Loss 0.029\n",
            "109 88 Loss 0.011\n",
            "109 89 Loss 0.014\n",
            "109 90 Loss 0.002\n",
            "109 91 Loss 0.012\n",
            "109 92 Loss 0.019\n",
            "109 93 Loss 0.013\n",
            "109 94 Loss 0.006\n",
            "109 95 Loss 0.006\n",
            "109 96 Loss 0.006\n",
            "109 97 Loss 0.046\n",
            "109 98 Loss 0.102\n",
            "109 99 Loss 0.006\n",
            "109 100 Loss 0.012\n",
            "109 101 Loss 0.013\n",
            "109 102 Loss 0.073\n",
            "109 103 Loss 0.008\n",
            "109 104 Loss 0.006\n",
            "109 105 Loss 0.005\n",
            "109 106 Loss 0.070\n",
            "109 107 Loss 0.003\n",
            "109 108 Loss 0.003\n",
            "109 109 Loss 0.013\n",
            "109 110 Loss 0.011\n",
            "109 111 Loss 0.013\n",
            "109 112 Loss 0.003\n",
            "109 113 Loss 0.021\n",
            "109 114 Loss 0.032\n",
            "109 115 Loss 0.004\n",
            "109 116 Loss 0.012\n",
            "109 117 Loss 0.036\n",
            "109 118 Loss 0.016\n",
            "109 119 Loss 0.020\n",
            "109 120 Loss 0.025\n",
            "109 121 Loss 0.011\n",
            "109 122 Loss 0.011\n",
            "109 123 Loss 0.013\n",
            "109 124 Loss 0.028\n",
            "109 125 Loss 0.035\n",
            "109 126 Loss 0.005\n",
            "109 127 Loss 0.002\n",
            "109 128 Loss 0.044\n",
            "109 129 Loss 0.036\n",
            "109 130 Loss 0.003\n",
            "109 131 Loss 0.016\n",
            "109 132 Loss 0.006\n",
            "109 133 Loss 0.008\n",
            "109 134 Loss 0.010\n",
            "109 135 Loss 0.010\n",
            "109 136 Loss 0.040\n",
            "109 137 Loss 0.008\n",
            "109 138 Loss 0.008\n",
            "109 139 Loss 0.041\n",
            "109 140 Loss 0.009\n",
            "109 141 Loss 0.006\n",
            "109 142 Loss 0.002\n",
            "109 143 Loss 0.045\n",
            "109 144 Loss 0.004\n",
            "109 145 Loss 0.087\n",
            "109 146 Loss 0.006\n",
            "109 147 Loss 0.028\n",
            "109 148 Loss 0.036\n",
            "109 149 Loss 0.014\n",
            "109 150 Loss 0.004\n",
            "109 151 Loss 0.009\n",
            "109 152 Loss 0.010\n",
            "109 153 Loss 0.046\n",
            "109 154 Loss 0.019\n",
            "109 155 Loss 0.008\n",
            "109 156 Loss 0.046\n",
            "109 157 Loss 0.012\n",
            "109 158 Loss 0.004\n",
            "109 159 Loss 0.009\n",
            "109 160 Loss 0.012\n",
            "109 161 Loss 0.012\n",
            "109 162 Loss 0.015\n",
            "109 163 Loss 0.015\n",
            "109 164 Loss 0.014\n",
            "109 165 Loss 0.034\n",
            "109 166 Loss 0.015\n",
            "109 167 Loss 0.015\n",
            "109 168 Loss 0.011\n",
            "109 169 Loss 0.010\n",
            "109 170 Loss 0.037\n",
            "109 171 Loss 0.018\n",
            "109 172 Loss 0.042\n",
            "109 173 Loss 0.003\n",
            "109 174 Loss 0.004\n",
            "109 175 Loss 0.006\n",
            "109 176 Loss 0.019\n",
            "109 177 Loss 0.014\n",
            "109 178 Loss 0.005\n",
            "109 179 Loss 0.061\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.90      0.81      0.85        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.90      0.90      0.90       360\n",
            "\n",
            "0.9\n",
            "110 0 Loss 0.009\n",
            "110 1 Loss 0.011\n",
            "110 2 Loss 0.010\n",
            "110 3 Loss 0.033\n",
            "110 4 Loss 0.009\n",
            "110 5 Loss 0.033\n",
            "110 6 Loss 0.008\n",
            "110 7 Loss 0.016\n",
            "110 8 Loss 0.004\n",
            "110 9 Loss 0.004\n",
            "110 10 Loss 0.012\n",
            "110 11 Loss 0.021\n",
            "110 12 Loss 0.016\n",
            "110 13 Loss 0.081\n",
            "110 14 Loss 0.014\n",
            "110 15 Loss 0.006\n",
            "110 16 Loss 0.011\n",
            "110 17 Loss 0.051\n",
            "110 18 Loss 0.008\n",
            "110 19 Loss 0.049\n",
            "110 20 Loss 0.013\n",
            "110 21 Loss 0.004\n",
            "110 22 Loss 0.094\n",
            "110 23 Loss 0.008\n",
            "110 24 Loss 0.013\n",
            "110 25 Loss 0.004\n",
            "110 26 Loss 0.021\n",
            "110 27 Loss 0.021\n",
            "110 28 Loss 0.034\n",
            "110 29 Loss 0.015\n",
            "110 30 Loss 0.017\n",
            "110 31 Loss 0.027\n",
            "110 32 Loss 0.038\n",
            "110 33 Loss 0.037\n",
            "110 34 Loss 0.008\n",
            "110 35 Loss 0.015\n",
            "110 36 Loss 0.016\n",
            "110 37 Loss 0.016\n",
            "110 38 Loss 0.004\n",
            "110 39 Loss 0.019\n",
            "110 40 Loss 0.015\n",
            "110 41 Loss 0.003\n",
            "110 42 Loss 0.025\n",
            "110 43 Loss 0.006\n",
            "110 44 Loss 0.016\n",
            "110 45 Loss 0.004\n",
            "110 46 Loss 0.002\n",
            "110 47 Loss 0.015\n",
            "110 48 Loss 0.028\n",
            "110 49 Loss 0.015\n",
            "110 50 Loss 0.020\n",
            "110 51 Loss 0.002\n",
            "110 52 Loss 0.033\n",
            "110 53 Loss 0.042\n",
            "110 54 Loss 0.035\n",
            "110 55 Loss 0.008\n",
            "110 56 Loss 0.015\n",
            "110 57 Loss 0.045\n",
            "110 58 Loss 0.007\n",
            "110 59 Loss 0.008\n",
            "110 60 Loss 0.020\n",
            "110 61 Loss 0.009\n",
            "110 62 Loss 0.044\n",
            "110 63 Loss 0.065\n",
            "110 64 Loss 0.006\n",
            "110 65 Loss 0.025\n",
            "110 66 Loss 0.008\n",
            "110 67 Loss 0.006\n",
            "110 68 Loss 0.014\n",
            "110 69 Loss 0.003\n",
            "110 70 Loss 0.011\n",
            "110 71 Loss 0.014\n",
            "110 72 Loss 0.011\n",
            "110 73 Loss 0.016\n",
            "110 74 Loss 0.056\n",
            "110 75 Loss 0.006\n",
            "110 76 Loss 0.008\n",
            "110 77 Loss 0.026\n",
            "110 78 Loss 0.068\n",
            "110 79 Loss 0.010\n",
            "110 80 Loss 0.011\n",
            "110 81 Loss 0.003\n",
            "110 82 Loss 0.070\n",
            "110 83 Loss 0.026\n",
            "110 84 Loss 0.029\n",
            "110 85 Loss 0.019\n",
            "110 86 Loss 0.010\n",
            "110 87 Loss 0.015\n",
            "110 88 Loss 0.028\n",
            "110 89 Loss 0.008\n",
            "110 90 Loss 0.024\n",
            "110 91 Loss 0.007\n",
            "110 92 Loss 0.009\n",
            "110 93 Loss 0.008\n",
            "110 94 Loss 0.026\n",
            "110 95 Loss 0.008\n",
            "110 96 Loss 0.012\n",
            "110 97 Loss 0.015\n",
            "110 98 Loss 0.008\n",
            "110 99 Loss 0.097\n",
            "110 100 Loss 0.013\n",
            "110 101 Loss 0.006\n",
            "110 102 Loss 0.008\n",
            "110 103 Loss 0.020\n",
            "110 104 Loss 0.004\n",
            "110 105 Loss 0.012\n",
            "110 106 Loss 0.034\n",
            "110 107 Loss 0.067\n",
            "110 108 Loss 0.006\n",
            "110 109 Loss 0.008\n",
            "110 110 Loss 0.057\n",
            "110 111 Loss 0.017\n",
            "110 112 Loss 0.011\n",
            "110 113 Loss 0.025\n",
            "110 114 Loss 0.050\n",
            "110 115 Loss 0.004\n",
            "110 116 Loss 0.006\n",
            "110 117 Loss 0.012\n",
            "110 118 Loss 0.006\n",
            "110 119 Loss 0.017\n",
            "110 120 Loss 0.032\n",
            "110 121 Loss 0.042\n",
            "110 122 Loss 0.007\n",
            "110 123 Loss 0.053\n",
            "110 124 Loss 0.013\n",
            "110 125 Loss 0.048\n",
            "110 126 Loss 0.029\n",
            "110 127 Loss 0.163\n",
            "110 128 Loss 0.013\n",
            "110 129 Loss 0.019\n",
            "110 130 Loss 0.028\n",
            "110 131 Loss 0.044\n",
            "110 132 Loss 0.008\n",
            "110 133 Loss 0.008\n",
            "110 134 Loss 0.008\n",
            "110 135 Loss 0.039\n",
            "110 136 Loss 0.011\n",
            "110 137 Loss 0.018\n",
            "110 138 Loss 0.031\n",
            "110 139 Loss 0.018\n",
            "110 140 Loss 0.005\n",
            "110 141 Loss 0.020\n",
            "110 142 Loss 0.017\n",
            "110 143 Loss 0.009\n",
            "110 144 Loss 0.008\n",
            "110 145 Loss 0.020\n",
            "110 146 Loss 0.008\n",
            "110 147 Loss 0.038\n",
            "110 148 Loss 0.030\n",
            "110 149 Loss 0.039\n",
            "110 150 Loss 0.015\n",
            "110 151 Loss 0.025\n",
            "110 152 Loss 0.023\n",
            "110 153 Loss 0.007\n",
            "110 154 Loss 0.008\n",
            "110 155 Loss 0.028\n",
            "110 156 Loss 0.082\n",
            "110 157 Loss 0.009\n",
            "110 158 Loss 0.014\n",
            "110 159 Loss 0.022\n",
            "110 160 Loss 0.003\n",
            "110 161 Loss 0.014\n",
            "110 162 Loss 0.011\n",
            "110 163 Loss 0.006\n",
            "110 164 Loss 0.014\n",
            "110 165 Loss 0.009\n",
            "110 166 Loss 0.116\n",
            "110 167 Loss 0.033\n",
            "110 168 Loss 0.003\n",
            "110 169 Loss 0.004\n",
            "110 170 Loss 0.029\n",
            "110 171 Loss 0.009\n",
            "110 172 Loss 0.033\n",
            "110 173 Loss 0.013\n",
            "110 174 Loss 0.015\n",
            "110 175 Loss 0.008\n",
            "110 176 Loss 0.007\n",
            "110 177 Loss 0.006\n",
            "110 178 Loss 0.035\n",
            "110 179 Loss 0.004\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.84      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.81      0.97      0.88        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.90      0.81      0.85        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.98      0.88      0.93        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "111 0 Loss 0.007\n",
            "111 1 Loss 0.109\n",
            "111 2 Loss 0.046\n",
            "111 3 Loss 0.060\n",
            "111 4 Loss 0.012\n",
            "111 5 Loss 0.022\n",
            "111 6 Loss 0.051\n",
            "111 7 Loss 0.003\n",
            "111 8 Loss 0.009\n",
            "111 9 Loss 0.020\n",
            "111 10 Loss 0.017\n",
            "111 11 Loss 0.015\n",
            "111 12 Loss 0.037\n",
            "111 13 Loss 0.009\n",
            "111 14 Loss 0.031\n",
            "111 15 Loss 0.006\n",
            "111 16 Loss 0.008\n",
            "111 17 Loss 0.023\n",
            "111 18 Loss 0.006\n",
            "111 19 Loss 0.007\n",
            "111 20 Loss 0.007\n",
            "111 21 Loss 0.008\n",
            "111 22 Loss 0.009\n",
            "111 23 Loss 0.007\n",
            "111 24 Loss 0.002\n",
            "111 25 Loss 0.015\n",
            "111 26 Loss 0.022\n",
            "111 27 Loss 0.007\n",
            "111 28 Loss 0.031\n",
            "111 29 Loss 0.007\n",
            "111 30 Loss 0.006\n",
            "111 31 Loss 0.020\n",
            "111 32 Loss 0.014\n",
            "111 33 Loss 0.003\n",
            "111 34 Loss 0.007\n",
            "111 35 Loss 0.013\n",
            "111 36 Loss 0.019\n",
            "111 37 Loss 0.002\n",
            "111 38 Loss 0.026\n",
            "111 39 Loss 0.005\n",
            "111 40 Loss 0.111\n",
            "111 41 Loss 0.009\n",
            "111 42 Loss 0.005\n",
            "111 43 Loss 0.019\n",
            "111 44 Loss 0.040\n",
            "111 45 Loss 0.006\n",
            "111 46 Loss 0.025\n",
            "111 47 Loss 0.029\n",
            "111 48 Loss 0.004\n",
            "111 49 Loss 0.005\n",
            "111 50 Loss 0.012\n",
            "111 51 Loss 0.019\n",
            "111 52 Loss 0.062\n",
            "111 53 Loss 0.014\n",
            "111 54 Loss 0.004\n",
            "111 55 Loss 0.022\n",
            "111 56 Loss 0.034\n",
            "111 57 Loss 0.012\n",
            "111 58 Loss 0.007\n",
            "111 59 Loss 0.023\n",
            "111 60 Loss 0.021\n",
            "111 61 Loss 0.029\n",
            "111 62 Loss 0.011\n",
            "111 63 Loss 0.028\n",
            "111 64 Loss 0.014\n",
            "111 65 Loss 0.005\n",
            "111 66 Loss 0.010\n",
            "111 67 Loss 0.010\n",
            "111 68 Loss 0.018\n",
            "111 69 Loss 0.019\n",
            "111 70 Loss 0.010\n",
            "111 71 Loss 0.005\n",
            "111 72 Loss 0.077\n",
            "111 73 Loss 0.007\n",
            "111 74 Loss 0.002\n",
            "111 75 Loss 0.042\n",
            "111 76 Loss 0.107\n",
            "111 77 Loss 0.003\n",
            "111 78 Loss 0.030\n",
            "111 79 Loss 0.010\n",
            "111 80 Loss 0.021\n",
            "111 81 Loss 0.042\n",
            "111 82 Loss 0.015\n",
            "111 83 Loss 0.022\n",
            "111 84 Loss 0.015\n",
            "111 85 Loss 0.014\n",
            "111 86 Loss 0.007\n",
            "111 87 Loss 0.036\n",
            "111 88 Loss 0.028\n",
            "111 89 Loss 0.006\n",
            "111 90 Loss 0.024\n",
            "111 91 Loss 0.123\n",
            "111 92 Loss 0.002\n",
            "111 93 Loss 0.050\n",
            "111 94 Loss 0.008\n",
            "111 95 Loss 0.018\n",
            "111 96 Loss 0.007\n",
            "111 97 Loss 0.014\n",
            "111 98 Loss 0.023\n",
            "111 99 Loss 0.004\n",
            "111 100 Loss 0.008\n",
            "111 101 Loss 0.004\n",
            "111 102 Loss 0.004\n",
            "111 103 Loss 0.048\n",
            "111 104 Loss 0.009\n",
            "111 105 Loss 0.009\n",
            "111 106 Loss 0.005\n",
            "111 107 Loss 0.001\n",
            "111 108 Loss 0.005\n",
            "111 109 Loss 0.007\n",
            "111 110 Loss 0.024\n",
            "111 111 Loss 0.015\n",
            "111 112 Loss 0.086\n",
            "111 113 Loss 0.023\n",
            "111 114 Loss 0.032\n",
            "111 115 Loss 0.006\n",
            "111 116 Loss 0.031\n",
            "111 117 Loss 0.032\n",
            "111 118 Loss 0.005\n",
            "111 119 Loss 0.085\n",
            "111 120 Loss 0.012\n",
            "111 121 Loss 0.016\n",
            "111 122 Loss 0.025\n",
            "111 123 Loss 0.057\n",
            "111 124 Loss 0.021\n",
            "111 125 Loss 0.035\n",
            "111 126 Loss 0.025\n",
            "111 127 Loss 0.041\n",
            "111 128 Loss 0.001\n",
            "111 129 Loss 0.006\n",
            "111 130 Loss 0.016\n",
            "111 131 Loss 0.010\n",
            "111 132 Loss 0.004\n",
            "111 133 Loss 0.016\n",
            "111 134 Loss 0.012\n",
            "111 135 Loss 0.020\n",
            "111 136 Loss 0.006\n",
            "111 137 Loss 0.027\n",
            "111 138 Loss 0.003\n",
            "111 139 Loss 0.013\n",
            "111 140 Loss 0.025\n",
            "111 141 Loss 0.012\n",
            "111 142 Loss 0.013\n",
            "111 143 Loss 0.003\n",
            "111 144 Loss 0.019\n",
            "111 145 Loss 0.005\n",
            "111 146 Loss 0.006\n",
            "111 147 Loss 0.019\n",
            "111 148 Loss 0.005\n",
            "111 149 Loss 0.015\n",
            "111 150 Loss 0.060\n",
            "111 151 Loss 0.003\n",
            "111 152 Loss 0.037\n",
            "111 153 Loss 0.001\n",
            "111 154 Loss 0.006\n",
            "111 155 Loss 0.029\n",
            "111 156 Loss 0.006\n",
            "111 157 Loss 0.016\n",
            "111 158 Loss 0.006\n",
            "111 159 Loss 0.038\n",
            "111 160 Loss 0.017\n",
            "111 161 Loss 0.047\n",
            "111 162 Loss 0.005\n",
            "111 163 Loss 0.040\n",
            "111 164 Loss 0.019\n",
            "111 165 Loss 0.013\n",
            "111 166 Loss 0.013\n",
            "111 167 Loss 0.042\n",
            "111 168 Loss 0.009\n",
            "111 169 Loss 0.044\n",
            "111 170 Loss 0.004\n",
            "111 171 Loss 0.003\n",
            "111 172 Loss 0.035\n",
            "111 173 Loss 0.077\n",
            "111 174 Loss 0.065\n",
            "111 175 Loss 0.021\n",
            "111 176 Loss 0.007\n",
            "111 177 Loss 0.003\n",
            "111 178 Loss 0.036\n",
            "111 179 Loss 0.030\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.84      0.93      0.88        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.82      0.82      0.82        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "112 0 Loss 0.029\n",
            "112 1 Loss 0.033\n",
            "112 2 Loss 0.016\n",
            "112 3 Loss 0.031\n",
            "112 4 Loss 0.010\n",
            "112 5 Loss 0.020\n",
            "112 6 Loss 0.013\n",
            "112 7 Loss 0.026\n",
            "112 8 Loss 0.010\n",
            "112 9 Loss 0.008\n",
            "112 10 Loss 0.010\n",
            "112 11 Loss 0.014\n",
            "112 12 Loss 0.012\n",
            "112 13 Loss 0.010\n",
            "112 14 Loss 0.011\n",
            "112 15 Loss 0.004\n",
            "112 16 Loss 0.012\n",
            "112 17 Loss 0.015\n",
            "112 18 Loss 0.011\n",
            "112 19 Loss 0.027\n",
            "112 20 Loss 0.017\n",
            "112 21 Loss 0.021\n",
            "112 22 Loss 0.055\n",
            "112 23 Loss 0.041\n",
            "112 24 Loss 0.010\n",
            "112 25 Loss 0.012\n",
            "112 26 Loss 0.037\n",
            "112 27 Loss 0.019\n",
            "112 28 Loss 0.155\n",
            "112 29 Loss 0.007\n",
            "112 30 Loss 0.004\n",
            "112 31 Loss 0.046\n",
            "112 32 Loss 0.075\n",
            "112 33 Loss 0.011\n",
            "112 34 Loss 0.008\n",
            "112 35 Loss 0.006\n",
            "112 36 Loss 0.005\n",
            "112 37 Loss 0.012\n",
            "112 38 Loss 0.010\n",
            "112 39 Loss 0.013\n",
            "112 40 Loss 0.008\n",
            "112 41 Loss 0.006\n",
            "112 42 Loss 0.043\n",
            "112 43 Loss 0.008\n",
            "112 44 Loss 0.015\n",
            "112 45 Loss 0.068\n",
            "112 46 Loss 0.011\n",
            "112 47 Loss 0.031\n",
            "112 48 Loss 0.024\n",
            "112 49 Loss 0.005\n",
            "112 50 Loss 0.036\n",
            "112 51 Loss 0.003\n",
            "112 52 Loss 0.041\n",
            "112 53 Loss 0.012\n",
            "112 54 Loss 0.013\n",
            "112 55 Loss 0.107\n",
            "112 56 Loss 0.016\n",
            "112 57 Loss 0.012\n",
            "112 58 Loss 0.019\n",
            "112 59 Loss 0.007\n",
            "112 60 Loss 0.027\n",
            "112 61 Loss 0.010\n",
            "112 62 Loss 0.030\n",
            "112 63 Loss 0.021\n",
            "112 64 Loss 0.044\n",
            "112 65 Loss 0.003\n",
            "112 66 Loss 0.008\n",
            "112 67 Loss 0.035\n",
            "112 68 Loss 0.035\n",
            "112 69 Loss 0.040\n",
            "112 70 Loss 0.009\n",
            "112 71 Loss 0.036\n",
            "112 72 Loss 0.004\n",
            "112 73 Loss 0.014\n",
            "112 74 Loss 0.014\n",
            "112 75 Loss 0.079\n",
            "112 76 Loss 0.010\n",
            "112 77 Loss 0.013\n",
            "112 78 Loss 0.002\n",
            "112 79 Loss 0.013\n",
            "112 80 Loss 0.003\n",
            "112 81 Loss 0.006\n",
            "112 82 Loss 0.024\n",
            "112 83 Loss 0.006\n",
            "112 84 Loss 0.027\n",
            "112 85 Loss 0.003\n",
            "112 86 Loss 0.059\n",
            "112 87 Loss 0.021\n",
            "112 88 Loss 0.020\n",
            "112 89 Loss 0.069\n",
            "112 90 Loss 0.011\n",
            "112 91 Loss 0.007\n",
            "112 92 Loss 0.005\n",
            "112 93 Loss 0.001\n",
            "112 94 Loss 0.007\n",
            "112 95 Loss 0.016\n",
            "112 96 Loss 0.002\n",
            "112 97 Loss 0.005\n",
            "112 98 Loss 0.080\n",
            "112 99 Loss 0.011\n",
            "112 100 Loss 0.003\n",
            "112 101 Loss 0.026\n",
            "112 102 Loss 0.025\n",
            "112 103 Loss 0.019\n",
            "112 104 Loss 0.004\n",
            "112 105 Loss 0.009\n",
            "112 106 Loss 0.008\n",
            "112 107 Loss 0.004\n",
            "112 108 Loss 0.003\n",
            "112 109 Loss 0.011\n",
            "112 110 Loss 0.004\n",
            "112 111 Loss 0.028\n",
            "112 112 Loss 0.010\n",
            "112 113 Loss 0.012\n",
            "112 114 Loss 0.010\n",
            "112 115 Loss 0.004\n",
            "112 116 Loss 0.048\n",
            "112 117 Loss 0.015\n",
            "112 118 Loss 0.027\n",
            "112 119 Loss 0.040\n",
            "112 120 Loss 0.024\n",
            "112 121 Loss 0.052\n",
            "112 122 Loss 0.011\n",
            "112 123 Loss 0.007\n",
            "112 124 Loss 0.021\n",
            "112 125 Loss 0.060\n",
            "112 126 Loss 0.024\n",
            "112 127 Loss 0.006\n",
            "112 128 Loss 0.021\n",
            "112 129 Loss 0.012\n",
            "112 130 Loss 0.015\n",
            "112 131 Loss 0.022\n",
            "112 132 Loss 0.014\n",
            "112 133 Loss 0.005\n",
            "112 134 Loss 0.039\n",
            "112 135 Loss 0.029\n",
            "112 136 Loss 0.042\n",
            "112 137 Loss 0.007\n",
            "112 138 Loss 0.012\n",
            "112 139 Loss 0.008\n",
            "112 140 Loss 0.009\n",
            "112 141 Loss 0.029\n",
            "112 142 Loss 0.019\n",
            "112 143 Loss 0.011\n",
            "112 144 Loss 0.013\n",
            "112 145 Loss 0.014\n",
            "112 146 Loss 0.004\n",
            "112 147 Loss 0.004\n",
            "112 148 Loss 0.019\n",
            "112 149 Loss 0.014\n",
            "112 150 Loss 0.046\n",
            "112 151 Loss 0.037\n",
            "112 152 Loss 0.015\n",
            "112 153 Loss 0.006\n",
            "112 154 Loss 0.011\n",
            "112 155 Loss 0.021\n",
            "112 156 Loss 0.007\n",
            "112 157 Loss 0.043\n",
            "112 158 Loss 0.010\n",
            "112 159 Loss 0.012\n",
            "112 160 Loss 0.026\n",
            "112 161 Loss 0.104\n",
            "112 162 Loss 0.047\n",
            "112 163 Loss 0.010\n",
            "112 164 Loss 0.015\n",
            "112 165 Loss 0.007\n",
            "112 166 Loss 0.024\n",
            "112 167 Loss 0.005\n",
            "112 168 Loss 0.021\n",
            "112 169 Loss 0.005\n",
            "112 170 Loss 0.006\n",
            "112 171 Loss 0.009\n",
            "112 172 Loss 0.034\n",
            "112 173 Loss 0.011\n",
            "112 174 Loss 0.010\n",
            "112 175 Loss 0.017\n",
            "112 176 Loss 0.095\n",
            "112 177 Loss 0.010\n",
            "112 178 Loss 0.022\n",
            "112 179 Loss 0.004\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.79      0.96      0.87        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.84      0.89        37\n",
            "           4       0.97      0.97      0.97        31\n",
            "           5       0.81      0.97      0.88        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.86      0.79      0.82        38\n",
            "           9       0.98      0.88      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "113 0 Loss 0.016\n",
            "113 1 Loss 0.023\n",
            "113 2 Loss 0.011\n",
            "113 3 Loss 0.007\n",
            "113 4 Loss 0.008\n",
            "113 5 Loss 0.006\n",
            "113 6 Loss 0.015\n",
            "113 7 Loss 0.005\n",
            "113 8 Loss 0.008\n",
            "113 9 Loss 0.013\n",
            "113 10 Loss 0.008\n",
            "113 11 Loss 0.003\n",
            "113 12 Loss 0.036\n",
            "113 13 Loss 0.012\n",
            "113 14 Loss 0.035\n",
            "113 15 Loss 0.008\n",
            "113 16 Loss 0.008\n",
            "113 17 Loss 0.010\n",
            "113 18 Loss 0.004\n",
            "113 19 Loss 0.083\n",
            "113 20 Loss 0.006\n",
            "113 21 Loss 0.013\n",
            "113 22 Loss 0.005\n",
            "113 23 Loss 0.048\n",
            "113 24 Loss 0.049\n",
            "113 25 Loss 0.012\n",
            "113 26 Loss 0.004\n",
            "113 27 Loss 0.016\n",
            "113 28 Loss 0.016\n",
            "113 29 Loss 0.017\n",
            "113 30 Loss 0.108\n",
            "113 31 Loss 0.044\n",
            "113 32 Loss 0.012\n",
            "113 33 Loss 0.022\n",
            "113 34 Loss 0.017\n",
            "113 35 Loss 0.007\n",
            "113 36 Loss 0.031\n",
            "113 37 Loss 0.010\n",
            "113 38 Loss 0.006\n",
            "113 39 Loss 0.011\n",
            "113 40 Loss 0.004\n",
            "113 41 Loss 0.006\n",
            "113 42 Loss 0.029\n",
            "113 43 Loss 0.072\n",
            "113 44 Loss 0.017\n",
            "113 45 Loss 0.048\n",
            "113 46 Loss 0.021\n",
            "113 47 Loss 0.005\n",
            "113 48 Loss 0.008\n",
            "113 49 Loss 0.032\n",
            "113 50 Loss 0.010\n",
            "113 51 Loss 0.006\n",
            "113 52 Loss 0.005\n",
            "113 53 Loss 0.002\n",
            "113 54 Loss 0.035\n",
            "113 55 Loss 0.019\n",
            "113 56 Loss 0.026\n",
            "113 57 Loss 0.021\n",
            "113 58 Loss 0.156\n",
            "113 59 Loss 0.009\n",
            "113 60 Loss 0.019\n",
            "113 61 Loss 0.015\n",
            "113 62 Loss 0.011\n",
            "113 63 Loss 0.006\n",
            "113 64 Loss 0.048\n",
            "113 65 Loss 0.003\n",
            "113 66 Loss 0.010\n",
            "113 67 Loss 0.005\n",
            "113 68 Loss 0.036\n",
            "113 69 Loss 0.006\n",
            "113 70 Loss 0.010\n",
            "113 71 Loss 0.020\n",
            "113 72 Loss 0.005\n",
            "113 73 Loss 0.022\n",
            "113 74 Loss 0.005\n",
            "113 75 Loss 0.027\n",
            "113 76 Loss 0.002\n",
            "113 77 Loss 0.014\n",
            "113 78 Loss 0.016\n",
            "113 79 Loss 0.031\n",
            "113 80 Loss 0.025\n",
            "113 81 Loss 0.011\n",
            "113 82 Loss 0.013\n",
            "113 83 Loss 0.005\n",
            "113 84 Loss 0.004\n",
            "113 85 Loss 0.022\n",
            "113 86 Loss 0.001\n",
            "113 87 Loss 0.046\n",
            "113 88 Loss 0.009\n",
            "113 89 Loss 0.041\n",
            "113 90 Loss 0.017\n",
            "113 91 Loss 0.019\n",
            "113 92 Loss 0.006\n",
            "113 93 Loss 0.011\n",
            "113 94 Loss 0.017\n",
            "113 95 Loss 0.023\n",
            "113 96 Loss 0.008\n",
            "113 97 Loss 0.021\n",
            "113 98 Loss 0.039\n",
            "113 99 Loss 0.004\n",
            "113 100 Loss 0.057\n",
            "113 101 Loss 0.009\n",
            "113 102 Loss 0.020\n",
            "113 103 Loss 0.014\n",
            "113 104 Loss 0.001\n",
            "113 105 Loss 0.005\n",
            "113 106 Loss 0.014\n",
            "113 107 Loss 0.023\n",
            "113 108 Loss 0.008\n",
            "113 109 Loss 0.008\n",
            "113 110 Loss 0.015\n",
            "113 111 Loss 0.015\n",
            "113 112 Loss 0.027\n",
            "113 113 Loss 0.019\n",
            "113 114 Loss 0.008\n",
            "113 115 Loss 0.025\n",
            "113 116 Loss 0.034\n",
            "113 117 Loss 0.050\n",
            "113 118 Loss 0.013\n",
            "113 119 Loss 0.017\n",
            "113 120 Loss 0.033\n",
            "113 121 Loss 0.003\n",
            "113 122 Loss 0.036\n",
            "113 123 Loss 0.016\n",
            "113 124 Loss 0.017\n",
            "113 125 Loss 0.013\n",
            "113 126 Loss 0.036\n",
            "113 127 Loss 0.014\n",
            "113 128 Loss 0.005\n",
            "113 129 Loss 0.013\n",
            "113 130 Loss 0.049\n",
            "113 131 Loss 0.008\n",
            "113 132 Loss 0.014\n",
            "113 133 Loss 0.026\n",
            "113 134 Loss 0.006\n",
            "113 135 Loss 0.021\n",
            "113 136 Loss 0.006\n",
            "113 137 Loss 0.013\n",
            "113 138 Loss 0.011\n",
            "113 139 Loss 0.027\n",
            "113 140 Loss 0.047\n",
            "113 141 Loss 0.029\n",
            "113 142 Loss 0.006\n",
            "113 143 Loss 0.005\n",
            "113 144 Loss 0.123\n",
            "113 145 Loss 0.016\n",
            "113 146 Loss 0.025\n",
            "113 147 Loss 0.022\n",
            "113 148 Loss 0.021\n",
            "113 149 Loss 0.003\n",
            "113 150 Loss 0.008\n",
            "113 151 Loss 0.013\n",
            "113 152 Loss 0.071\n",
            "113 153 Loss 0.022\n",
            "113 154 Loss 0.014\n",
            "113 155 Loss 0.077\n",
            "113 156 Loss 0.007\n",
            "113 157 Loss 0.006\n",
            "113 158 Loss 0.101\n",
            "113 159 Loss 0.003\n",
            "113 160 Loss 0.003\n",
            "113 161 Loss 0.029\n",
            "113 162 Loss 0.011\n",
            "113 163 Loss 0.019\n",
            "113 164 Loss 0.006\n",
            "113 165 Loss 0.003\n",
            "113 166 Loss 0.005\n",
            "113 167 Loss 0.029\n",
            "113 168 Loss 0.009\n",
            "113 169 Loss 0.025\n",
            "113 170 Loss 0.034\n",
            "113 171 Loss 0.005\n",
            "113 172 Loss 0.010\n",
            "113 173 Loss 0.031\n",
            "113 174 Loss 0.009\n",
            "113 175 Loss 0.012\n",
            "113 176 Loss 0.007\n",
            "113 177 Loss 0.058\n",
            "113 178 Loss 0.007\n",
            "113 179 Loss 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "114 0 Loss 0.006\n",
            "114 1 Loss 0.005\n",
            "114 2 Loss 0.033\n",
            "114 3 Loss 0.041\n",
            "114 4 Loss 0.073\n",
            "114 5 Loss 0.008\n",
            "114 6 Loss 0.012\n",
            "114 7 Loss 0.004\n",
            "114 8 Loss 0.013\n",
            "114 9 Loss 0.006\n",
            "114 10 Loss 0.033\n",
            "114 11 Loss 0.004\n",
            "114 12 Loss 0.031\n",
            "114 13 Loss 0.004\n",
            "114 14 Loss 0.007\n",
            "114 15 Loss 0.031\n",
            "114 16 Loss 0.029\n",
            "114 17 Loss 0.051\n",
            "114 18 Loss 0.086\n",
            "114 19 Loss 0.052\n",
            "114 20 Loss 0.004\n",
            "114 21 Loss 0.017\n",
            "114 22 Loss 0.003\n",
            "114 23 Loss 0.014\n",
            "114 24 Loss 0.025\n",
            "114 25 Loss 0.007\n",
            "114 26 Loss 0.003\n",
            "114 27 Loss 0.008\n",
            "114 28 Loss 0.032\n",
            "114 29 Loss 0.016\n",
            "114 30 Loss 0.011\n",
            "114 31 Loss 0.043\n",
            "114 32 Loss 0.007\n",
            "114 33 Loss 0.024\n",
            "114 34 Loss 0.035\n",
            "114 35 Loss 0.015\n",
            "114 36 Loss 0.028\n",
            "114 37 Loss 0.001\n",
            "114 38 Loss 0.038\n",
            "114 39 Loss 0.134\n",
            "114 40 Loss 0.007\n",
            "114 41 Loss 0.012\n",
            "114 42 Loss 0.014\n",
            "114 43 Loss 0.018\n",
            "114 44 Loss 0.015\n",
            "114 45 Loss 0.019\n",
            "114 46 Loss 0.007\n",
            "114 47 Loss 0.014\n",
            "114 48 Loss 0.003\n",
            "114 49 Loss 0.010\n",
            "114 50 Loss 0.009\n",
            "114 51 Loss 0.014\n",
            "114 52 Loss 0.063\n",
            "114 53 Loss 0.007\n",
            "114 54 Loss 0.026\n",
            "114 55 Loss 0.026\n",
            "114 56 Loss 0.069\n",
            "114 57 Loss 0.008\n",
            "114 58 Loss 0.011\n",
            "114 59 Loss 0.003\n",
            "114 60 Loss 0.001\n",
            "114 61 Loss 0.006\n",
            "114 62 Loss 0.011\n",
            "114 63 Loss 0.006\n",
            "114 64 Loss 0.011\n",
            "114 65 Loss 0.025\n",
            "114 66 Loss 0.034\n",
            "114 67 Loss 0.037\n",
            "114 68 Loss 0.025\n",
            "114 69 Loss 0.051\n",
            "114 70 Loss 0.053\n",
            "114 71 Loss 0.022\n",
            "114 72 Loss 0.012\n",
            "114 73 Loss 0.012\n",
            "114 74 Loss 0.014\n",
            "114 75 Loss 0.049\n",
            "114 76 Loss 0.006\n",
            "114 77 Loss 0.003\n",
            "114 78 Loss 0.002\n",
            "114 79 Loss 0.003\n",
            "114 80 Loss 0.004\n",
            "114 81 Loss 0.008\n",
            "114 82 Loss 0.027\n",
            "114 83 Loss 0.016\n",
            "114 84 Loss 0.014\n",
            "114 85 Loss 0.029\n",
            "114 86 Loss 0.003\n",
            "114 87 Loss 0.034\n",
            "114 88 Loss 0.013\n",
            "114 89 Loss 0.030\n",
            "114 90 Loss 0.010\n",
            "114 91 Loss 0.013\n",
            "114 92 Loss 0.007\n",
            "114 93 Loss 0.004\n",
            "114 94 Loss 0.006\n",
            "114 95 Loss 0.041\n",
            "114 96 Loss 0.026\n",
            "114 97 Loss 0.017\n",
            "114 98 Loss 0.032\n",
            "114 99 Loss 0.006\n",
            "114 100 Loss 0.022\n",
            "114 101 Loss 0.035\n",
            "114 102 Loss 0.005\n",
            "114 103 Loss 0.023\n",
            "114 104 Loss 0.015\n",
            "114 105 Loss 0.024\n",
            "114 106 Loss 0.053\n",
            "114 107 Loss 0.009\n",
            "114 108 Loss 0.047\n",
            "114 109 Loss 0.046\n",
            "114 110 Loss 0.013\n",
            "114 111 Loss 0.006\n",
            "114 112 Loss 0.019\n",
            "114 113 Loss 0.008\n",
            "114 114 Loss 0.003\n",
            "114 115 Loss 0.005\n",
            "114 116 Loss 0.020\n",
            "114 117 Loss 0.054\n",
            "114 118 Loss 0.011\n",
            "114 119 Loss 0.013\n",
            "114 120 Loss 0.007\n",
            "114 121 Loss 0.018\n",
            "114 122 Loss 0.031\n",
            "114 123 Loss 0.008\n",
            "114 124 Loss 0.018\n",
            "114 125 Loss 0.011\n",
            "114 126 Loss 0.016\n",
            "114 127 Loss 0.009\n",
            "114 128 Loss 0.016\n",
            "114 129 Loss 0.004\n",
            "114 130 Loss 0.003\n",
            "114 131 Loss 0.024\n",
            "114 132 Loss 0.012\n",
            "114 133 Loss 0.012\n",
            "114 134 Loss 0.004\n",
            "114 135 Loss 0.009\n",
            "114 136 Loss 0.009\n",
            "114 137 Loss 0.016\n",
            "114 138 Loss 0.015\n",
            "114 139 Loss 0.021\n",
            "114 140 Loss 0.004\n",
            "114 141 Loss 0.010\n",
            "114 142 Loss 0.051\n",
            "114 143 Loss 0.011\n",
            "114 144 Loss 0.019\n",
            "114 145 Loss 0.003\n",
            "114 146 Loss 0.028\n",
            "114 147 Loss 0.021\n",
            "114 148 Loss 0.012\n",
            "114 149 Loss 0.017\n",
            "114 150 Loss 0.016\n",
            "114 151 Loss 0.014\n",
            "114 152 Loss 0.007\n",
            "114 153 Loss 0.019\n",
            "114 154 Loss 0.010\n",
            "114 155 Loss 0.016\n",
            "114 156 Loss 0.079\n",
            "114 157 Loss 0.014\n",
            "114 158 Loss 0.002\n",
            "114 159 Loss 0.040\n",
            "114 160 Loss 0.017\n",
            "114 161 Loss 0.017\n",
            "114 162 Loss 0.009\n",
            "114 163 Loss 0.005\n",
            "114 164 Loss 0.013\n",
            "114 165 Loss 0.043\n",
            "114 166 Loss 0.011\n",
            "114 167 Loss 0.039\n",
            "114 168 Loss 0.009\n",
            "114 169 Loss 0.031\n",
            "114 170 Loss 0.020\n",
            "114 171 Loss 0.014\n",
            "114 172 Loss 0.010\n",
            "114 173 Loss 0.027\n",
            "114 174 Loss 0.050\n",
            "114 175 Loss 0.011\n",
            "114 176 Loss 0.011\n",
            "114 177 Loss 0.006\n",
            "114 178 Loss 0.049\n",
            "114 179 Loss 0.138\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.90      0.93      0.92        29\n",
            "           3       0.97      0.86      0.91        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.86      0.79      0.82        38\n",
            "           9       0.94      0.90      0.92        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "115 0 Loss 0.011\n",
            "115 1 Loss 0.019\n",
            "115 2 Loss 0.037\n",
            "115 3 Loss 0.040\n",
            "115 4 Loss 0.014\n",
            "115 5 Loss 0.024\n",
            "115 6 Loss 0.007\n",
            "115 7 Loss 0.007\n",
            "115 8 Loss 0.011\n",
            "115 9 Loss 0.020\n",
            "115 10 Loss 0.006\n",
            "115 11 Loss 0.019\n",
            "115 12 Loss 0.010\n",
            "115 13 Loss 0.005\n",
            "115 14 Loss 0.034\n",
            "115 15 Loss 0.044\n",
            "115 16 Loss 0.008\n",
            "115 17 Loss 0.013\n",
            "115 18 Loss 0.012\n",
            "115 19 Loss 0.068\n",
            "115 20 Loss 0.026\n",
            "115 21 Loss 0.013\n",
            "115 22 Loss 0.007\n",
            "115 23 Loss 0.002\n",
            "115 24 Loss 0.039\n",
            "115 25 Loss 0.007\n",
            "115 26 Loss 0.008\n",
            "115 27 Loss 0.008\n",
            "115 28 Loss 0.007\n",
            "115 29 Loss 0.007\n",
            "115 30 Loss 0.009\n",
            "115 31 Loss 0.071\n",
            "115 32 Loss 0.038\n",
            "115 33 Loss 0.005\n",
            "115 34 Loss 0.045\n",
            "115 35 Loss 0.003\n",
            "115 36 Loss 0.010\n",
            "115 37 Loss 0.003\n",
            "115 38 Loss 0.033\n",
            "115 39 Loss 0.028\n",
            "115 40 Loss 0.014\n",
            "115 41 Loss 0.020\n",
            "115 42 Loss 0.004\n",
            "115 43 Loss 0.021\n",
            "115 44 Loss 0.012\n",
            "115 45 Loss 0.012\n",
            "115 46 Loss 0.013\n",
            "115 47 Loss 0.009\n",
            "115 48 Loss 0.018\n",
            "115 49 Loss 0.022\n",
            "115 50 Loss 0.030\n",
            "115 51 Loss 0.008\n",
            "115 52 Loss 0.011\n",
            "115 53 Loss 0.009\n",
            "115 54 Loss 0.018\n",
            "115 55 Loss 0.006\n",
            "115 56 Loss 0.008\n",
            "115 57 Loss 0.060\n",
            "115 58 Loss 0.017\n",
            "115 59 Loss 0.012\n",
            "115 60 Loss 0.006\n",
            "115 61 Loss 0.001\n",
            "115 62 Loss 0.011\n",
            "115 63 Loss 0.099\n",
            "115 64 Loss 0.058\n",
            "115 65 Loss 0.022\n",
            "115 66 Loss 0.014\n",
            "115 67 Loss 0.009\n",
            "115 68 Loss 0.007\n",
            "115 69 Loss 0.010\n",
            "115 70 Loss 0.020\n",
            "115 71 Loss 0.022\n",
            "115 72 Loss 0.076\n",
            "115 73 Loss 0.013\n",
            "115 74 Loss 0.011\n",
            "115 75 Loss 0.014\n",
            "115 76 Loss 0.005\n",
            "115 77 Loss 0.030\n",
            "115 78 Loss 0.038\n",
            "115 79 Loss 0.004\n",
            "115 80 Loss 0.008\n",
            "115 81 Loss 0.012\n",
            "115 82 Loss 0.007\n",
            "115 83 Loss 0.030\n",
            "115 84 Loss 0.016\n",
            "115 85 Loss 0.018\n",
            "115 86 Loss 0.021\n",
            "115 87 Loss 0.009\n",
            "115 88 Loss 0.028\n",
            "115 89 Loss 0.016\n",
            "115 90 Loss 0.007\n",
            "115 91 Loss 0.001\n",
            "115 92 Loss 0.006\n",
            "115 93 Loss 0.017\n",
            "115 94 Loss 0.031\n",
            "115 95 Loss 0.012\n",
            "115 96 Loss 0.015\n",
            "115 97 Loss 0.006\n",
            "115 98 Loss 0.011\n",
            "115 99 Loss 0.019\n",
            "115 100 Loss 0.009\n",
            "115 101 Loss 0.014\n",
            "115 102 Loss 0.029\n",
            "115 103 Loss 0.022\n",
            "115 104 Loss 0.015\n",
            "115 105 Loss 0.058\n",
            "115 106 Loss 0.022\n",
            "115 107 Loss 0.003\n",
            "115 108 Loss 0.012\n",
            "115 109 Loss 0.007\n",
            "115 110 Loss 0.012\n",
            "115 111 Loss 0.011\n",
            "115 112 Loss 0.077\n",
            "115 113 Loss 0.013\n",
            "115 114 Loss 0.017\n",
            "115 115 Loss 0.030\n",
            "115 116 Loss 0.015\n",
            "115 117 Loss 0.033\n",
            "115 118 Loss 0.020\n",
            "115 119 Loss 0.025\n",
            "115 120 Loss 0.060\n",
            "115 121 Loss 0.005\n",
            "115 122 Loss 0.011\n",
            "115 123 Loss 0.005\n",
            "115 124 Loss 0.026\n",
            "115 125 Loss 0.007\n",
            "115 126 Loss 0.004\n",
            "115 127 Loss 0.006\n",
            "115 128 Loss 0.014\n",
            "115 129 Loss 0.042\n",
            "115 130 Loss 0.046\n",
            "115 131 Loss 0.018\n",
            "115 132 Loss 0.008\n",
            "115 133 Loss 0.001\n",
            "115 134 Loss 0.060\n",
            "115 135 Loss 0.037\n",
            "115 136 Loss 0.019\n",
            "115 137 Loss 0.157\n",
            "115 138 Loss 0.004\n",
            "115 139 Loss 0.010\n",
            "115 140 Loss 0.008\n",
            "115 141 Loss 0.007\n",
            "115 142 Loss 0.089\n",
            "115 143 Loss 0.011\n",
            "115 144 Loss 0.053\n",
            "115 145 Loss 0.101\n",
            "115 146 Loss 0.012\n",
            "115 147 Loss 0.033\n",
            "115 148 Loss 0.005\n",
            "115 149 Loss 0.005\n",
            "115 150 Loss 0.007\n",
            "115 151 Loss 0.003\n",
            "115 152 Loss 0.004\n",
            "115 153 Loss 0.013\n",
            "115 154 Loss 0.010\n",
            "115 155 Loss 0.014\n",
            "115 156 Loss 0.006\n",
            "115 157 Loss 0.015\n",
            "115 158 Loss 0.002\n",
            "115 159 Loss 0.004\n",
            "115 160 Loss 0.021\n",
            "115 161 Loss 0.019\n",
            "115 162 Loss 0.014\n",
            "115 163 Loss 0.005\n",
            "115 164 Loss 0.041\n",
            "115 165 Loss 0.016\n",
            "115 166 Loss 0.010\n",
            "115 167 Loss 0.018\n",
            "115 168 Loss 0.005\n",
            "115 169 Loss 0.009\n",
            "115 170 Loss 0.022\n",
            "115 171 Loss 0.064\n",
            "115 172 Loss 0.013\n",
            "115 173 Loss 0.024\n",
            "115 174 Loss 0.015\n",
            "115 175 Loss 0.013\n",
            "115 176 Loss 0.005\n",
            "115 177 Loss 0.013\n",
            "115 178 Loss 0.010\n",
            "115 179 Loss 0.013\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        32\n",
            "           1       0.84      0.93      0.88        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.82      0.82      0.82        38\n",
            "           9       0.96      0.88      0.92        50\n",
            "\n",
            "    accuracy                           0.90       360\n",
            "   macro avg       0.90      0.91      0.90       360\n",
            "weighted avg       0.91      0.90      0.90       360\n",
            "\n",
            "0.9027777777777778\n",
            "116 0 Loss 0.043\n",
            "116 1 Loss 0.008\n",
            "116 2 Loss 0.012\n",
            "116 3 Loss 0.020\n",
            "116 4 Loss 0.002\n",
            "116 5 Loss 0.012\n",
            "116 6 Loss 0.016\n",
            "116 7 Loss 0.023\n",
            "116 8 Loss 0.018\n",
            "116 9 Loss 0.030\n",
            "116 10 Loss 0.058\n",
            "116 11 Loss 0.026\n",
            "116 12 Loss 0.014\n",
            "116 13 Loss 0.006\n",
            "116 14 Loss 0.016\n",
            "116 15 Loss 0.004\n",
            "116 16 Loss 0.003\n",
            "116 17 Loss 0.069\n",
            "116 18 Loss 0.039\n",
            "116 19 Loss 0.013\n",
            "116 20 Loss 0.023\n",
            "116 21 Loss 0.024\n",
            "116 22 Loss 0.016\n",
            "116 23 Loss 0.003\n",
            "116 24 Loss 0.010\n",
            "116 25 Loss 0.005\n",
            "116 26 Loss 0.048\n",
            "116 27 Loss 0.029\n",
            "116 28 Loss 0.013\n",
            "116 29 Loss 0.002\n",
            "116 30 Loss 0.005\n",
            "116 31 Loss 0.021\n",
            "116 32 Loss 0.025\n",
            "116 33 Loss 0.003\n",
            "116 34 Loss 0.007\n",
            "116 35 Loss 0.003\n",
            "116 36 Loss 0.002\n",
            "116 37 Loss 0.016\n",
            "116 38 Loss 0.037\n",
            "116 39 Loss 0.001\n",
            "116 40 Loss 0.002\n",
            "116 41 Loss 0.039\n",
            "116 42 Loss 0.014\n",
            "116 43 Loss 0.150\n",
            "116 44 Loss 0.011\n",
            "116 45 Loss 0.015\n",
            "116 46 Loss 0.017\n",
            "116 47 Loss 0.001\n",
            "116 48 Loss 0.006\n",
            "116 49 Loss 0.011\n",
            "116 50 Loss 0.001\n",
            "116 51 Loss 0.008\n",
            "116 52 Loss 0.009\n",
            "116 53 Loss 0.012\n",
            "116 54 Loss 0.014\n",
            "116 55 Loss 0.007\n",
            "116 56 Loss 0.015\n",
            "116 57 Loss 0.007\n",
            "116 58 Loss 0.009\n",
            "116 59 Loss 0.015\n",
            "116 60 Loss 0.008\n",
            "116 61 Loss 0.014\n",
            "116 62 Loss 0.006\n",
            "116 63 Loss 0.046\n",
            "116 64 Loss 0.022\n",
            "116 65 Loss 0.005\n",
            "116 66 Loss 0.019\n",
            "116 67 Loss 0.037\n",
            "116 68 Loss 0.010\n",
            "116 69 Loss 0.005\n",
            "116 70 Loss 0.007\n",
            "116 71 Loss 0.028\n",
            "116 72 Loss 0.006\n",
            "116 73 Loss 0.019\n",
            "116 74 Loss 0.048\n",
            "116 75 Loss 0.013\n",
            "116 76 Loss 0.001\n",
            "116 77 Loss 0.010\n",
            "116 78 Loss 0.016\n",
            "116 79 Loss 0.018\n",
            "116 80 Loss 0.086\n",
            "116 81 Loss 0.012\n",
            "116 82 Loss 0.016\n",
            "116 83 Loss 0.020\n",
            "116 84 Loss 0.014\n",
            "116 85 Loss 0.020\n",
            "116 86 Loss 0.007\n",
            "116 87 Loss 0.028\n",
            "116 88 Loss 0.085\n",
            "116 89 Loss 0.016\n",
            "116 90 Loss 0.015\n",
            "116 91 Loss 0.030\n",
            "116 92 Loss 0.016\n",
            "116 93 Loss 0.022\n",
            "116 94 Loss 0.009\n",
            "116 95 Loss 0.010\n",
            "116 96 Loss 0.016\n",
            "116 97 Loss 0.005\n",
            "116 98 Loss 0.002\n",
            "116 99 Loss 0.022\n",
            "116 100 Loss 0.033\n",
            "116 101 Loss 0.059\n",
            "116 102 Loss 0.006\n",
            "116 103 Loss 0.053\n",
            "116 104 Loss 0.010\n",
            "116 105 Loss 0.006\n",
            "116 106 Loss 0.009\n",
            "116 107 Loss 0.014\n",
            "116 108 Loss 0.079\n",
            "116 109 Loss 0.073\n",
            "116 110 Loss 0.005\n",
            "116 111 Loss 0.007\n",
            "116 112 Loss 0.023\n",
            "116 113 Loss 0.005\n",
            "116 114 Loss 0.016\n",
            "116 115 Loss 0.008\n",
            "116 116 Loss 0.009\n",
            "116 117 Loss 0.017\n",
            "116 118 Loss 0.002\n",
            "116 119 Loss 0.013\n",
            "116 120 Loss 0.006\n",
            "116 121 Loss 0.015\n",
            "116 122 Loss 0.034\n",
            "116 123 Loss 0.010\n",
            "116 124 Loss 0.006\n",
            "116 125 Loss 0.014\n",
            "116 126 Loss 0.006\n",
            "116 127 Loss 0.047\n",
            "116 128 Loss 0.013\n",
            "116 129 Loss 0.012\n",
            "116 130 Loss 0.009\n",
            "116 131 Loss 0.024\n",
            "116 132 Loss 0.029\n",
            "116 133 Loss 0.014\n",
            "116 134 Loss 0.029\n",
            "116 135 Loss 0.052\n",
            "116 136 Loss 0.007\n",
            "116 137 Loss 0.012\n",
            "116 138 Loss 0.005\n",
            "116 139 Loss 0.004\n",
            "116 140 Loss 0.002\n",
            "116 141 Loss 0.066\n",
            "116 142 Loss 0.039\n",
            "116 143 Loss 0.015\n",
            "116 144 Loss 0.042\n",
            "116 145 Loss 0.023\n",
            "116 146 Loss 0.004\n",
            "116 147 Loss 0.009\n",
            "116 148 Loss 0.038\n",
            "116 149 Loss 0.015\n",
            "116 150 Loss 0.017\n",
            "116 151 Loss 0.004\n",
            "116 152 Loss 0.010\n",
            "116 153 Loss 0.035\n",
            "116 154 Loss 0.018\n",
            "116 155 Loss 0.013\n",
            "116 156 Loss 0.009\n",
            "116 157 Loss 0.010\n",
            "116 158 Loss 0.006\n",
            "116 159 Loss 0.012\n",
            "116 160 Loss 0.012\n",
            "116 161 Loss 0.008\n",
            "116 162 Loss 0.074\n",
            "116 163 Loss 0.021\n",
            "116 164 Loss 0.010\n",
            "116 165 Loss 0.002\n",
            "116 166 Loss 0.019\n",
            "116 167 Loss 0.003\n",
            "116 168 Loss 0.011\n",
            "116 169 Loss 0.022\n",
            "116 170 Loss 0.010\n",
            "116 171 Loss 0.051\n",
            "116 172 Loss 0.020\n",
            "116 173 Loss 0.042\n",
            "116 174 Loss 0.021\n",
            "116 175 Loss 0.044\n",
            "116 176 Loss 0.004\n",
            "116 177 Loss 0.069\n",
            "116 178 Loss 0.023\n",
            "116 179 Loss 0.002\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.94      0.86      0.90        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.81      0.86        43\n",
            "           8       0.84      0.82      0.83        38\n",
            "           9       0.96      0.90      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "117 0 Loss 0.016\n",
            "117 1 Loss 0.135\n",
            "117 2 Loss 0.008\n",
            "117 3 Loss 0.019\n",
            "117 4 Loss 0.003\n",
            "117 5 Loss 0.025\n",
            "117 6 Loss 0.014\n",
            "117 7 Loss 0.013\n",
            "117 8 Loss 0.002\n",
            "117 9 Loss 0.006\n",
            "117 10 Loss 0.031\n",
            "117 11 Loss 0.012\n",
            "117 12 Loss 0.005\n",
            "117 13 Loss 0.031\n",
            "117 14 Loss 0.006\n",
            "117 15 Loss 0.055\n",
            "117 16 Loss 0.011\n",
            "117 17 Loss 0.011\n",
            "117 18 Loss 0.012\n",
            "117 19 Loss 0.044\n",
            "117 20 Loss 0.027\n",
            "117 21 Loss 0.035\n",
            "117 22 Loss 0.085\n",
            "117 23 Loss 0.032\n",
            "117 24 Loss 0.017\n",
            "117 25 Loss 0.005\n",
            "117 26 Loss 0.016\n",
            "117 27 Loss 0.015\n",
            "117 28 Loss 0.007\n",
            "117 29 Loss 0.019\n",
            "117 30 Loss 0.031\n",
            "117 31 Loss 0.002\n",
            "117 32 Loss 0.027\n",
            "117 33 Loss 0.015\n",
            "117 34 Loss 0.009\n",
            "117 35 Loss 0.006\n",
            "117 36 Loss 0.007\n",
            "117 37 Loss 0.017\n",
            "117 38 Loss 0.011\n",
            "117 39 Loss 0.049\n",
            "117 40 Loss 0.023\n",
            "117 41 Loss 0.026\n",
            "117 42 Loss 0.035\n",
            "117 43 Loss 0.012\n",
            "117 44 Loss 0.009\n",
            "117 45 Loss 0.003\n",
            "117 46 Loss 0.005\n",
            "117 47 Loss 0.029\n",
            "117 48 Loss 0.010\n",
            "117 49 Loss 0.014\n",
            "117 50 Loss 0.025\n",
            "117 51 Loss 0.025\n",
            "117 52 Loss 0.023\n",
            "117 53 Loss 0.005\n",
            "117 54 Loss 0.010\n",
            "117 55 Loss 0.007\n",
            "117 56 Loss 0.023\n",
            "117 57 Loss 0.037\n",
            "117 58 Loss 0.005\n",
            "117 59 Loss 0.008\n",
            "117 60 Loss 0.022\n",
            "117 61 Loss 0.012\n",
            "117 62 Loss 0.009\n",
            "117 63 Loss 0.020\n",
            "117 64 Loss 0.007\n",
            "117 65 Loss 0.005\n",
            "117 66 Loss 0.012\n",
            "117 67 Loss 0.014\n",
            "117 68 Loss 0.047\n",
            "117 69 Loss 0.043\n",
            "117 70 Loss 0.017\n",
            "117 71 Loss 0.023\n",
            "117 72 Loss 0.004\n",
            "117 73 Loss 0.014\n",
            "117 74 Loss 0.006\n",
            "117 75 Loss 0.006\n",
            "117 76 Loss 0.008\n",
            "117 77 Loss 0.008\n",
            "117 78 Loss 0.018\n",
            "117 79 Loss 0.011\n",
            "117 80 Loss 0.016\n",
            "117 81 Loss 0.025\n",
            "117 82 Loss 0.012\n",
            "117 83 Loss 0.005\n",
            "117 84 Loss 0.007\n",
            "117 85 Loss 0.016\n",
            "117 86 Loss 0.032\n",
            "117 87 Loss 0.010\n",
            "117 88 Loss 0.002\n",
            "117 89 Loss 0.037\n",
            "117 90 Loss 0.017\n",
            "117 91 Loss 0.019\n",
            "117 92 Loss 0.017\n",
            "117 93 Loss 0.013\n",
            "117 94 Loss 0.049\n",
            "117 95 Loss 0.024\n",
            "117 96 Loss 0.031\n",
            "117 97 Loss 0.018\n",
            "117 98 Loss 0.029\n",
            "117 99 Loss 0.004\n",
            "117 100 Loss 0.055\n",
            "117 101 Loss 0.016\n",
            "117 102 Loss 0.035\n",
            "117 103 Loss 0.002\n",
            "117 104 Loss 0.090\n",
            "117 105 Loss 0.043\n",
            "117 106 Loss 0.010\n",
            "117 107 Loss 0.029\n",
            "117 108 Loss 0.021\n",
            "117 109 Loss 0.055\n",
            "117 110 Loss 0.016\n",
            "117 111 Loss 0.006\n",
            "117 112 Loss 0.027\n",
            "117 113 Loss 0.006\n",
            "117 114 Loss 0.019\n",
            "117 115 Loss 0.023\n",
            "117 116 Loss 0.017\n",
            "117 117 Loss 0.039\n",
            "117 118 Loss 0.016\n",
            "117 119 Loss 0.042\n",
            "117 120 Loss 0.011\n",
            "117 121 Loss 0.025\n",
            "117 122 Loss 0.015\n",
            "117 123 Loss 0.037\n",
            "117 124 Loss 0.013\n",
            "117 125 Loss 0.013\n",
            "117 126 Loss 0.006\n",
            "117 127 Loss 0.008\n",
            "117 128 Loss 0.002\n",
            "117 129 Loss 0.008\n",
            "117 130 Loss 0.017\n",
            "117 131 Loss 0.011\n",
            "117 132 Loss 0.021\n",
            "117 133 Loss 0.012\n",
            "117 134 Loss 0.042\n",
            "117 135 Loss 0.011\n",
            "117 136 Loss 0.043\n",
            "117 137 Loss 0.011\n",
            "117 138 Loss 0.003\n",
            "117 139 Loss 0.017\n",
            "117 140 Loss 0.004\n",
            "117 141 Loss 0.017\n",
            "117 142 Loss 0.012\n",
            "117 143 Loss 0.011\n",
            "117 144 Loss 0.015\n",
            "117 145 Loss 0.010\n",
            "117 146 Loss 0.027\n",
            "117 147 Loss 0.002\n",
            "117 148 Loss 0.003\n",
            "117 149 Loss 0.006\n",
            "117 150 Loss 0.025\n",
            "117 151 Loss 0.009\n",
            "117 152 Loss 0.012\n",
            "117 153 Loss 0.007\n",
            "117 154 Loss 0.022\n",
            "117 155 Loss 0.013\n",
            "117 156 Loss 0.002\n",
            "117 157 Loss 0.015\n",
            "117 158 Loss 0.004\n",
            "117 159 Loss 0.009\n",
            "117 160 Loss 0.002\n",
            "117 161 Loss 0.039\n",
            "117 162 Loss 0.009\n",
            "117 163 Loss 0.003\n",
            "117 164 Loss 0.017\n",
            "117 165 Loss 0.004\n",
            "117 166 Loss 0.003\n",
            "117 167 Loss 0.100\n",
            "117 168 Loss 0.029\n",
            "117 169 Loss 0.041\n",
            "117 170 Loss 0.014\n",
            "117 171 Loss 0.006\n",
            "117 172 Loss 0.017\n",
            "117 173 Loss 0.011\n",
            "117 174 Loss 0.089\n",
            "117 175 Loss 0.024\n",
            "117 176 Loss 0.020\n",
            "117 177 Loss 0.017\n",
            "117 178 Loss 0.008\n",
            "117 179 Loss 0.016\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98        32\n",
            "           1       0.82      0.96      0.89        28\n",
            "           2       0.93      0.93      0.93        29\n",
            "           3       0.91      0.86      0.89        37\n",
            "           4       0.97      0.94      0.95        31\n",
            "           5       0.80      0.94      0.87        35\n",
            "           6       0.90      0.97      0.94        37\n",
            "           7       0.92      0.84      0.88        43\n",
            "           8       0.86      0.79      0.82        38\n",
            "           9       0.96      0.90      0.93        50\n",
            "\n",
            "    accuracy                           0.91       360\n",
            "   macro avg       0.91      0.91      0.91       360\n",
            "weighted avg       0.91      0.91      0.91       360\n",
            "\n",
            "0.9055555555555556\n",
            "118 0 Loss 0.031\n",
            "118 1 Loss 0.007\n",
            "118 2 Loss 0.007\n",
            "118 3 Loss 0.010\n",
            "118 4 Loss 0.010\n",
            "118 5 Loss 0.007\n",
            "118 6 Loss 0.003\n",
            "118 7 Loss 0.024\n",
            "118 8 Loss 0.016\n",
            "118 9 Loss 0.018\n",
            "118 10 Loss 0.006\n",
            "118 11 Loss 0.027\n",
            "118 12 Loss 0.007\n",
            "118 13 Loss 0.013\n",
            "118 14 Loss 0.030\n",
            "118 15 Loss 0.018\n",
            "118 16 Loss 0.011\n",
            "118 17 Loss 0.012\n",
            "118 18 Loss 0.004\n",
            "118 19 Loss 0.003\n",
            "118 20 Loss 0.028\n",
            "118 21 Loss 0.003\n",
            "118 22 Loss 0.010\n",
            "118 23 Loss 0.009\n",
            "118 24 Loss 0.023\n",
            "118 25 Loss 0.025\n",
            "118 26 Loss 0.013\n",
            "118 27 Loss 0.022\n",
            "118 28 Loss 0.071\n",
            "118 29 Loss 0.007\n",
            "118 30 Loss 0.010\n",
            "118 31 Loss 0.038\n",
            "118 32 Loss 0.011\n",
            "118 33 Loss 0.012\n",
            "118 34 Loss 0.006\n",
            "118 35 Loss 0.023\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-109aa67009fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-a210e171818d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-a210e171818d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}